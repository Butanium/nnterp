{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9699fa29",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Butanium/nnterp/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef12d930",
      "metadata": {
        "id": "ef12d930"
      },
      "source": [
        "# Demo: nnterp Features Showcase\n",
        "\n",
        "This notebook demonstrates the key features of `nnterp`, which aims to offer a unified interface for all transformer models and give best `NNsight` practices for LLMs in everyone's hands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d40bbfdb",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  !pip install git+https://github.com/Butanium/nnterp.git\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d8075c",
      "metadata": {},
      "source": [
        "## 1. Standardized Interface\n",
        "\n",
        "Similar to [`transformer_lens`](https://github.com/TransformerLensOrg/TransformerLens), `nnterp` provides a standardized interface for all transformer models.\n",
        "The main difference is that `nnterp` still uses the huggingface implementation under the hood through `NNsight`, while transformer_lens uses its own implementation of the transformer architecture. However, each transformer implementation has its own quirks, such that `transformer_lens` is not able to support all models, and can sometimes have significant difference with the huggingface implementation.\n",
        "\n",
        "Note that `nnterp` doesn't support all models either, since `NNsight` itself doesn't support all architectures. Additionally, because different models use different naming conventions, `nnterp` doesn't support all HuggingFace models, but it does support a good portion of them. When a model is loaded in `nnterp`, automatic tests are performed to verify that the model has been correctly renamed and that `nnterp`'s hooks return the expected shapes. This means that even if an architecture hasn't been officially tested, the simple fact that it loads successfully indicates it's probably working correctly.\n",
        "\n",
        "The way it's implemented is based on the `NNsight` built-in renaming feature, to make all models look like the llama naming convention, without having to write `model.model`, namely:\n",
        "```ocaml\n",
        "StandardizedTransformer\n",
        "├── embed_tokens\n",
        "├── layers\n",
        "│   ├── self_attn\n",
        "│   └── mlp\n",
        "├── ln_final\n",
        "└── lm_head\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "506925b4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 64, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-7): 8 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=64, out_features=256, bias=False)\n",
            "          (up_proj): Linear(in_features=64, out_features=256, bias=False)\n",
            "          (down_proj): Linear(in_features=256, out_features=64, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((64,), eps=1e-06)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((64,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((64,), eps=1e-06)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=64, out_features=32000, bias=False)\n",
            ")\n",
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "print(AutoModelForCausalLM.from_pretrained(\"Maykeye/TinyLLama-v0\"))\n",
        "print(AutoModelForCausalLM.from_pretrained(\"gpt2\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1bae472",
      "metadata": {},
      "source": [
        "As you can see, the naming scheme of gpt2 is different from the llama naming convention.\n",
        "A simple way to fix this is to use the `rename` feature of `NNsight` to rename the gpt2 modules to the llama naming convention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b5e3bbdf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (model/transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (layers/h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn/attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_final/ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "  (generator): Generator(\n",
            "    (streamer): Streamer()\n",
            "  )\n",
            ")\n",
            "GPT2Attention(\n",
            "  (c_attn): Conv1D()\n",
            "  (c_proj): Conv1D()\n",
            "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from nnsight import LanguageModel\n",
        "\n",
        "model = LanguageModel(\n",
        "    \"gpt2\",\n",
        "    rename=dict(transformer=\"model\", h=\"layers\", ln_f=\"ln_final\", attn=\"self_attn\"),\n",
        ")\n",
        "print(model)\n",
        "# Access the attn module as if it was a llama model\n",
        "print(model.model.layers[0].self_attn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48d5f88c",
      "metadata": {},
      "source": [
        "You can see the that renamed modules are displayed like `(new_name)/old_name`. However, many models family have their own naming convention, `nnterp` has a global renaming scheme that should transform any model to the llama naming convention. The easiest way to use it is to load your model using the `StandardizedTransformer` class that inherits from `nnsight.LanguageModel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8b95cf9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-10-03 16:41:19.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (model/transformer): GPT2Model(\n",
            "    (embed_tokens/wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (layers/h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn/attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_final/ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "  (generator): Generator(\n",
            "    (streamer): Streamer()\n",
            "  )\n",
            "  (layers): ModuleList(\n",
            "    (0-11): 12 x GPT2Block(\n",
            "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (self_attn/attn): GPT2Attention(\n",
            "        (c_attn): Conv1D()\n",
            "        (c_proj): Conv1D()\n",
            "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): GPT2MLP(\n",
            "        (c_fc): Conv1D()\n",
            "        (c_proj): Conv1D()\n",
            "        (act): NewGELUActivation()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (embed_tokens): Embedding(50257, 768)\n",
            ")\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "from nnterp import StandardizedTransformer\n",
        "\n",
        "# You will see the `layers` module printed two times, it'll be explained later.\n",
        "nnterp_gpt2 = StandardizedTransformer(\"gpt2\")\n",
        "print(nnterp_gpt2)\n",
        "# StandardizedTransformer also use `device_map=\"auto\"` by default:\n",
        "nnterp_gpt2.dispatch()\n",
        "print(nnterp_gpt2.model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9243fdb9",
      "metadata": {},
      "source": [
        "Great! But I can see you at the back of the classroom, asking yourself:\n",
        "> \"Why would you create a package that just pass the right dict to the `NNsight` `rename` feature?\"\n",
        "\n",
        "And actually, I'm glad you asked! `StandardizedTransformer` and `nnterp` have a lot of other features, so bear with me!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6e849e",
      "metadata": {},
      "source": [
        "## 2. Accessing Modules I/O\n",
        "With `NNsight`, the most robust way to set the residual stream after layer 1 to be the residual stream after layer 0 for a LLama-like model would be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dfd93a0e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        }
      ],
      "source": [
        "from transformers import __version__ as TRANSFORMERS_VERSION\n",
        "from packaging.version import parse\n",
        "llama = LanguageModel(\"Maykeye/TinyLLama-v0\")\n",
        "# code for transformer \"<4.54\"\n",
        "is_old_transformers = parse(TRANSFORMERS_VERSION) < parse(\"4.54\")\n",
        "if is_old_transformers:\n",
        "    with llama.trace(\"hello\"):\n",
        "        llama.model.layers[1].output = (\n",
        "            llama.model.layers[0].output[0],\n",
        "            *llama.model.layers[1].output[1:],\n",
        "        )\n",
        "else:\n",
        "    with llama.trace(\"hello\"):\n",
        "        llama.model.layers[1].output = llama.model.layers[0].output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb2b6e8b",
      "metadata": {},
      "source": [
        "Note that the following can cause issues:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7b6f1153",
      "metadata": {},
      "outputs": [],
      "source": [
        "with llama.trace(\"hello\"):\n",
        "    # can't do this because .output is a tuple\n",
        "\n",
        "    # Can cause errors with gradient computation\n",
        "    if is_old_transformers:\n",
        "        # llama.model.layers[1].output[0] = llama.model.layers[0].output[0]\n",
        "        llama.model.layers[1].output[0][:] = llama.model.layers[0].output[0]\n",
        "    else:\n",
        "        llama.model.layers[1].output[:] = llama.model.layers[0].output\n",
        "\n",
        "if is_old_transformers:\n",
        "    with llama.trace(\"hello\"):\n",
        "        # Can cause errors with opt if you do this at its last layer (thanks pytest)\n",
        "        llama.model.layers[1].output = (llama.model.layers[0].output[0],)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12af609a",
      "metadata": {},
      "source": [
        "`nnterp` makes this much cleaner:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bda6cc29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# the version of transformers does not matter, the tuple vs not tuple stuff is handled internally\n",
        "# First, you can access layer inputs and outputs directly:\n",
        "with nnterp_gpt2.trace(\"hello\"):\n",
        "    # Access layer 5's output\n",
        "    layer_5_output = nnterp_gpt2.layers_output[5]\n",
        "    # Set layer 10's output to be layer 5's output\n",
        "    nnterp_gpt2.layers_output[10] = layer_5_output\n",
        "\n",
        "# You can also access attention and MLP outputs:\n",
        "with nnterp_gpt2.trace(\"hello\"):\n",
        "    attn_output = nnterp_gpt2.attentions_output[3]\n",
        "    mlp_output = nnterp_gpt2.mlps_output[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2085327c",
      "metadata": {},
      "source": [
        "## 3. `nnterp` Guarantees\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf83bee",
      "metadata": {},
      "source": [
        "When designing, `nnterp` I was very worried about silent failures, where you load a model, and then get an unexpected failure in your code downstream, or worst, it doesn't fail but give you fake results. To avoid this, when you load an `nnterp` model, a series of fast tests are run to ensure that:\n",
        "- The model has been correctly renamed\n",
        "- The model module output are of the expected shape\n",
        "- Attention probabilities have the right shape, sum to 1, and changing them changes the output\n",
        "\n",
        "This comes with the trade-off that `nnterp` will dispatch your model when you load it, which can be annoying if you don't want to load the model's weights. Also to be able to access the attention probabibilties, `nnterp` loads your model with the `eager` attention implementation, which can be slower than the default hf implementation. If you don't need the attention probabilities, you can force to use the default hf implementation / another one by passing `attn_implementation=None` or `attn_implementation=\"your_implementation\"`.\n",
        "\n",
        "What `nnterp` can NOT guarantee:\n",
        "- The attention probabilities won't be modified by the model before being multiplied by the values. To ensure this, you can check `model.attention_probabilities.print_source()` (preferably in a notebook for markdown display) to understand where the attention probabilities are computed.\n",
        "- Huggingface's transformers sheringan w\n",
        "\n",
        "If youe model is not properly renamed, you can pass a `RenameConfig` to the `nnterp` constructor to rename the model. See more in the advanced usage section of this demo.\n",
        "\n",
        "On top of that, before releasing a new version of `nnterp`, a series of tests covering most architectures are performed. When you load a model, `nnterp` will check if tests were run for your `nnsight` and `transformers` versions, and will check the tests results for the class of your model. I chose to include the tests in the `nnterp` package, so that if your model architecture has not been tested / you use a different version of `nnsight` or `transformers`, you can run `python -m nnterp run_tests --model-names foo bar --class-names LlamaForCausalLM` to run the tests for your model. `--class-names` allow you to run the tests on a toy model of the same class as your model to make it cheaper and faster."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bfe6199",
      "metadata": {},
      "source": [
        "## 4. Attention Probabilities\n",
        "\n",
        "For models that support it, you can access attention probabilities directly. You can check if a model supports it by calling `model.supports_attention_probabilities`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "141813bc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention probs shape will be: (batch, heads, seq_len, seq_len): torch.Size([1, 12, 6, 6])\n"
          ]
        }
      ],
      "source": [
        "import torch as th\n",
        "\n",
        "nnterp_gpt2.tokenizer.padding_side = (\n",
        "    \"left\"  # ensure left padding for easy access to the first token\n",
        ")\n",
        "\n",
        "with th.no_grad():\n",
        "    with nnterp_gpt2.trace(\"The cat sat on the mat\"):\n",
        "        # Access attention probabilities for layer 5\n",
        "        attn_probs_l2 = nnterp_gpt2.attention_probabilities[2].save()\n",
        "        attn_probs = nnterp_gpt2.attention_probabilities[5].save()\n",
        "        print(\n",
        "            f\"Attention probs shape will be: (batch, heads, seq_len, seq_len): {attn_probs.shape}\"\n",
        "        )\n",
        "        # knock out the attention to the first token\n",
        "        attn_probs[:, :, :, 0] = 0\n",
        "        attn_probs /= attn_probs.sum(dim=-1, keepdim=True)\n",
        "        corr_logits = nnterp_gpt2.logits.save()\n",
        "    with nnterp_gpt2.trace(\"The cat sat on the mat\"):\n",
        "        baseline_logits = nnterp_gpt2.logits.save()\n",
        "\n",
        "assert not th.allclose(corr_logits, baseline_logits)\n",
        "\n",
        "sums = attn_probs_l2.sum(dim=-1)\n",
        "# last dimension is the attention of token i to all other tokens, so should sum to 1\n",
        "assert th.allclose(sums, th.ones_like(sums))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d98920c",
      "metadata": {},
      "source": [
        "Under the hood this uses the new tracing system implemented in `NNsight v0.5` which allow to access most model intermediate variables during the forward pass. This means that if the `transformers` implementation were to change, this could break or give unexpected results, so it is recommended to use one of the tested versions of `transformers` and to check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()` if you want to use a different version of `transformers` / a architecture that has not been tested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "83968d2e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Accessing attention probabilities from:\n",
              "```py\n",
              "model.transformer.h.0.attn.attention_interface_0.module_attn_dropout_0:\n",
              "\n",
              "    ....\n",
              "            attn_weights = attn_weights + causal_mask\n",
              "    \n",
              "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
              "    \n",
              "        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
              "        attn_weights = attn_weights.type(value.dtype)\n",
              "    --> attn_weights = module.attn_dropout(attn_weights) <--\n",
              "    \n",
              "        # Mask heads if we want to\n",
              "        if head_mask is not None:\n",
              "            attn_weights = attn_weights * head_mask\n",
              "    \n",
              "        attn_output = torch.matmul(attn_weights, value)\n",
              "    ....\n",
              "```\n",
              "\n",
              "## Full module source:\n",
              "```py\n",
              "                             * def eager_attention_forward(module, query, key, value, attention_mask, head_mask=None, **kwargs):\n",
              " key_transpose_0         ->  0     attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
              " torch_matmul_0          ->  +     ...\n",
              "                             1 \n",
              "                             2     if module.scale_attn_weights:\n",
              " torch_full_0            ->  3         attn_weights = attn_weights / torch.full(\n",
              " value_size_0            ->  4             [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n",
              "                             5         )\n",
              "                             6 \n",
              "                             7     # Layer-wise attention scaling\n",
              "                             8     if module.scale_attn_by_inverse_layer_idx:\n",
              " float_0                 ->  9         attn_weights = attn_weights / float(module.layer_idx + 1)\n",
              "                            10 \n",
              "                            11     if not module.is_cross_attention:\n",
              "                            12         # if only \"normal\" attention layer implements causal mask\n",
              " query_size_0            -> 13         query_length, key_length = query.size(-2), key.size(-2)\n",
              " key_size_0              ->  +         ...\n",
              "                            14         causal_mask = module.bias[:, :, key_length - query_length : key_length, :key_length]\n",
              " torch_finfo_0           -> 15         mask_value = torch.finfo(attn_weights.dtype).min\n",
              "                            16         # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
              "                            17         # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
              " torch_full_1            -> 18         mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
              " attn_weights_to_0       -> 19         attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
              " torch_where_0           ->  +         ...\n",
              "                            20 \n",
              "                            21     if attention_mask is not None:\n",
              "                            22         # Apply the attention mask\n",
              "                            23         causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n",
              "                            24         attn_weights = attn_weights + causal_mask\n",
              "                            25 \n",
              " nn_functional_softmax_0 -> 26     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
              "                            27 \n",
              "                            28     # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
              " attn_weights_type_0     -> 29     attn_weights = attn_weights.type(value.dtype)\n",
              " module_attn_dropout_0   -> 30     attn_weights = module.attn_dropout(attn_weights)\n",
              "                            31 \n",
              "                            32     # Mask heads if we want to\n",
              "                            33     if head_mask is not None:\n",
              "                            34         attn_weights = attn_weights * head_mask\n",
              "                            35 \n",
              " torch_matmul_1          -> 36     attn_output = torch.matmul(attn_weights, value)\n",
              " attn_output_transpose_0 -> 37     attn_output = attn_output.transpose(1, 2)\n",
              "                            38 \n",
              "                            39     return attn_output, attn_weights\n",
              "                            40 \n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "nnterp_gpt2.attention_probabilities.print_source()  # pretty markdown display in a notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b85f355f",
      "metadata": {},
      "source": [
        "## 5. Builtin interventions\n",
        "\n",
        "`StandardizedTransformer` also provides convenient methods for common operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8651c10e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch as th\n",
        "\n",
        "# Project hidden states to vocabulary using the unembed norm and lm_head\n",
        "with nnterp_gpt2.trace(\"The capital of France is\"):\n",
        "    hidden = nnterp_gpt2.layers_output[5]\n",
        "    logits = nnterp_gpt2.project_on_vocab(hidden)\n",
        "\n",
        "# Skip layers entirely\n",
        "with nnterp_gpt2.trace(\"Hello world\"):\n",
        "    # Skip layer 1\n",
        "    nnterp_gpt2.skip_layer(1)\n",
        "    # Skip layers 2 through 3 (inclusive)\n",
        "    nnterp_gpt2.skip_layers(2, 3)\n",
        "\n",
        "# This is useful if you want to start at a later layer than the first one\n",
        "with nnterp_gpt2.trace(\"Hello world\") as tracer:\n",
        "    layer_6_out = nnterp_gpt2.layers_output[6].save()\n",
        "    tracer.stop()  # avoid computations after layer 6\n",
        "\n",
        "with nnterp_gpt2.trace(\"Hello world\"):\n",
        "    nnterp_gpt2.skip_layers(0, 6, skip_with=layer_6_out)\n",
        "    half_half_logits = nnterp_gpt2.logits.save()\n",
        "\n",
        "with nnterp_gpt2.trace(\"Hello world\"):\n",
        "    vanilla_logits = nnterp_gpt2.logits.save()\n",
        "\n",
        "assert th.allclose(vanilla_logits, half_half_logits)  # they should be the same\n",
        "\n",
        "# Direct steering\n",
        "steering_vector = th.randn(768)  # gpt2 hidden size\n",
        "with nnterp_gpt2.trace(\"The weather today is\"):\n",
        "    nnterp_gpt2.steer(layers=[1, 3], steering_vector=steering_vector, factor=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b783b615",
      "metadata": {},
      "source": [
        "## 6. Specific Token Activation Collection\n",
        "\n",
        "`nnterp` provides utilities for collecting activations efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6c33979c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batched activations shape: torch.Size([3, 100, 768])\n"
          ]
        }
      ],
      "source": [
        "from nnterp.nnsight_utils import (\n",
        "    get_token_activations,\n",
        "    collect_token_activations_batched,\n",
        ")\n",
        "\n",
        "# Collect activations for specific tokens\n",
        "prompts = [\"The capital of France is\", \"The weather today is\"]\n",
        "with nnterp_gpt2.trace(prompts) as tracer:\n",
        "    # Get last token activations for all layers\n",
        "    activations = get_token_activations(nnterp_gpt2, prompts, idx=-1, tracer=tracer)\n",
        "    # activations shape: (num_layers, batch_size, hidden_size)\n",
        "\n",
        "# For large datasets, use batched collection\n",
        "large_prompts = [\"Sample text \" + str(i) for i in range(100)]\n",
        "batch_activations = collect_token_activations_batched(\n",
        "    nnterp_gpt2,\n",
        "    large_prompts,\n",
        "    batch_size=16,\n",
        "    layers=[3, 9, 11],  # Only collect specific layers, default is all layers\n",
        "    idx=-1,  # Last token (default)\n",
        ")\n",
        "print(f\"Batched activations shape: {batch_activations.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1a92f9a",
      "metadata": {},
      "source": [
        "## 7. Prompt Utilities\n",
        "\n",
        "`nnterp` provides utilities for working with prompts and tracking probabilities of first tokens of certain strings. It tracks both the first token of \"string\" and \" string\".\n",
        "\n",
        "You can provide multiple string per category, the probabilities returned will be the sum of the probabilities of all the first tokens of the strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "149f0c3f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "target: ['Paris', 'ĠParis']\n",
            "traps: ['London', 'ĠLondon', 'Mad', 'ĠMadrid']\n",
            "longstring: ['the', 'Ġthe']\n",
            "target: ['J', 'ĠJupiter']\n",
            "traps: ['Earth', 'ĠEarth', 'Ne', 'ĠNeptune']\n",
            "longstring: ['Pal', 'ĠPalace']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0fd89fc88064f339f8c10419acea9d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target token probabilities:\n",
            "  target: shape torch.Size([2, 1])\n",
            "  traps: shape torch.Size([2, 1])\n",
            "  longstring: shape torch.Size([2, 1])\n"
          ]
        }
      ],
      "source": [
        "from nnterp.prompt_utils import Prompt, run_prompts\n",
        "\n",
        "# Create prompts with target tokens to track\n",
        "prompt1 = Prompt.from_strings(\n",
        "    \"The capital of France (not England or Spain) is\",\n",
        "    {\n",
        "        \"target\": \"Paris\",\n",
        "        \"traps\": [\"London\", \"Madrid\"],\n",
        "        \"longstring\": \"the country of France\",\n",
        "    },\n",
        "    nnterp_gpt2.tokenizer,\n",
        ")\n",
        "for name, tokens in prompt1.target_tokens.items():\n",
        "    print(f\"{name}: {nnterp_gpt2.tokenizer.convert_ids_to_tokens(tokens)}\")\n",
        "\n",
        "prompt2 = Prompt.from_strings(\n",
        "    \"The largest planet (not Earth or Neptune) is\",\n",
        "    {\"target\": \"Jupiter\", \"traps\": [\"Earth\", \"Neptune\"], \"longstring\": \"Palace planet\"},\n",
        "    nnterp_gpt2.tokenizer,\n",
        ")\n",
        "for name, tokens in prompt2.target_tokens.items():\n",
        "    print(f\"{name}: {nnterp_gpt2.tokenizer.convert_ids_to_tokens(tokens)}\")\n",
        "\n",
        "# Run prompts and get probabilities for target tokens\n",
        "results = run_prompts(nnterp_gpt2, [prompt1, prompt2], batch_size=2)\n",
        "print(\"Target token probabilities:\")\n",
        "for target, probs in results.items():\n",
        "    print(f\"  {target}: shape {probs.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e661dcb8",
      "metadata": {},
      "source": [
        "## 8. Interventions\n",
        "\n",
        "`nnterp` provides several intervention methods inspired by mechanistic interpretability research:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c34179bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logit lens output shape: torch.Size([2, 12, 50257])\n",
            "repeat_prompt: TargetPrompt(prompt='car:car\\n\\ncross:cross\\n\\nazdrfa:azdrfa\\n\\n*', index_to_patch=-1)\n",
            "custom_repeat_prompt: TargetPrompt(prompt='car:car\\n\\ncross:cross\\n\\nazdrfa:azdrfa\\n\\n*', index_to_patch=-1)\n",
            "patchscope_probs: torch.Size([2, 12, 50257])\n"
          ]
        }
      ],
      "source": [
        "from nnterp.interventions import (\n",
        "    logit_lens,\n",
        "    patchscope_lens,\n",
        "    TargetPrompt,\n",
        "    repeat_prompt,\n",
        "    steer,\n",
        ")\n",
        "\n",
        "# Logit Lens: See predictions at each layer\n",
        "prompts = [\"The capital of France is\", \"The sun rises in the\"]\n",
        "probs = logit_lens(nnterp_gpt2, prompts)\n",
        "print(f\"Logit lens output shape: {probs.shape}\")  # (batch, layers, vocab)\n",
        "\n",
        "# Patchscope: Replace activations from one context into another\n",
        "source_prompts = [\"Paris is beautiful\", \"London is foggy\"]\n",
        "custom_target_prompt = TargetPrompt(\"city: Paris\\nfood: croissant\\n?\", -1)\n",
        "target_prompt = repeat_prompt()  # Creates a repetition task\n",
        "custom_repeat_prompt = repeat_prompt(\n",
        "    words=[\"car\", \"cross\", \"azdrfa\"],\n",
        "    rel=\":\",\n",
        "    sep=\"\\n\\n\",\n",
        "    placeholder=\"*\",\n",
        ")\n",
        "print(f\"repeat_prompt: {custom_repeat_prompt}\")\n",
        "print(f\"custom_repeat_prompt: {custom_repeat_prompt}\")\n",
        "patchscope_probs = patchscope_lens(\n",
        "    nnterp_gpt2, source_prompts=source_prompts, target_patch_prompts=target_prompt\n",
        ")\n",
        "print(f\"patchscope_probs: {patchscope_probs.shape}\")\n",
        "\n",
        "# Steering with intervention function\n",
        "with nnterp_gpt2.trace(\"The weather is\"):\n",
        "    steer(nnterp_gpt2, layers=[5, 10], steering_vector=steering_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f50694e",
      "metadata": {
        "lines_to_next_cell": 0
      },
      "source": [
        "You can use a combination of run_prompts and interventions to get the probabilities of certain tokens according to your custom intervention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074c3253",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef4899795501417aac2e47802790b0d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "target: torch.Size([2, 12])\n",
            "english: torch.Size([2, 12])\n",
            "format: torch.Size([2, 12])\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'plotly'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobs.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# (batch, layers)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Create a plotly plot showing mean probabilities for each category across layers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_objects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgo\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Calculate mean probabilities across batches for each category and layer\u001b[39;00m\n\u001b[32m     25\u001b[39m mean_probs = {category: probs.mean(dim=\u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m category, probs \u001b[38;5;129;01min\u001b[39;00m results.items()}\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "demo_model = nnterp_gpt2\n",
        "# uncomment if you have a GPU for cooler results\n",
        "# demo_model = StandardizedTransformer(\"google/gemma-2-2b\")\n",
        "\n",
        "prompts_str = [\n",
        "    \"The translation of 'car' in French is\",\n",
        "    \"The translation of 'cat' in Spanish is\",\n",
        "]\n",
        "tokens = [\n",
        "    {\"target\": [\"voiture\", \"bagnole\"], \"english\": \"car\", \"format\": \"'\"},\n",
        "    {\"target\": [\"gato\", \"minino\"], \"english\": \"cat\", \"format\": \"'\"},\n",
        "]\n",
        "prompts = [\n",
        "    Prompt.from_strings(prompt, tokens, demo_model.tokenizer)\n",
        "    for prompt, tokens in zip(prompts_str, tokens)\n",
        "]\n",
        "results = run_prompts(demo_model, prompts, batch_size=2, get_probs_func=logit_lens)\n",
        "for category, probs in results.items():\n",
        "    print(f\"{category}: {probs.shape}\")  # (batch, layers)\n",
        "\n",
        "# Create a plotly plot showing mean probabilities for each category across layers\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Calculate mean probabilities across batches for each category and layer\n",
        "mean_probs = {category: probs.mean(dim=0) for category, probs in results.items()}\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add a line for each category\n",
        "for category, probs in mean_probs.items():\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=list(range(len(probs))),\n",
        "            y=probs.tolist(),\n",
        "            mode=\"lines+markers\",\n",
        "            name=category,\n",
        "            line=dict(width=2),\n",
        "            marker=dict(size=6),\n",
        "        )\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Mean Token Probabilities Across Layers\",\n",
        "    xaxis_title=\"Layer\",\n",
        "    yaxis_title=\"Mean Probability\",\n",
        "    hovermode=\"x unified\",\n",
        "    template=\"plotly_white\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5ec6bce",
      "metadata": {},
      "source": [
        "## 9. Visualization\n",
        "\n",
        "Finally, `nnterp` provides visualization utilities for analyzing model probabilities and prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09369de6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nnterp.display import plot_topk_tokens, prompts_to_df\n",
        "\n",
        "probs = logit_lens(demo_model, prompts_str[0])\n",
        "# Visualize top tokens from logit lens\n",
        "plot_topk_tokens(\n",
        "    probs[0],\n",
        "    demo_model.tokenizer,\n",
        "    k=5,\n",
        "    width=1000,\n",
        "    height=1000,\n",
        "    title=\"Top 5 tokens at each layer for 'The translation of 'car' in French is\",\n",
        ")\n",
        "\n",
        "# Convert prompts to DataFrame for analysis\n",
        "df = prompts_to_df(prompts, demo_model.tokenizer)\n",
        "print(\"\\nPrompts DataFrame:\")\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "337c0673",
      "metadata": {},
      "source": [
        "# Advanced usage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5d6c6a9",
      "metadata": {},
      "source": [
        "Sometime, your model might not be supported yet by nnterp. In this case, you'll be able to use a `RenameConfig` to properly initialize your model.\n",
        "\n",
        "In this section, I'll show you the steps I took to add support for the `gpt2` to `nnterp`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1deded2",
      "metadata": {},
      "source": [
        "###  Renaming a module not automatically renamed\n",
        "\n",
        "Let's say that you load a `gpt2` model that is a bit special: every module is called \"super_module\" instead of \"module\".\n",
        "\n",
        "First, let's build such a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d8af2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "for layer in model.transformer.h:\n",
        "    layer.super_mlp = layer.mlp\n",
        "    delattr(layer, \"mlp\")\n",
        "    layer.super_attn = layer.attn\n",
        "    delattr(layer, \"attn\")\n",
        "model.transformer.super_h = model.transformer.h\n",
        "delattr(model.transformer, \"h\")\n",
        "# Let's keep the final layer norm as is\n",
        "# model.transformer.super_ln_f = model.transformer.ln_f\n",
        "# delattr(model.transformer, \"ln_f\")\n",
        "model.super_transformer = model.transformer\n",
        "delattr(model, \"transformer\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35a9399b",
      "metadata": {},
      "source": [
        "now if we try to use nnterp, the renaming check automatically performed will fail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e618552",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nnterp import StandardizedTransformer\n",
        "from traceback import print_exc\n",
        "\n",
        "try:\n",
        "    StandardizedTransformer(model)\n",
        "except Exception as e:\n",
        "    print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4718b85b",
      "metadata": {},
      "source": [
        "`nnterp` can't find the layers because they're located under `super_transformer`, that nnterp doesn't know about. We have 2 choices in this case:\n",
        "1. Rename `super_transformer` to `model` and `super_h` to `layers` such that it matches the `model.model.layers` Llama architecture and let `nnterp` do the rest.\n",
        "2. Rename `super_transformer.super_h` directly to `layers`, matching the StandardizedTransformer architecture.\n",
        "\n",
        "Let's try the second option first. And let's not forget that we still need to rename\n",
        "\n",
        "In order to do that we can instantiate a `StandardizedTransformer` with a `RenameConfig` with the correct aliases provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57a33bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nnterp.rename_utils import RenameConfig\n",
        "\n",
        "rename_cfg = RenameConfig(\n",
        "    layers_name=\"super_transformer.super_h\",\n",
        "    attn_name=\"super_attn\",\n",
        "    mlp_name=\"super_mlp\",\n",
        ")\n",
        "try:\n",
        "    StandardizedTransformer(model, rename_config=rename_cfg)\n",
        "except Exception as e:\n",
        "    print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a71b0a52",
      "metadata": {},
      "source": [
        "We're still getting an error because `nnterp` doesn't find the `ln_f`. This is because `nnterp` will automatically rename the `ln_f` to `ln_final`, but fails to rename `model.ln_final` to `ln_final`. Again, we can either rename `super_transformer` to `model` or directly rename `super_transformer.ln_f` to `ln_final`.\n",
        "\n",
        "⚠️ The code will still fail, because our \"super_gpt2\" model can't run its forward pass as we deleted its modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b86b02",
      "metadata": {},
      "outputs": [],
      "source": [
        "rename_cfg = RenameConfig(\n",
        "    model_name=\"super_transformer\",\n",
        "    layers_name=\"super_h\",\n",
        "    attn_name=\"super_attn\",\n",
        "    mlp_name=\"super_mlp\",\n",
        "    ln_final_name=\"super_transformer.ln_f\",\n",
        ")\n",
        "from transformers import AutoConfig\n",
        "\n",
        "try:\n",
        "    StandardizedTransformer(\n",
        "        model, rename_config=rename_cfg, config=AutoConfig.from_pretrained(\"gpt2\")\n",
        "    )\n",
        "except Exception as e:\n",
        "    print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f99d41",
      "metadata": {},
      "source": [
        "## Adding attention probabilities support"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "076d4d81",
      "metadata": {},
      "source": [
        "To access the attention probabilities, `nnterp` uses the `NNsight` ability to hook on most intermediate variables of the forward pass. This is very architecture dependent, as even 2 equivalent models, if they use different names for the intermediate variables, will need different hooks.\n",
        "\n",
        "As I'm writing this tutorial, I'm adding support for attention probabilities for `GPTJ` models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5bf6f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nnterp import StandardizedTransformer\n",
        "\n",
        "gptj = StandardizedTransformer(\n",
        "    \"yujiepan/gptj-tiny-random\"\n",
        ")  # In the current version of nnterp, this will work out of the box"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88e1b5d5",
      "metadata": {},
      "source": [
        "As you can see, when you load a model,`nnterp` will automatically test if the attention probabilities hook is working and returns a tensor of shape `(batch_size, num_heads, seq_len, seq_len)` where the last dimension sums to 1. In this case, the test failed and `nnterp` logs the error.\n",
        "\n",
        "Now let's look at the `yujiepan/gptj-tiny-random` forward pass and try to understand where are the attention probabilities computed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc7148a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nnterp.utils import display_source\n",
        "\n",
        "display_source(gptj.attentions[0].source)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a3755d0",
      "metadata": {},
      "source": [
        "Lines 60-61:\n",
        "```py\n",
        "                                60     # compute self-attention: V x Softmax(QK^T)\n",
        " self__attn_0                -> 61     attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
        " ```\n",
        "⚠️ Be careful! if you set the hook here, you'll be able to successfully access the attention probabilities, but not to edit them! ⚠️\n",
        "\n",
        "We need to check the source of `self__attn_0` to see where `attn_weights` is used. In order to access a deeper variable like this, we have to actually run the model with `trace` or `scan`. I'd advise to start with `scan` first, but switch to `trace` if you encounter an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b91759",
      "metadata": {},
      "outputs": [],
      "source": [
        "with gptj.scan(\"a\"):\n",
        "    display_source(gptj.attentions[0].source.self__attn_0.source)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d662d5c4",
      "metadata": {},
      "source": [
        "Here, line 20-24:\n",
        "```py\n",
        " self_attn_dropout_0     -> 20     attn_weights = self.attn_dropout(attn_weights)\n",
        "                            21\n",
        "                            22     # Mask heads if we want to\n",
        "                            23     if head_mask is not None:\n",
        "                            24         attn_weights = attn_weights * head_mask\n",
        "```\n",
        "\n",
        "In the current `NNsight` version, the results of operators like `*` are not hooked. But even if they were, I'd be careful to use line 24 here, as it's inside a `if` statement. Therefore, we'll use `self_attn_dropout_0` instead.\n",
        "\n",
        "Note that we could also look at `torch_matmul_1` input and edit the value here. However, this looks less robust to me as it assumes this is the only place where `attn_weights` is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402b7798",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch as th\n",
        "\n",
        "with gptj.scan(th.tensor([[1, 2, 3]])):\n",
        "    print(\n",
        "        gptj.attentions[0].source.self__attn_0.source.self_attn_dropout_0.output.shape\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f580ca",
      "metadata": {},
      "source": [
        "Nice! The shape looks good. Now we can initialize our model with the right RenameConfig, and let `nnterp` run the tests for us.\n",
        "\n",
        "To do this, we'll need to create a `AttnProbFunction` and implement the `get_attention_prob_source` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d8ffe15",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nnterp.rename_utils import AttnProbFunction, RenameConfig\n",
        "\n",
        "\n",
        "class GPTJAttnProbFunction(AttnProbFunction):\n",
        "\n",
        "    def get_attention_prob_source(\n",
        "        self, attention_module, return_module_source: bool = False\n",
        "    ):\n",
        "        if return_module_source:\n",
        "            # in this case, return source of the module from where the attention probabilities are computed\n",
        "            return attention_module.source.self__attn_0.source\n",
        "        else:\n",
        "            # in this case, return the attention probabilities hook\n",
        "            return attention_module.source.self__attn_0.source.self_attn_dropout_0\n",
        "\n",
        "\n",
        "gptj = StandardizedTransformer(\n",
        "    \"yujiepan/gptj-tiny-random\",\n",
        "    rename_config=RenameConfig(attn_prob_source=GPTJAttnProbFunction()),\n",
        ")\n",
        "\n",
        "with gptj.trace(\"Hello world!\"):\n",
        "    batch_size, seq_len = gptj.input_size\n",
        "    attn_probs = gptj.attention_probabilities[0].save()\n",
        "    print(f\"attn_probs.shape: {attn_probs.shape}\")\n",
        "    assert attn_probs.shape == (batch_size, gptj.num_heads, seq_len, seq_len)\n",
        "    gptj.attention_probabilities[0] = attn_probs / 2\n",
        "    corrupt_logits = gptj.logits.save()\n",
        "\n",
        "with gptj.trace(\"Hello world!\"):\n",
        "    clean_logits = gptj.logits.save()\n",
        "\n",
        "assert gptj.attention_probabilities.enabled\n",
        "assert not th.allclose(clean_logits, corrupt_logits)\n",
        "summed_attn_probs = attn_probs.sum(dim=-1)\n",
        "assert th.allclose(summed_attn_probs, th.ones_like(summed_attn_probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "874995da",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "`nnterp` provides a unified, standardized interface for working with transformer models, built on top of `nnsight`. Key features include:\n",
        "\n",
        "1. **Standardized naming** across all transformer architectures\n",
        "2. **Easy access** to layer/attention/MLP inputs and outputs\n",
        "3. **Built-in methods** for common operations (steering, skipping layers, projecting to vocab)\n",
        "4. **Efficient activation collection** with batching support\n",
        "5. **Prompt utilities** for tracking target tokens\n",
        "6. **Intervention methods** from mechanistic interpretability research\n",
        "7. **Visualization tools** for analyzing model behavior\n",
        "\n",
        "All of this while maintaining the full power and flexibility of `nnsight` under the hood!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc4bcc7",
      "metadata": {},
      "source": [
        "# Appendix: `NNsight` cheatsheet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6300a0e7",
      "metadata": {},
      "source": [
        "## 1) You must execute your interventions in order\n",
        "In the new `NNsight` versions, it is enforced that you must access to model internals *in the same order* as the model execute them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cec3d994",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nnterp import StandardizedTransformer\n",
        "from traceback import print_exc\n",
        "\n",
        "nnterp_gpt2 = StandardizedTransformer(\"gpt2\")\n",
        "try:\n",
        "    with nnterp_gpt2.trace(\"My tailor is rich\"):\n",
        "        l2 = nnterp_gpt2.layers_output[2]\n",
        "        l1 = nnterp_gpt2.layers_output[1]  # will fail! You need to collect l1 before l2\n",
        "except Exception as e:\n",
        "    print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2255426b",
      "metadata": {},
      "source": [
        "## 2) Gradient computation\n",
        "To compute gradients, you need to open a `.backward()` context, and save the gradients *inside it*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bac27d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "with nnterp_gpt2.trace(\"My tailor is rich\"):\n",
        "    l1_out = nnterp_gpt2.layers_output[1]  # get l1 before accessing logits\n",
        "    logits = nnterp_gpt2.output.logits\n",
        "    with logits.sum().backward(\n",
        "        retain_graph=True\n",
        "    ):  # use retain_graph if you want to do multiple backprops\n",
        "        if False:\n",
        "            l1_grad = nnterp_gpt2.layers_output[1].grad.save()\n",
        "            # this would fail as we'd access nnterp_gpt2.layers_output[1] after nnterp_gpt2.output\n",
        "        l1_grad = l1_out.grad.save()\n",
        "    with (logits.sum() ** 2).backward():\n",
        "        l1_grad_2 = l1_out.grad.save()\n",
        "\n",
        "assert not th.allclose(l1_grad, l1_grad_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7809aa53",
      "metadata": {},
      "source": [
        "## 3) Use tracer.stop() to save useless computations\n",
        "If you're just computing activations, don't forget to call `tracer.stop()` at the end of your trace. This will stop the model from executing the rest of its computations, and save you some time, as demonstrated below (with the contribution of Claude 4 Sonnet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ac14c2",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "print(\n",
        "    \"🎭 Welcome to the Theatrical Performance Comparison! 🎭\\n\"\n",
        "    + \"=\" * 60\n",
        "    + \"\\n\\n🐌 ACT I: 'The Tragedy of the Unstoppable Tracer' 🐌\\nIn which our hero forgets to call tracer.stop()...\"\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "for _ in range(30):\n",
        "    with nnterp_gpt2.trace([\"Neel Samba\", \"Chris Aloha\"]):\n",
        "        out5 = nnterp_gpt2.layers_output[5].save()\n",
        "end_time = time.time()\n",
        "nostop_time = end_time - start_time\n",
        "\n",
        "print(\n",
        "    f\"⏰ Duration of suffering: {nostop_time:.4f} seconds\\n\\n⚡ ACT II: 'The Redemption of the Stopped Tracer' ⚡\\nOur hero learns the ancient art of tracer.stop()...\"\n",
        ")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "for _ in range(30):\n",
        "    with nnterp_gpt2.trace([\"Neel Samba\", \"Chris Aloha\"]) as tracer:\n",
        "        out5 = nnterp_gpt2.layers_output[5].save()\n",
        "        tracer.stop()\n",
        "end_time = time.time()\n",
        "stop_time = end_time - start_time\n",
        "\n",
        "print(f\"⏰ Duration of enlightenment: {stop_time:.4f} seconds\")\n",
        "\n",
        "speedup = nostop_time / stop_time\n",
        "time_saved = nostop_time - stop_time\n",
        "\n",
        "# fun display\n",
        "print(\"\\n\" + \"=\" * 60 + \"\\n🎉 THE GRAND RESULTS SPECTACULAR! 🎉\\n\" + \"=\" * 60)\n",
        "results_df = pd.DataFrame(\n",
        "    {\n",
        "        \"🎭 Performance Type\": [\n",
        "            \"Without tracer.stop() 🐌\",\n",
        "            \"With tracer.stop() ⚡\",\n",
        "            \"Time Saved 💰\",\n",
        "        ],\n",
        "        \"⏱️ Time (seconds)\": [\n",
        "            f\"{nostop_time:.4f}\",\n",
        "            f\"{stop_time:.4f}\",\n",
        "            f\"{time_saved:.4f}\",\n",
        "        ],\n",
        "        \"🎯 Rating\": [\"Tragic 😭\", \"Magnificent! 🌟\", \"PROFIT! 📈\"],\n",
        "    }\n",
        ")\n",
        "display(results_df)\n",
        "speedup_bars = int(speedup * 10)\n",
        "meter = \"█\" * min(speedup_bars, 48) + \"░\" * (50 - min(speedup_bars, 48))\n",
        "print(\n",
        "    f\"\\n🏎️ SPEEDUP METER 🏎️\\n┌{'─' * 50}┐\\n│{meter}│\\n└{'─' * 50}┘\\n   💫 COSMIC SPEEDUP: {speedup:.2f}x FASTER! 💫\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6008bf2",
      "metadata": {},
      "source": [
        "## 4) Using NNsight builtin cache to collect activations\n",
        "\n",
        "`NNsight 0.5` introduces a builtin way to cache activations during the forward pass. Be careful not to call `tracer.stop()` before all the module of the cache have been accessed.\n",
        "\n",
        "The cache supports both renamed and original module names. You can access cached activations using attribute notation or dictionary keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc6ddb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "with nnterp_gpt2.trace(\"Hello\") as tracer:\n",
        "    cache = tracer.cache(modules=[layer for layer in nnterp_gpt2.layers[::2]]).save()\n",
        "\n",
        "# Access with renamed names using attribute notation\n",
        "print(cache.model.layers[10].output)\n",
        "# Or using dictionary syntax with renamed path\n",
        "print(cache[\"model.layers.10\"].output)\n",
        "# Original names still work\n",
        "print(cache[\"model.transformer.h.10\"].output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
