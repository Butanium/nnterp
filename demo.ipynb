{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca352b0",
   "metadata": {},
   "source": [
    "# Demo: nnterp Features Showcase\n",
    "\n",
    "This notebook demonstrates the key features of `nnterp`, which aims to offer a unified interface for all transformer models and give best `NNsight` practices for LLMs in everyone's hands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbfd064",
   "metadata": {},
   "source": [
    "## 1. Standardized Interface\n",
    "\n",
    "Similar to [`transformer_lens`](https://github.com/TransformerLensOrg/TransformerLens), `nnterp` provides a standardized interface for all transformer models.\n",
    "The main difference is that `nnterp` still uses the huggingface implementation under the hood through `NNsight`, while transformer_lens uses its own implementation of the transformer architecture. However, each transformer implementation has its own quirks, such that `transformer_lens` is not able to support all models, and can sometimes have significant difference with the huggingface implementation.\n",
    "\n",
    "The way it's implemented is based on the `NNsight` built-in renaming feature, to make all models look like the llama naming convention, without having to write `model.model`, namely:\n",
    "```ocaml\n",
    "StandardizedTransformer\n",
    "├── layers\n",
    "│   ├── self_attn\n",
    "│   └── mlp\n",
    "├── norm\n",
    "└── lm_head\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0578967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 64, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=64, out_features=256, bias=False)\n",
      "          (up_proj): Linear(in_features=64, out_features=256, bias=False)\n",
      "          (down_proj): Linear(in_features=256, out_features=64, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((64,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((64,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((64,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=64, out_features=32000, bias=False)\n",
      ")\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "print(AutoModelForCausalLM.from_pretrained(\"Maykeye/TinyLLama-v0\"))\n",
    "print(AutoModelForCausalLM.from_pretrained(\"gpt2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ac8fd",
   "metadata": {},
   "source": [
    "As you can see, the naming scheme of gpt2 is different from the llama naming convention.\n",
    "A simple way to fix this is to use the `rename` feature of `NNsight` to rename the gpt2 modules to the llama naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d832453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (model/transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (layers/h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn/attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm/ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (generator): WrapperModule()\n",
      ")\n",
      "GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel(\n",
    "    \"gpt2\", rename=dict(transformer=\"model\", h=\"layers\", ln_f=\"norm\", attn=\"self_attn\")\n",
    ")\n",
    "print(model)\n",
    "# Access the attn module as if it was a llama model\n",
    "print(model.model.layers[0].self_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94624f4",
   "metadata": {},
   "source": [
    "You can see the that renamed modules are displayed like `(new_name)/old_name`. However, many models family have their own naming convention, `nnterp` has a global renaming scheme that should transform any model to the llama naming convention. The easiest way to use it is to load your model using the `StandardizedTransformer` class that inherits from `nnsight.LanguageModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e2a9a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-07 17:18:26.564\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m61\u001b[0m - \u001b[33m\u001b[1mnnterp was not tested with Transformers version 4.53.0. Closest below: 0.4.53, closest above: None\n",
      "This is most likely okay, but you may want to at least check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()`. It is recommended to switch to None or 0.4.53 if possible or:\n",
      "  - run the nnterp tests with your version of transformers to ensure everything works as expected.\n",
      "  - check if the attention probabilities hook makes sense before using them by calling `model.attention_probabilities.print_source()` (prettier in a notebook).\u001b[0m\n",
      "\u001b[32m2025-07-07 17:18:26.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (model/transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (layers/h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn/attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm/ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (generator): WrapperModule()\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attn/attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "\n",
    "# You will see the `layers` module printed two times, it'll be explained later.\n",
    "nnterp_gpt2 = StandardizedTransformer(\"gpt2\")\n",
    "print(nnterp_gpt2)\n",
    "# StandardizedTransformer also use `device_map=\"auto\"` by default:\n",
    "nnterp_gpt2.dispatch()\n",
    "print(nnterp_gpt2.model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef78e8",
   "metadata": {},
   "source": [
    "Great! But I can see you at the back of the classroom, asking yourself:\n",
    "> \"Why would you create a package that just pass the right dict to the `NNsight` `rename` feature?\"\n",
    "\n",
    "And actually, I'm glad you asked! `StandardizedTransformer` and `nnterp` have a lot of other features, so bear with me!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc460297",
   "metadata": {},
   "source": [
    "## 2. Accessing Modules I/O\n",
    "With `NNsight`, the most robust way to set the residual stream after layer 1 to be the residual stream after layer 0 for a LLama-like model would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d78c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "llama = LanguageModel(\"Maykeye/TinyLLama-v0\")\n",
    "with llama.trace(\"hello\"):\n",
    "    llama.model.layers[1].output = (llama.model.layers[0].output[0], *llama.model.layers[1].output[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2356c88",
   "metadata": {},
   "source": [
    "Note that the following can cause issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e6f0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "with llama.trace(\"hello\"):\n",
    "    # can't do this because .output is a tuple\n",
    "    # llama.model.layers[1].output[0] = llama.model.layers[0].output[0]\n",
    "\n",
    "    # Can cause errors with gradient computation\n",
    "    llama.model.layers[1].output[0][:] = llama.model.layers[0].output[0]\n",
    "\n",
    "with llama.trace(\"hello\"):\n",
    "    # Can cause errors with opt if you do this at its last layer (thanks pytest)\n",
    "    llama.model.layers[1].output = (llama.model.layers[0].output[0], )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0744061",
   "metadata": {},
   "source": [
    "`nnterp` makes this much cleaner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22034aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, you can access layer inputs and outputs directly:\n",
    "with nnterp_gpt2.trace(\"hello\"):\n",
    "    # Access layer 5's output\n",
    "    layer_5_output = nnterp_gpt2.layers_output[5]\n",
    "    # Set layer 10's output to be layer 5's output\n",
    "    nnterp_gpt2.layers_output[10] = layer_5_output\n",
    "\n",
    "# You can also access attention and MLP outputs:\n",
    "with nnterp_gpt2.trace(\"hello\"):\n",
    "    attn_output = nnterp_gpt2.attentions_output[3]\n",
    "    mlp_output = nnterp_gpt2.mlps_output[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8a52d6",
   "metadata": {},
   "source": [
    "## 3. Builtin interventions\n",
    "\n",
    "`StandardizedTransformer` also provides convenient methods for common operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee31ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "# Project hidden states to vocabulary using the unembed norm and lm_head\n",
    "with nnterp_gpt2.trace(\"The capital of France is\"):\n",
    "    hidden = nnterp_gpt2.layers_output[5]\n",
    "    logits = nnterp_gpt2.project_on_vocab(hidden)\n",
    "\n",
    "# Skip layers entirely\n",
    "with nnterp_gpt2.trace(\"Hello world\"):\n",
    "    # Skip layer 1\n",
    "    nnterp_gpt2.skip_layer(1)\n",
    "    # Skip layers 2 through 3 (inclusive)\n",
    "    nnterp_gpt2.skip_layers(2, 3)\n",
    "\n",
    "# This is useful if you want to start at a later layer than the first one\n",
    "with nnterp_gpt2.trace(\"Hello world\") as tracer:\n",
    "    layer_6_out = nnterp_gpt2.layers_output[6].save()\n",
    "    tracer.stop()  # avoid computations after layer 6\n",
    "\n",
    "with nnterp_gpt2.trace(\"Hello world\"):\n",
    "    nnterp_gpt2.skip_layers(0, 6, skip_with=layer_6_out)\n",
    "    half_half_logits = nnterp_gpt2.logits.save()\n",
    "\n",
    "with nnterp_gpt2.trace(\"Hello world\"):\n",
    "    vanilla_logits = nnterp_gpt2.logits.save()\n",
    "\n",
    "assert th.allclose(vanilla_logits, half_half_logits)  # they should be the same\n",
    "\n",
    "# Direct steering\n",
    "steering_vector = th.randn(768)  # gpt2 hidden size\n",
    "with nnterp_gpt2.trace(\"The weather today is\"):\n",
    "    nnterp_gpt2.steer(layers=[1, 3], steering_vector=steering_vector, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f97e9",
   "metadata": {},
   "source": [
    "## 4. Attention Probabilities\n",
    "\n",
    "For models that support it, you can access attention probabilities directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94acbf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention probs shape will be: (batch, heads, seq_len, seq_len): torch.Size([1, 12, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "nnterp_gpt2.tokenizer.padding_side = \"left\"  # ensure left padding for easy access to the first token\n",
    "\n",
    "with th.no_grad():\n",
    "    with nnterp_gpt2.trace(\"The cat sat on the mat\"):\n",
    "        # Access attention probabilities for layer 5\n",
    "        attn_probs_l2 = nnterp_gpt2.attention_probabilities[2].save()\n",
    "        attn_probs = nnterp_gpt2.attention_probabilities[5].save()\n",
    "        print(\n",
    "            f\"Attention probs shape will be: (batch, heads, seq_len, seq_len): {attn_probs.shape}\"\n",
    "        )\n",
    "        # knock out the attention to the first token\n",
    "        attn_probs[:, :, :, 0] = 0\n",
    "        attn_probs /= attn_probs.sum(\n",
    "            dim=-1, keepdim=True\n",
    "        )\n",
    "        corr_logits = nnterp_gpt2.logits.save()\n",
    "    with nnterp_gpt2.trace(\"The cat sat on the mat\"):\n",
    "        baseline_logits = nnterp_gpt2.logits.save()\n",
    "\n",
    "assert not th.allclose(corr_logits, baseline_logits)\n",
    "\n",
    "sums = attn_probs_l2.sum(dim=-1)\n",
    "assert th.allclose(sums, th.ones_like(sums))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ed199",
   "metadata": {},
   "source": [
    "Under the hood this uses the new tracing system implemented in `NNsight v0.5` which allow to access most model intermediate variables during the forward pass. This means that if the `transformers` implementation were to change, this could break or give unexpected results, so it is recommended to use one of the tested versions of `transformers` and to check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()` if you want to use a different version of `transformers` / a architecture that has not been tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c19ce0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Accessing attention probabilities from:\n",
       "```py\n",
       "model.transformer.h.0.attn.attention_interface_0.module_attn_dropout_0:\n",
       "\n",
       "    ....\n",
       "            attn_weights = attn_weights + causal_mask\n",
       "    \n",
       "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "    \n",
       "        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
       "        attn_weights = attn_weights.type(value.dtype)\n",
       "    --> attn_weights = module.attn_dropout(attn_weights) <--\n",
       "    \n",
       "        # Mask heads if we want to\n",
       "        if head_mask is not None:\n",
       "            attn_weights = attn_weights * head_mask\n",
       "    \n",
       "        attn_output = torch.matmul(attn_weights, value)\n",
       "    ....\n",
       "```\n",
       "\n",
       "## Full module source:\n",
       "```py\n",
       "                             * def eager_attention_forward(module, query, key, value, attention_mask, head_mask=None, **kwargs):\n",
       " key_transpose_0         ->  0     attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
       " torch_matmul_0          ->  +     ...\n",
       "                             1 \n",
       "                             2     if module.scale_attn_weights:\n",
       " torch_full_0            ->  3         attn_weights = attn_weights / torch.full(\n",
       " value_size_0            ->  4             [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n",
       "                             5         )\n",
       "                             6 \n",
       "                             7     # Layer-wise attention scaling\n",
       "                             8     if module.scale_attn_by_inverse_layer_idx:\n",
       " float_0                 ->  9         attn_weights = attn_weights / float(module.layer_idx + 1)\n",
       "                            10 \n",
       "                            11     if not module.is_cross_attention:\n",
       "                            12         # if only \"normal\" attention layer implements causal mask\n",
       " query_size_0            -> 13         query_length, key_length = query.size(-2), key.size(-2)\n",
       " key_size_0              ->  +         ...\n",
       "                            14         causal_mask = module.bias[:, :, key_length - query_length : key_length, :key_length]\n",
       " torch_finfo_0           -> 15         mask_value = torch.finfo(attn_weights.dtype).min\n",
       "                            16         # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
       "                            17         # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
       " torch_full_1            -> 18         mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
       " attn_weights_to_0       -> 19         attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
       " torch_where_0           ->  +         ...\n",
       "                            20 \n",
       "                            21     if attention_mask is not None:\n",
       "                            22         # Apply the attention mask\n",
       "                            23         causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n",
       "                            24         attn_weights = attn_weights + causal_mask\n",
       "                            25 \n",
       " nn_functional_softmax_0 -> 26     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                            27 \n",
       "                            28     # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
       " attn_weights_type_0     -> 29     attn_weights = attn_weights.type(value.dtype)\n",
       " module_attn_dropout_0   -> 30     attn_weights = module.attn_dropout(attn_weights)\n",
       "                            31 \n",
       "                            32     # Mask heads if we want to\n",
       "                            33     if head_mask is not None:\n",
       "                            34         attn_weights = attn_weights * head_mask\n",
       "                            35 \n",
       " torch_matmul_1          -> 36     attn_output = torch.matmul(attn_weights, value)\n",
       " attn_output_transpose_0 -> 37     attn_output = attn_output.transpose(1, 2)\n",
       "                            38 \n",
       "                            39     return attn_output, attn_weights\n",
       "                            40 \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nnterp_gpt2.attention_probabilities.print_source()  # pretty markdown display in a notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cebf2",
   "metadata": {},
   "source": [
    "## 5. Activation Collection\n",
    "\n",
    "`nnterp` provides utilities for collecting activations efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaad73ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched activations shape: torch.Size([3, 100, 768])\n"
     ]
    }
   ],
   "source": [
    "from nnterp.nnsight_utils import (\n",
    "    get_token_activations,\n",
    "    collect_token_activations_batched,\n",
    ")\n",
    "\n",
    "# Collect activations for specific tokens\n",
    "prompts = [\"The capital of France is\", \"The weather today is\"]\n",
    "with nnterp_gpt2.trace(prompts) as tracer:\n",
    "    # Get last token activations for all layers\n",
    "    activations = get_token_activations(nnterp_gpt2, prompts, idx=-1, tracer=tracer)\n",
    "    # activations shape: (num_layers, batch_size, hidden_size)\n",
    "\n",
    "# For large datasets, use batched collection\n",
    "large_prompts = [\"Sample text \" + str(i) for i in range(100)]\n",
    "batch_activations = collect_token_activations_batched(\n",
    "    nnterp_gpt2,\n",
    "    large_prompts,\n",
    "    batch_size=16,\n",
    "    layers=[3, 9, 11],  # Only collect specific layers, default is all layers\n",
    "    idx=-1,  # Last token (default)\n",
    ")\n",
    "print(f\"Batched activations shape: {batch_activations.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4526f117",
   "metadata": {},
   "source": [
    "## 6. Prompt Utilities\n",
    "\n",
    "`nnterp` provides utilities for working with prompts and tracking probabilities of first tokens of certain strings. It tracks both the first token of \"string\" and \" string\".\n",
    "\n",
    "You can provide multiple string per category, the probabilities returned will be the sum of the probabilities of all the first tokens of the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62579129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: ['Paris', 'ĠParis']\n",
      "traps: ['London', 'ĠLondon', 'Mad', 'ĠMadrid']\n",
      "longstring: ['the', 'Ġthe']\n",
      "target: ['J', 'ĠJupiter']\n",
      "traps: ['Earth', 'ĠEarth', 'Ne', 'ĠNeptune']\n",
      "longstring: ['Pal', 'ĠPalace']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b98438cc9543c481f7c3f86bb66f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target token probabilities:\n",
      "  target: shape torch.Size([2, 1])\n",
      "  traps: shape torch.Size([2, 1])\n",
      "  longstring: shape torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "from nnterp.prompt_utils import Prompt, run_prompts\n",
    "\n",
    "# Create prompts with target tokens to track\n",
    "prompt1 = Prompt.from_strings(\n",
    "    \"The capital of France (not England or Spain) is\",\n",
    "    {\n",
    "        \"target\": \"Paris\",\n",
    "        \"traps\": [\"London\", \"Madrid\"],\n",
    "        \"longstring\": \"the country of France\",\n",
    "    },\n",
    "    nnterp_gpt2.tokenizer,\n",
    ")\n",
    "for name, tokens in prompt1.target_tokens.items():\n",
    "    print(f\"{name}: {nnterp_gpt2.tokenizer.convert_ids_to_tokens(tokens)}\")\n",
    "\n",
    "prompt2 = Prompt.from_strings(\n",
    "    \"The largest planet (not Earth or Neptune) is\",\n",
    "    {\"target\": \"Jupiter\", \"traps\": [\"Earth\", \"Neptune\"], \"longstring\": \"Palace planet\"},\n",
    "    nnterp_gpt2.tokenizer,\n",
    ")\n",
    "for name, tokens in prompt2.target_tokens.items():\n",
    "    print(f\"{name}: {nnterp_gpt2.tokenizer.convert_ids_to_tokens(tokens)}\")\n",
    "\n",
    "# Run prompts and get probabilities for target tokens\n",
    "results = run_prompts(nnterp_gpt2, [prompt1, prompt2], batch_size=2)\n",
    "print(\"Target token probabilities:\")\n",
    "for target, probs in results.items():\n",
    "    print(f\"  {target}: shape {probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1508ed7",
   "metadata": {},
   "source": [
    "## 7. Interventions\n",
    "\n",
    "`nnterp` provides several intervention methods inspired by mechanistic interpretability research:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3915a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit lens output shape: torch.Size([2, 12, 50257])\n",
      "repeat_prompt: TargetPrompt(prompt='car:car\\n\\ncross:cross\\n\\nazdrfa:azdrfa\\n\\n*', index_to_patch=-1)\n",
      "custom_repeat_prompt: TargetPrompt(prompt='car:car\\n\\ncross:cross\\n\\nazdrfa:azdrfa\\n\\n*', index_to_patch=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patchscope_probs: torch.Size([2, 12, 50257])\n"
     ]
    }
   ],
   "source": [
    "from nnterp.interventions import (\n",
    "    logit_lens,\n",
    "    patchscope_lens,\n",
    "    TargetPrompt,\n",
    "    repeat_prompt,\n",
    "    steer,\n",
    ")\n",
    "\n",
    "# Logit Lens: See predictions at each layer\n",
    "prompts = [\"The capital of France is\", \"The sun rises in the\"]\n",
    "probs = logit_lens(nnterp_gpt2, prompts)\n",
    "print(f\"Logit lens output shape: {probs.shape}\")  # (batch, layers, vocab)\n",
    "\n",
    "# Patchscope: Replace activations from one context into another\n",
    "source_prompts = [\"Paris is beautiful\", \"London is foggy\"]\n",
    "custom_target_prompt = TargetPrompt(\"city: Paris\\nfood: croissant\\n?\", -1)\n",
    "target_prompt = repeat_prompt()  # Creates a repetition task\n",
    "custom_repeat_prompt = repeat_prompt(\n",
    "    words=[\"car\", \"cross\", \"azdrfa\"],\n",
    "    rel=\":\",\n",
    "    sep=\"\\n\\n\",\n",
    "    placeholder=\"*\",\n",
    ")\n",
    "print(f\"repeat_prompt: {custom_repeat_prompt}\")\n",
    "print(f\"custom_repeat_prompt: {custom_repeat_prompt}\")\n",
    "patchscope_probs = patchscope_lens(\n",
    "    nnterp_gpt2, source_prompts=source_prompts, target_patch_prompts=target_prompt\n",
    ")\n",
    "print(f\"patchscope_probs: {patchscope_probs.shape}\")\n",
    "\n",
    "# Steering with intervention function\n",
    "with nnterp_gpt2.trace(\"The weather is\"):\n",
    "    steer(nnterp_gpt2, layers=[5, 10], steering_vector=steering_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aac551",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "You can use a combination of run_prompts and interventions to get the probabilities of certain tokens according to your custom intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f496146",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-07 17:18:33.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
      "\u001b[32m2025-07-07 17:18:35.428\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m208\u001b[0m - \u001b[33m\u001b[1mgoogle/gemma-2-2b's architecture is not tested. This may cause unexpected behavior. It is recommended to check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()` if you plan on using it (prettier in a notebook).\n",
      "Feel free to open an issue on github (https://github.com/butanium/nnterp/issues) or run the tests yourself with a toy model if you want to add test coverage for this model.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0882134d3ab47c6bab1cd59fda6bf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e1ca97751e4d1b94e497c246d10908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: torch.Size([2, 26])\n",
      "english: torch.Size([2, 26])\n",
      "format: torch.Size([2, 26])\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "target",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25
         ],
         "y": [
          0,
          0,
          1.0477929007295955e-39,
          3.3572008169903216e-37,
          3.3725472632050863e-25,
          3.6082769559333792e-37,
          5.974479717547129e-23,
          7.064204322083499e-18,
          2.661365939744436e-17,
          1.1582176732451155e-13,
          7.272823937868027e-16,
          5.24414403029372e-16,
          2.400891383828028e-15,
          8.364434091103629e-14,
          1.771075473777639e-11,
          3.0354375697011493e-13,
          3.620844596374795e-11,
          2.011593203208456e-13,
          3.741373327142451e-15,
          2.67894092326193e-11,
          1.154006225431367e-13,
          1.9552523662647037e-13,
          8.380037982980149e-12,
          4.287754979087448e-14,
          1.202039336123395e-12,
          0.0002228509692940861
         ]
        },
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "english",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25
         ],
         "y": [
          0,
          1.401298464324817e-45,
          4.751985777003652e-36,
          1.249700094947333e-33,
          1.2111213844915835e-25,
          1.113362701513488e-34,
          4.810917581628571e-22,
          2.584157728549349e-18,
          4.2512577005248434e-17,
          9.211833125997732e-15,
          2.55039481660078e-15,
          1.8504258604660294e-17,
          9.279078019506512e-18,
          4.034329497532539e-14,
          1.0548329570925219e-11,
          8.504329375045791e-14,
          5.178877882094923e-10,
          6.9792739296192785e-12,
          5.079090925619312e-8,
          0.8435060977935791,
          0.9909697771072388,
          0.702079176902771,
          0.2419319450855255,
          2.0453693139188545e-7,
          4.922426910525246e-9,
          0.0000037258014344843104
         ]
        },
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "format",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25
         ],
         "y": [
          2.4922373447709737e-40,
          3.3020952868270977e-32,
          7.28434178113041e-40,
          1.864506922709312e-29,
          2.5811363221150282e-20,
          2.2197348907458836e-28,
          1.914033071721152e-18,
          5.4384453415808574e-15,
          1.5282396116818309e-13,
          8.418319210126701e-13,
          9.86899689574272e-13,
          7.13409637354695e-13,
          3.191941719618563e-12,
          7.809057933627628e-8,
          4.6338300307979807e-7,
          0.0000022637991605733987,
          0.00007198070670710877,
          0.005840994417667389,
          0.0015627413522452116,
          0.0005657284636981785,
          0.008743060752749443,
          0.062422383576631546,
          0.13925188779830933,
          0.12920856475830078,
          0.9999398589134216,
          0.9995877146720886
         ]
        }
       ],
       "layout": {
        "hovermode": "x unified",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Mean Token Probabilities Across Layers"
        },
        "xaxis": {
         "title": {
          "text": "Layer"
         }
        },
        "yaxis": {
         "title": {
          "text": "Mean Probability"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_model = StandardizedTransformer(\"google/gemma-2-2b\")\n",
    "# uncomment if you don't have a GPU\n",
    "# demo_model = nnterp_gpt2\n",
    "\n",
    "prompts_str = [\n",
    "    \"The translation of 'car' in French is\",\n",
    "    \"The translation of 'cat' in Spanish is\",\n",
    "]\n",
    "tokens = [\n",
    "    {\"target\": [\"voiture\", \"bagnole\"], \"english\": \"car\", \"format\": \"'\"},\n",
    "    {\"target\": [\"gato\", \"minino\"], \"english\": \"cat\", \"format\": \"'\"},\n",
    "]\n",
    "prompts = [\n",
    "    Prompt.from_strings(prompt, tokens, demo_model.tokenizer)\n",
    "    for prompt, tokens in zip(prompts_str, tokens)\n",
    "]\n",
    "results = run_prompts(demo_model, prompts, batch_size=2, get_probs_func=logit_lens)\n",
    "for category, probs in results.items():\n",
    "    print(f\"{category}: {probs.shape}\")  # (batch, layers)\n",
    "\n",
    "# Create a plotly plot showing mean probabilities for each category across layers\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calculate mean probabilities across batches for each category and layer\n",
    "mean_probs = {category: probs.mean(dim=0) for category, probs in results.items()}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add a line for each category\n",
    "for category, probs in mean_probs.items():\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(probs))),\n",
    "            y=probs.tolist(),\n",
    "            mode=\"lines+markers\",\n",
    "            name=category,\n",
    "            line=dict(width=2),\n",
    "            marker=dict(size=6),\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Mean Token Probabilities Across Layers\",\n",
    "    xaxis_title=\"Layer\",\n",
    "    yaxis_title=\"Mean Probability\",\n",
    "    hovermode=\"x unified\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd285456",
   "metadata": {},
   "source": [
    "## 8. Visualization\n",
    "\n",
    "Finally, `nnterp` provides visualization utilities for analyzing model probabilities and prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04ce6f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorbar": {
          "len": 0.9,
          "thickness": 15,
          "title": {
           "text": "Probability"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(5,48,97)"
          ],
          [
           0.1,
           "rgb(33,102,172)"
          ],
          [
           0.2,
           "rgb(67,147,195)"
          ],
          [
           0.3,
           "rgb(146,197,222)"
          ],
          [
           0.4,
           "rgb(209,229,240)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(253,219,199)"
          ],
          [
           0.7,
           "rgb(244,165,130)"
          ],
          [
           0.8,
           "rgb(214,96,77)"
          ],
          [
           0.9,
           "rgb(178,24,43)"
          ],
          [
           1,
           "rgb(103,0,31)"
          ]
         ],
         "hovertemplate": "Layer: %{y}<br>%{hovertext}<br>Probability: %{z}<extra></extra>",
         "hovertext": [
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 919<br>Token: '▁has'",
           "ID: 2125<br>Token: '▁Is'"
          ],
          [
           "ID: 2<br>Token: '<bos>'",
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 919<br>Token: '▁has'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 2<br>Token: '<bos>'",
           "ID: 708<br>Token: '▁are'",
           "ID: 729<br>Token: '▁was'",
           "ID: 919<br>Token: '▁has'"
          ],
          [
           "ID: 2<br>Token: '<bos>'",
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 919<br>Token: '▁has'",
           "ID: 708<br>Token: '▁are'"
          ],
          [
           "ID: 2<br>Token: '<bos>'",
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 919<br>Token: '▁has'"
          ],
          [
           "ID: 2<br>Token: '<bos>'",
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 780<br>Token: '▁not'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 212549<br>Token: '▁nahilalakip'",
           "ID: 708<br>Token: '▁are'",
           "ID: 780<br>Token: '▁not'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 780<br>Token: '▁not'",
           "ID: 614<br>Token: '▁be'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 212549<br>Token: '▁nahilalakip'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 919<br>Token: '▁has'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 780<br>Token: '▁not'",
           "ID: 28707<br>Token: '▁translated'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 6947<br>Token: '▁sometimes'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 212549<br>Token: '▁nahilalakip'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 708<br>Token: '▁are'",
           "ID: 729<br>Token: '▁was'",
           "ID: 1170<br>Token: '▁also'",
           "ID: 476<br>Token: '▁a'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 708<br>Token: '▁are'",
           "ID: 235248<br>Token: '▁'",
           "ID: 729<br>Token: '▁was'",
           "ID: 476<br>Token: '▁a'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 6574<br>Token: '▁simply'",
           "ID: 780<br>Token: '▁not'",
           "ID: 1170<br>Token: '▁also'",
           "ID: 575<br>Token: '▁in'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 6574<br>Token: '▁simply'",
           "ID: 5250<br>Token: '▁usually'",
           "ID: 1508<br>Token: '▁very'",
           "ID: 1170<br>Token: '▁also'"
          ],
          [
           "ID: 5250<br>Token: '▁usually'",
           "ID: 1508<br>Token: '▁very'",
           "ID: 6574<br>Token: '▁simply'",
           "ID: 780<br>Token: '▁not'",
           "ID: 15208<br>Token: '▁basically'"
          ],
          [
           "ID: 6574<br>Token: '▁simply'",
           "ID: 1170<br>Token: '▁also'",
           "ID: 3906<br>Token: '▁either'",
           "ID: 5250<br>Token: '▁usually'",
           "ID: 780<br>Token: '▁not'"
          ],
          [
           "ID: 664<br>Token: '▁\"'",
           "ID: 6574<br>Token: '▁simply'",
           "ID: 3906<br>Token: '▁either'",
           "ID: 1170<br>Token: '▁also'",
           "ID: 5250<br>Token: '▁usually'"
          ],
          [
           "ID: 6574<br>Token: '▁simply'",
           "ID: 664<br>Token: '▁\"'",
           "ID: 3906<br>Token: '▁either'",
           "ID: 2167<br>Token: '▁different'",
           "ID: 777<br>Token: '▁''"
          ],
          [
           "ID: 1226<br>Token: '▁car'",
           "ID: 6574<br>Token: '▁simply'",
           "ID: 664<br>Token: '▁\"'",
           "ID: 1080<br>Token: '▁“'",
           "ID: 1170<br>Token: '▁also'"
          ],
          [
           "ID: 1226<br>Token: '▁car'",
           "ID: 777<br>Token: '▁''",
           "ID: 664<br>Token: '▁\"'",
           "ID: 3031<br>Token: '▁‘'",
           "ID: 5250<br>Token: '▁usually'"
          ],
          [
           "ID: 664<br>Token: '▁\"'",
           "ID: 1226<br>Token: '▁car'",
           "ID: 777<br>Token: '▁''",
           "ID: 3031<br>Token: '▁‘'",
           "ID: 5250<br>Token: '▁usually'"
          ],
          [
           "ID: 664<br>Token: '▁\"'",
           "ID: 777<br>Token: '▁''",
           "ID: 1226<br>Token: '▁car'",
           "ID: 3031<br>Token: '▁‘'",
           "ID: 1080<br>Token: '▁“'"
          ],
          [
           "ID: 664<br>Token: '▁\"'",
           "ID: 777<br>Token: '▁''",
           "ID: 235248<br>Token: '▁'",
           "ID: 476<br>Token: '▁a'",
           "ID: 3031<br>Token: '▁‘'"
          ],
          [
           "ID: 777<br>Token: '▁''",
           "ID: 664<br>Token: '▁\"'",
           "ID: 3031<br>Token: '▁‘'",
           "ID: 476<br>Token: '▁a'",
           "ID: 1508<br>Token: '▁very'"
          ],
          [
           "ID: 777<br>Token: '▁''",
           "ID: 573<br>Token: '▁the'",
           "ID: 476<br>Token: '▁a'",
           "ID: 235248<br>Token: '▁'",
           "ID: 664<br>Token: '▁\"'"
          ]
         ],
         "text": [
          [
           "'▁is'",
           "'▁was'",
           "'▁are'",
           "'▁has'",
           "'▁Is'"
          ],
          [
           "'<bos>'",
           "'▁is'",
           "'▁was'",
           "'▁are'",
           "'▁has'"
          ],
          [
           "'▁is'",
           "'<bos>'",
           "'▁are'",
           "'▁was'",
           "'▁has'"
          ],
          [
           "'<bos>'",
           "'▁is'",
           "'▁was'",
           "'▁has'",
           "'▁are'"
          ],
          [
           "'<bos>'",
           "'▁is'",
           "'▁was'",
           "'▁are'",
           "'▁has'"
          ],
          [
           "'<bos>'",
           "'▁is'",
           "'▁was'",
           "'▁are'",
           "'▁not'"
          ],
          [
           "'▁is'",
           "'▁was'",
           "'▁nahilalakip'",
           "'▁are'",
           "'▁not'"
          ],
          [
           "'▁is'",
           "'▁was'",
           "'▁are'",
           "'▁not'",
           "'▁be'"
          ],
          [
           "'▁is'",
           "'▁nahilalakip'",
           "'▁was'",
           "'▁are'",
           "'▁has'"
          ],
          [
           "'▁is'",
           "'▁was'",
           "'▁are'",
           "'▁not'",
           "'▁translated'"
          ],
          [
           "'▁is'",
           "'▁sometimes'",
           "'▁was'",
           "'▁are'",
           "'▁nahilalakip'"
          ],
          [
           "'▁is'",
           "'▁are'",
           "'▁was'",
           "'▁also'",
           "'▁a'"
          ],
          [
           "'▁is'",
           "'▁are'",
           "'▁'",
           "'▁was'",
           "'▁a'"
          ],
          [
           "'▁is'",
           "'▁simply'",
           "'▁not'",
           "'▁also'",
           "'▁in'"
          ],
          [
           "'▁is'",
           "'▁simply'",
           "'▁usually'",
           "'▁very'",
           "'▁also'"
          ],
          [
           "'▁usually'",
           "'▁very'",
           "'▁simply'",
           "'▁not'",
           "'▁basically'"
          ],
          [
           "'▁simply'",
           "'▁also'",
           "'▁either'",
           "'▁usually'",
           "'▁not'"
          ],
          [
           "'▁\"'",
           "'▁simply'",
           "'▁either'",
           "'▁also'",
           "'▁usually'"
          ],
          [
           "'▁simply'",
           "'▁\"'",
           "'▁either'",
           "'▁different'",
           "'▁''"
          ],
          [
           "'▁car'",
           "'▁simply'",
           "'▁\"'",
           "'▁“'",
           "'▁also'"
          ],
          [
           "'▁car'",
           "'▁''",
           "'▁\"'",
           "'▁‘'",
           "'▁usually'"
          ],
          [
           "'▁\"'",
           "'▁car'",
           "'▁''",
           "'▁‘'",
           "'▁usually'"
          ],
          [
           "'▁\"'",
           "'▁''",
           "'▁car'",
           "'▁‘'",
           "'▁“'"
          ],
          [
           "'▁\"'",
           "'▁''",
           "'▁'",
           "'▁a'",
           "'▁‘'"
          ],
          [
           "'▁''",
           "'▁\"'",
           "'▁‘'",
           "'▁a'",
           "'▁very'"
          ],
          [
           "'▁''",
           "'▁the'",
           "'▁a'",
           "'▁'",
           "'▁\"'"
          ]
         ],
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          0,
          1,
          2,
          3,
          4
         ],
         "xaxis": "x",
         "y": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25
         ],
         "yaxis": "y",
         "z": [
          [
           1,
           2.3151466180599098e-24,
           8.42044016187807e-28,
           2.0047263587963015e-29,
           8.650025455723612e-30
          ],
          [
           0.9992285966873169,
           0.0007713921368122101,
           1.7992664663191533e-21,
           1.7071412450799772e-21,
           1.0732710967478325e-24
          ],
          [
           1,
           8.535231188488979e-9,
           1.6947293330645376e-15,
           2.240163123377821e-16,
           1.1926126290028268e-17
          ],
          [
           1,
           2.987542657706399e-9,
           1.3279856372922591e-18,
           6.402782484178761e-20,
           2.0227336708856153e-20
          ],
          [
           0.9999854564666748,
           0.000014573995940736495,
           2.1106125180914148e-10,
           1.82993550867927e-12,
           1.0954423132480962e-12
          ],
          [
           1,
           7.569549948000187e-19,
           8.413709383389827e-25,
           3.975750000901366e-25,
           7.895352118131407e-27
          ],
          [
           0.9999974966049194,
           0.000002450321971991798,
           2.4282755362037278e-8,
           1.404038485475212e-8,
           8.987405486493572e-9
          ],
          [
           0.9998475313186646,
           0.00015150685794651508,
           3.738690850241255e-7,
           2.4958359290394583e-7,
           1.4379475032910705e-7
          ],
          [
           0.9978814721107483,
           0.002040071180090308,
           0.00007608687883475795,
           0.0000017660672710917424,
           2.2895261508892872e-7
          ],
          [
           0.9993451237678528,
           0.0004691276990342885,
           0.00009598900942364708,
           0.00001783434890967328,
           0.00001303147291764617
          ],
          [
           0.9998301267623901,
           0.00005989967394270934,
           0.00004061039362568408,
           0.000021555966668529436,
           0.00001177772537630517
          ],
          [
           0.9999406337738037,
           0.00003853342423099093,
           0.000015297280697268434,
           0.0000010349804142606445,
           9.18196576549235e-7
          ],
          [
           0.9998801946640015,
           0.00005662661715177819,
           0.000030496132239932194,
           0.000013081919860269409,
           0.000005348958893591771
          ],
          [
           0.9877098798751831,
           0.00826561264693737,
           0.002280906541272998,
           0.0006596657913178205,
           0.00020977204258088022
          ],
          [
           0.4871099293231964,
           0.36218827962875366,
           0.05066414549946785,
           0.030026275664567947,
           0.02534375712275505
          ],
          [
           0.5050486326217651,
           0.223493292927742,
           0.10977663844823837,
           0.10093633830547333,
           0.0150370541960001
          ],
          [
           0.3661052882671356,
           0.2514839768409729,
           0.1304149329662323,
           0.10803717374801636,
           0.07067609578371048
          ],
          [
           0.46861255168914795,
           0.3733682930469513,
           0.10170669108629227,
           0.023432526737451553,
           0.011813758872449398
          ],
          [
           0.7756552696228027,
           0.20536597073078156,
           0.006097967736423016,
           0.0056435284204781055,
           0.0024093182291835546
          ],
          [
           0.6871451139450073,
           0.15660768747329712,
           0.14415711164474487,
           0.002450173022225499,
           0.0018776600481942296
          ],
          [
           0.981977641582489,
           0.017486538738012314,
           0.0005235348362475634,
           0.000009828595466387924,
           0.000001020675426843809
          ],
          [
           0.4706916809082031,
           0.4045005142688751,
           0.1247280165553093,
           0.0000411471955885645,
           0.000014091730918153189
          ],
          [
           0.797338604927063,
           0.1997614949941635,
           0.0027032806538045406,
           0.00015175306180026382,
           0.000042159281292697415
          ],
          [
           0.8389861583709717,
           0.15910100936889648,
           0.0011611240915954113,
           0.00028001455939374864,
           0.0002583090972620994
          ],
          [
           0.9999836683273315,
           0.000015939473087200895,
           3.3427886592107825e-7,
           1.2600869681023141e-8,
           1.1745839856303064e-8
          ],
          [
           0.9999192953109741,
           0.00002427246727165766,
           0.000012648476513277274,
           0.000010786672646645457,
           0.000007066440048220102
          ]
         ],
         "zmax": 1,
         "zmin": 0
        }
       ],
       "layout": {
        "height": 1000,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Top 5 tokens at each layer for 'The translation of 'car' in French is"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Tokens"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Layers"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompts DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prompt</th>\n",
       "      <td>The translation of 'car' in French is</td>\n",
       "      <td>The translation of 'cat' in Spanish is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_string</th>\n",
       "      <td>[voiture, bagnole]</td>\n",
       "      <td>[gato, minino]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_string</th>\n",
       "      <td>car</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format_string</th>\n",
       "      <td>'</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_tokens</th>\n",
       "      <td>[voiture, ▁voiture, bagno, ▁bagno]</td>\n",
       "      <td>[gato, ▁gato, min, ▁min]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_tokens</th>\n",
       "      <td>[car, ▁car]</td>\n",
       "      <td>[cat, ▁cat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format_tokens</th>\n",
       "      <td>[', ▁']</td>\n",
       "      <td>[', ▁']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  \\\n",
       "prompt          The translation of 'car' in French is   \n",
       "target_string                      [voiture, bagnole]   \n",
       "english_string                                    car   \n",
       "format_string                                       '   \n",
       "target_tokens      [voiture, ▁voiture, bagno, ▁bagno]   \n",
       "english_tokens                            [car, ▁car]   \n",
       "format_tokens                                 [', ▁']   \n",
       "\n",
       "                                                     1  \n",
       "prompt          The translation of 'cat' in Spanish is  \n",
       "target_string                           [gato, minino]  \n",
       "english_string                                     cat  \n",
       "format_string                                        '  \n",
       "target_tokens                 [gato, ▁gato, min, ▁min]  \n",
       "english_tokens                             [cat, ▁cat]  \n",
       "format_tokens                                  [', ▁']  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nnterp.display import plot_topk_tokens, prompts_to_df\n",
    "\n",
    "probs = logit_lens(demo_model, prompts_str[0])\n",
    "# Visualize top tokens from logit lens\n",
    "plot_topk_tokens(\n",
    "    probs[0],\n",
    "    demo_model.tokenizer,\n",
    "    k=5,\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    title=\"Top 5 tokens at each layer for 'The translation of 'car' in French is\",\n",
    ")\n",
    "\n",
    "# Convert prompts to DataFrame for analysis\n",
    "df = prompts_to_df(prompts, demo_model.tokenizer)\n",
    "print(\"\\nPrompts DataFrame:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572972e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "`nnterp` provides a unified, standardized interface for working with transformer models, built on top of `nnsight`. Key features include:\n",
    "\n",
    "1. **Standardized naming** across all transformer architectures\n",
    "2. **Easy access** to layer/attention/MLP inputs and outputs\n",
    "3. **Built-in methods** for common operations (steering, skipping layers, projecting to vocab)\n",
    "4. **Efficient activation collection** with batching support\n",
    "5. **Prompt utilities** for tracking target tokens\n",
    "6. **Intervention methods** from mechanistic interpretability research\n",
    "7. **Visualization tools** for analyzing model behavior\n",
    "\n",
    "All of this while maintaining the full power and flexibility of `nnsight` under the hood!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52adb44e",
   "metadata": {},
   "source": [
    "# Appendix: `NNsight` cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743fe4e5",
   "metadata": {},
   "source": [
    "## 1) You must execute your interventions in order\n",
    "In the new `NNsight` versions, it is enforced that you must access to model internals *in the same order* as the model execute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0e7647f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-07 17:18:40.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n"
     ]
    },
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_112758/2815926761.py\", line 7, in <module>\n    l1 = nnterp_gpt2.layers_output[1]  # will fail! You need to collect l1 before l2\n  File \"/workspace/nnterp/nnterp/rename_utils.py\", line 99, in __getitem__\n    target = module.output\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 140, in output\n    return self._interleaver.current.request(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 804, in request\n    value = self.send(Events.VALUE, requester)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 788, in send\n    raise response\n\nOutOfOrderError: Value was missed for model.transformer.h.1.output.i0. Did you call an Envoy out of order?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnnterp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardizedTransformer\n\u001b[1;32m      3\u001b[0m nnterp_gpt2 \u001b[38;5;241m=\u001b[39m StandardizedTransformer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m nnterp_gpt2\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy tailor is rich\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     l2 \u001b[38;5;241m=\u001b[39m nnterp_gpt2\u001b[38;5;241m.\u001b[39mlayers_output[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      7\u001b[0m     l1 \u001b[38;5;241m=\u001b[39m nnterp_gpt2\u001b[38;5;241m.\u001b[39mlayers_output[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# will fail! You need to collect l1 before l2\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:408\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# Suppress the ExitTracingException but let other exceptions propagate\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    406\u001b[0m \n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     21\u001b[0m     tracer\u001b[38;5;241m.\u001b[39mexecute(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     Globals\u001b[38;5;241m.\u001b[39mexit()\n",
      "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_112758/2815926761.py\", line 7, in <module>\n    l1 = nnterp_gpt2.layers_output[1]  # will fail! You need to collect l1 before l2\n  File \"/workspace/nnterp/nnterp/rename_utils.py\", line 99, in __getitem__\n    target = module.output\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 140, in output\n    return self._interleaver.current.request(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 804, in request\n    value = self.send(Events.VALUE, requester)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 788, in send\n    raise response\n\nOutOfOrderError: Value was missed for model.transformer.h.1.output.i0. Did you call an Envoy out of order?"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "\n",
    "nnterp_gpt2 = StandardizedTransformer(\"gpt2\")\n",
    "\n",
    "with nnterp_gpt2.trace(\"My tailor is rich\"):\n",
    "    l2 = nnterp_gpt2.layers_output[2]\n",
    "    l1 = nnterp_gpt2.layers_output[1]  # will fail! You need to collect l1 before l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd4d2f",
   "metadata": {},
   "source": [
    "## 2) Gradient computation\n",
    "To compute gradients, you need to open a `.backward()` context, and save the gradients *inside it*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a4beb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nnterp_gpt2.trace(\"My tailor is rich\"):\n",
    "    l1_out = nnterp_gpt2.layers_output[1]  # get l1 before accessing logits\n",
    "    logits = nnterp_gpt2.output.logits\n",
    "    with logits.sum().backward(\n",
    "        retain_graph=True\n",
    "    ):  # use retain_graph if you want to do multiple backprops\n",
    "        if False:\n",
    "            l1_grad = nnterp_gpt2.layers_output[1].grad.save()\n",
    "            # this would fail as we'd access nnterp_gpt2.layers_output[1] after nnterp_gpt2.output\n",
    "        l1_grad = l1_out.grad.save()\n",
    "    with (logits.sum() ** 2).backward():\n",
    "        l1_grad_2 = l1_out.grad.save()\n",
    "\n",
    "assert not th.allclose(l1_grad, l1_grad_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d3d50c",
   "metadata": {},
   "source": [
    "## 3) Use tracer.stop() to save useless computations\n",
    "If you're just computing activations, don't forget to call `tracer.stop()` at the end of your trace. This will stop the model from executing the rest of its computations, and save you some time, as demonstrated below (with the contribution of Claude 4 Sonnet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba7fdc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎭 Welcome to the Theatrical Performance Comparison! 🎭\n",
      "============================================================\n",
      "\n",
      "🐌 ACT I: 'The Tragedy of the Unstoppable Tracer' 🐌\n",
      "In which our hero forgets to call tracer.stop()...\n",
      "⏰ Duration of suffering: 0.8498 seconds\n",
      "\n",
      "⚡ ACT II: 'The Redemption of the Stopped Tracer' ⚡\n",
      "Our hero learns the ancient art of tracer.stop()...\n",
      "⏰ Duration of enlightenment: 0.3863 seconds\n",
      "\n",
      "============================================================\n",
      "🎉 THE GRAND RESULTS SPECTACULAR! 🎉\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>🎭 Performance Type</th>\n",
       "      <th>⏱️ Time (seconds)</th>\n",
       "      <th>🎯 Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Without tracer.stop() 🐌</td>\n",
       "      <td>0.8498</td>\n",
       "      <td>Tragic 😭</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With tracer.stop() ⚡</td>\n",
       "      <td>0.3863</td>\n",
       "      <td>Magnificent! 🌟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Time Saved 💰</td>\n",
       "      <td>0.4635</td>\n",
       "      <td>PROFIT! 📈</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        🎭 Performance Type ⏱️ Time (seconds)        🎯 Rating\n",
       "0  Without tracer.stop() 🐌            0.8498        Tragic 😭\n",
       "1     With tracer.stop() ⚡            0.3863  Magnificent! 🌟\n",
       "2             Time Saved 💰            0.4635       PROFIT! 📈"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏎️ SPEEDUP METER 🏎️\n",
      "┌──────────────────────────────────────────────────┐\n",
      "│█████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░│\n",
      "└──────────────────────────────────────────────────┘\n",
      "   💫 COSMIC SPEEDUP: 2.20x FASTER! 💫\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(\"🎭 Welcome to the Theatrical Performance Comparison! 🎭\\n\" + \"=\" * 60 + \"\\n\\n🐌 ACT I: 'The Tragedy of the Unstoppable Tracer' 🐌\\nIn which our hero forgets to call tracer.stop()...\")\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(30):\n",
    "    with nnterp_gpt2.trace([\"Neel Samba\", \"Chris Aloha\"]):\n",
    "        out5 = nnterp_gpt2.layers_output[5].save()\n",
    "end_time = time.time()\n",
    "nostop_time = end_time - start_time\n",
    "\n",
    "print(f\"⏰ Duration of suffering: {nostop_time:.4f} seconds\\n\\n⚡ ACT II: 'The Redemption of the Stopped Tracer' ⚡\\nOur hero learns the ancient art of tracer.stop()...\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(30):\n",
    "    with nnterp_gpt2.trace([\"Neel Samba\", \"Chris Aloha\"]) as tracer:\n",
    "        out5 = nnterp_gpt2.layers_output[5].save()\n",
    "        tracer.stop()\n",
    "end_time = time.time()\n",
    "stop_time = end_time - start_time\n",
    "\n",
    "print(f\"⏰ Duration of enlightenment: {stop_time:.4f} seconds\")\n",
    "\n",
    "speedup = nostop_time / stop_time\n",
    "time_saved = nostop_time - stop_time\n",
    "\n",
    "# fun display\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n🎉 THE GRAND RESULTS SPECTACULAR! 🎉\\n\" + \"=\" * 60)\n",
    "results_df = pd.DataFrame({\n",
    "    '🎭 Performance Type': ['Without tracer.stop() 🐌', 'With tracer.stop() ⚡', 'Time Saved 💰'],\n",
    "    '⏱️ Time (seconds)': [f\"{nostop_time:.4f}\", f\"{stop_time:.4f}\", f\"{time_saved:.4f}\"],\n",
    "    '🎯 Rating': ['Tragic 😭', 'Magnificent! 🌟', 'PROFIT! 📈']\n",
    "})\n",
    "display(results_df)\n",
    "speedup_bars = int(speedup * 10)\n",
    "meter = \"█\" * min(speedup_bars, 48) + \"░\" * (50 - min(speedup_bars, 48))\n",
    "print(f\"\\n🏎️ SPEEDUP METER 🏎️\\n┌{'─' * 50}┐\\n│{meter}│\\n└{'─' * 50}┘\\n   💫 COSMIC SPEEDUP: {speedup:.2f}x FASTER! 💫\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7f664",
   "metadata": {},
   "source": [
    "## 4) Using NNsight builtin cache to collect activations\n",
    "\n",
    "`NNsight 0.5` introduces a builtin way to cache activations during the forward pass. Be careful not to call `tracer.stop()` before all the module of the cache have been accessed.\n",
    "\n",
    "NOTE: Broken in current dev version of `NNsight` (0.5.0dev7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nnterp_gpt2.trace(\"Hello\"):\n",
    "    cache = nnterp_gpt2.cache(modules=[layer for layer in nnterp_gpt2.layers[::2]]).save()\n",
    "\n",
    "print(cache.keys())\n",
    "print(cache['model.layers.10'].inputs)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
