{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef12d930",
   "metadata": {},
   "source": [
    "# Demo: nnterp Features Showcase\n",
    "\n",
    "This notebook demonstrates the key features of `nnterp`, which aims to offer a unified interface for all transformer models and give best `NNsight` practices for LLMs in everyone's hands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aacf2a",
   "metadata": {},
   "source": [
    "## 1. Standardized Interface\n",
    "\n",
    "Similar to [`transformer_lens`](https://github.com/TransformerLensOrg/TransformerLens), `nnterp` provides a standardized interface for all transformer models.\n",
    "The main difference is that `nnterp` still uses the huggingface implementation under the hood through `NNsight`, while transformer_lens uses its own implementation of the transformer architecture. However, each transformer implementation has its own quirks, such that `transformer_lens` is not able to support all models, and can sometimes have significant difference with the huggingface implementation.\n",
    "\n",
    "Note that `nnterp` doesn't support all models either, since `NNsight` itself doesn't support all architectures. Additionally, because different models use different naming conventions, `nnterp` doesn't support all HuggingFace models, but it does support a good portion of them. When a model is loaded in `nnterp`, automatic tests are performed to verify that the model has been correctly renamed and that `nnterp`'s hooks return the expected shapes. This means that even if an architecture hasn't been officially tested, the simple fact that it loads successfully indicates it's probably working correctly.\n",
    "\n",
    "The way it's implemented is based on the `NNsight` built-in renaming feature, to make all models look like the llama naming convention, without having to write `model.model`, namely:\n",
    "```ocaml\n",
    "StandardizedTransformer\n",
    "├── layers\n",
    "│   ├── self_attn\n",
    "│   └── mlp\n",
    "├── ln_final\n",
    "└── lm_head\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13faa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 64, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=64, out_features=256, bias=False)\n",
      "          (up_proj): Linear(in_features=64, out_features=256, bias=False)\n",
      "          (down_proj): Linear(in_features=256, out_features=64, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((64,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((64,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((64,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=64, out_features=32000, bias=False)\n",
      ")\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "print(AutoModelForCausalLM.from_pretrained(\"Maykeye/TinyLLama-v0\"))\n",
    "print(AutoModelForCausalLM.from_pretrained(\"gpt2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259defb",
   "metadata": {},
   "source": [
    "As you can see, the naming scheme of gpt2 is different from the llama naming convention.\n",
    "A simple way to fix this is to use the `rename` feature of `NNsight` to rename the gpt2 modules to the llama naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6cbba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (model/transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (layers/h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn/attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm/ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (generator): WrapperModule()\n",
      ")\n",
      "GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel(\n",
    "    \"gpt2\", rename=dict(transformer=\"model\", h=\"layers\", ln_f=\"ln_final\", attn=\"self_attn\")\n",
    ")\n",
    "print(model)\n",
    "# Access the attn module as if it was a llama model\n",
    "print(model.model.layers[0].self_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe2e5c",
   "metadata": {},
   "source": [
    "You can see the that renamed modules are displayed like `(new_name)/old_name`. However, many models family have their own naming convention, `nnterp` has a global renaming scheme that should transform any model to the llama naming convention. The easiest way to use it is to load your model using the `StandardizedTransformer` class that inherits from `nnsight.LanguageModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f830ab98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-10 17:00:11.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m184\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
      "\u001b[32m2025-07-10 17:00:12.782\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m213\u001b[0m - \u001b[33m\u001b[1mgpt2's architecture has failed tests for this transformer version. Use at your own risks. If you want to be safe use only the renaming feature of nnterp, and do not use model.layers_output and other accessors\u001b[0m\n",
      "\u001b[32m2025-07-10 17:00:12.783\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m217\u001b[0m - \u001b[33m\u001b[1mgpt2's architecture is not tested. This may cause unexpected behavior. It is recommended to check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()` if you plan on using it (prettier in a notebook).\n",
      "Feel free to open an issue on github (https://github.com/butanium/nnterp/issues) or run the tests yourself with a toy model if you want to add test coverage for this model.\u001b[0m\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (model/transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (layers/h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn/attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm/ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (generator): WrapperModule()\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (self_attn/attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "\n",
    "# You will see the `layers` module printed two times, it'll be explained later.\n",
    "nnterp_gpt2 = StandardizedTransformer(\"gpt2\")\n",
    "print(nnterp_gpt2)\n",
    "# StandardizedTransformer also use `device_map=\"auto\"` by default:\n",
    "nnterp_gpt2.dispatch()\n",
    "print(nnterp_gpt2.model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd4f50",
   "metadata": {},
   "source": [
    "Great! But I can see you at the back of the classroom, asking yourself:\n",
    "> \"Why would you create a package that just pass the right dict to the `NNsight` `rename` feature?\"\n",
    "\n",
    "And actually, I'm glad you asked! `StandardizedTransformer` and `nnterp` have a lot of other features, so bear with me!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541f3b8",
   "metadata": {},
   "source": [
    "## 2. Accessing Modules I/O\n",
    "With `NNsight`, the most robust way to set the residual stream after layer 1 to be the residual stream after layer 0 for a LLama-like model would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8452f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "llama = LanguageModel(\"Maykeye/TinyLLama-v0\")\n",
    "with llama.trace(\"hello\"):\n",
    "    llama.model.layers[1].output = (\n",
    "        llama.model.layers[0].output[0],\n",
    "        *llama.model.layers[1].output[1:],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0ef20",
   "metadata": {},
   "source": [
    "Note that the following can cause issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60409efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with llama.trace(\"hello\"):\n",
    "    # can't do this because .output is a tuple\n",
    "    # llama.model.layers[1].output[0] = llama.model.layers[0].output[0]\n",
    "\n",
    "    # Can cause errors with gradient computation\n",
    "    llama.model.layers[1].output[0][:] = llama.model.layers[0].output[0]\n",
    "\n",
    "with llama.trace(\"hello\"):\n",
    "    # Can cause errors with opt if you do this at its last layer (thanks pytest)\n",
    "    llama.model.layers[1].output = (llama.model.layers[0].output[0],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fbd600",
   "metadata": {},
   "source": [
    "`nnterp` makes this much cleaner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f9b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, you can access layer inputs and outputs directly:\n",
    "with nnterp_gpt2.trace(\"hello\"):\n",
    "    # Access layer 5's output\n",
    "    layer_5_output = nnterp_gpt2.layers_output[5]\n",
    "    # Set layer 10's output to be layer 5's output\n",
    "    nnterp_gpt2.layers_output[10] = layer_5_output\n",
    "\n",
    "# You can also access attention and MLP outputs:\n",
    "with nnterp_gpt2.trace(\"hello\"):\n",
    "    attn_output = nnterp_gpt2.attentions_output[3]\n",
    "    mlp_output = nnterp_gpt2.mlps_output[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3317e47b",
   "metadata": {},
   "source": [
    "## 3. Builtin interventions\n",
    "\n",
    "`StandardizedTransformer` also provides convenient methods for common operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "665f3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "# Project hidden states to vocabulary using the unembed norm and lm_head\n",
    "with nnterp_gpt2.trace(\"The capital of France is\"):\n",
    "    hidden = nnterp_gpt2.layers_output[5]\n",
    "    logits = nnterp_gpt2.project_on_vocab(hidden)\n",
    "\n",
    "# Skip layers entirely\n",
    "with nnterp_gpt2.trace(\"Hello world\"):\n",
    "    # Skip layer 1\n",
    "    nnterp_gpt2.skip_layer(1)\n",
    "    # Skip layers 2 through 3 (inclusive)\n",
    "    nnterp_gpt2.skip_layers(2, 3)\n",
    "\n",
    "# This is useful if you want to start at a later layer than the first one\n",
    "with nnterp_gpt2.trace(\"Hello world\") as tracer:\n",
    "    layer_6_out = nnterp_gpt2.layers_output[6].save()\n",
    "    tracer.stop()  # avoid computations after layer 6\n",
    "\n",
    "with nnterp_gpt2.trace(\"Hello world\"):\n",
    "    nnterp_gpt2.skip_layers(0, 6, skip_with=layer_6_out)\n",
    "    half_half_logits = nnterp_gpt2.logits.save()\n",
    "\n",
    "with nnterp_gpt2.trace(\"Hello world\"):\n",
    "    vanilla_logits = nnterp_gpt2.logits.save()\n",
    "\n",
    "assert th.allclose(vanilla_logits, half_half_logits)  # they should be the same\n",
    "\n",
    "# Direct steering\n",
    "steering_vector = th.randn(768)  # gpt2 hidden size\n",
    "with nnterp_gpt2.trace(\"The weather today is\"):\n",
    "    nnterp_gpt2.steer(layers=[1, 3], steering_vector=steering_vector, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111a4ac",
   "metadata": {},
   "source": [
    "## 4. Attention Probabilities\n",
    "\n",
    "For models that support it, you can access attention probabilities directly. You can check if a model supports it by calling `model.supports_attention_probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296ac73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention probs shape will be: (batch, heads, seq_len, seq_len): torch.Size([1, 12, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "nnterp_gpt2.tokenizer.padding_side = (\n",
    "    \"left\"  # ensure left padding for easy access to the first token\n",
    ")\n",
    "\n",
    "with th.no_grad():\n",
    "    with nnterp_gpt2.trace(\"The cat sat on the mat\"):\n",
    "        # Access attention probabilities for layer 5\n",
    "        attn_probs_l2 = nnterp_gpt2.attention_probabilities[2].save()\n",
    "        attn_probs = nnterp_gpt2.attention_probabilities[5].save()\n",
    "        print(\n",
    "            f\"Attention probs shape will be: (batch, heads, seq_len, seq_len): {attn_probs.shape}\"\n",
    "        )\n",
    "        # knock out the attention to the first token\n",
    "        attn_probs[:, :, :, 0] = 0\n",
    "        attn_probs /= attn_probs.sum(dim=-1, keepdim=True)\n",
    "        corr_logits = nnterp_gpt2.logits.save()\n",
    "    with nnterp_gpt2.trace(\"The cat sat on the mat\"):\n",
    "        baseline_logits = nnterp_gpt2.logits.save()\n",
    "\n",
    "assert not th.allclose(corr_logits, baseline_logits)\n",
    "\n",
    "sums = attn_probs_l2.sum(dim=-1)\n",
    "# last dimension is the attention of token i to all other tokens, so should sum to 1\n",
    "assert th.allclose(sums, th.ones_like(sums))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5363fb",
   "metadata": {},
   "source": [
    "Under the hood this uses the new tracing system implemented in `NNsight v0.5` which allow to access most model intermediate variables during the forward pass. This means that if the `transformers` implementation were to change, this could break or give unexpected results, so it is recommended to use one of the tested versions of `transformers` and to check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()` if you want to use a different version of `transformers` / a architecture that has not been tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb2c8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Accessing attention probabilities from:\n",
       "```py\n",
       "model.transformer.h.0.attn.attention_interface_0.module_attn_dropout_0:\n",
       "\n",
       "    ....\n",
       "            attn_weights = attn_weights + causal_mask\n",
       "    \n",
       "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "    \n",
       "        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
       "        attn_weights = attn_weights.type(value.dtype)\n",
       "    --> attn_weights = module.attn_dropout(attn_weights) <--\n",
       "    \n",
       "        # Mask heads if we want to\n",
       "        if head_mask is not None:\n",
       "            attn_weights = attn_weights * head_mask\n",
       "    \n",
       "        attn_output = torch.matmul(attn_weights, value)\n",
       "    ....\n",
       "```\n",
       "\n",
       "## Full module source:\n",
       "```py\n",
       "                             * def eager_attention_forward(module, query, key, value, attention_mask, head_mask=None, **kwargs):\n",
       " key_transpose_0         ->  0     attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
       " torch_matmul_0          ->  +     ...\n",
       "                             1 \n",
       "                             2     if module.scale_attn_weights:\n",
       " torch_full_0            ->  3         attn_weights = attn_weights / torch.full(\n",
       " value_size_0            ->  4             [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n",
       "                             5         )\n",
       "                             6 \n",
       "                             7     # Layer-wise attention scaling\n",
       "                             8     if module.scale_attn_by_inverse_layer_idx:\n",
       " float_0                 ->  9         attn_weights = attn_weights / float(module.layer_idx + 1)\n",
       "                            10 \n",
       "                            11     if not module.is_cross_attention:\n",
       "                            12         # if only \"normal\" attention layer implements causal mask\n",
       " query_size_0            -> 13         query_length, key_length = query.size(-2), key.size(-2)\n",
       " key_size_0              ->  +         ...\n",
       "                            14         causal_mask = module.bias[:, :, key_length - query_length : key_length, :key_length]\n",
       " torch_finfo_0           -> 15         mask_value = torch.finfo(attn_weights.dtype).min\n",
       "                            16         # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
       "                            17         # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
       " torch_full_1            -> 18         mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
       " attn_weights_to_0       -> 19         attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
       " torch_where_0           ->  +         ...\n",
       "                            20 \n",
       "                            21     if attention_mask is not None:\n",
       "                            22         # Apply the attention mask\n",
       "                            23         causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n",
       "                            24         attn_weights = attn_weights + causal_mask\n",
       "                            25 \n",
       " nn_functional_softmax_0 -> 26     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                            27 \n",
       "                            28     # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
       " attn_weights_type_0     -> 29     attn_weights = attn_weights.type(value.dtype)\n",
       " module_attn_dropout_0   -> 30     attn_weights = module.attn_dropout(attn_weights)\n",
       "                            31 \n",
       "                            32     # Mask heads if we want to\n",
       "                            33     if head_mask is not None:\n",
       "                            34         attn_weights = attn_weights * head_mask\n",
       "                            35 \n",
       " torch_matmul_1          -> 36     attn_output = torch.matmul(attn_weights, value)\n",
       " attn_output_transpose_0 -> 37     attn_output = attn_output.transpose(1, 2)\n",
       "                            38 \n",
       "                            39     return attn_output, attn_weights\n",
       "                            40 \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nnterp_gpt2.attention_probabilities.print_source()  # pretty markdown display in a notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d1c5f",
   "metadata": {},
   "source": [
    "## 5. Specific Token Activation Collection\n",
    "\n",
    "`nnterp` provides utilities for collecting activations efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "611a2f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched activations shape: torch.Size([3, 100, 768])\n"
     ]
    }
   ],
   "source": [
    "from nnterp.nnsight_utils import (\n",
    "    get_token_activations,\n",
    "    collect_token_activations_batched,\n",
    ")\n",
    "\n",
    "# Collect activations for specific tokens\n",
    "prompts = [\"The capital of France is\", \"The weather today is\"]\n",
    "with nnterp_gpt2.trace(prompts) as tracer:\n",
    "    # Get last token activations for all layers\n",
    "    activations = get_token_activations(nnterp_gpt2, prompts, idx=-1, tracer=tracer)\n",
    "    # activations shape: (num_layers, batch_size, hidden_size)\n",
    "\n",
    "# For large datasets, use batched collection\n",
    "large_prompts = [\"Sample text \" + str(i) for i in range(100)]\n",
    "batch_activations = collect_token_activations_batched(\n",
    "    nnterp_gpt2,\n",
    "    large_prompts,\n",
    "    batch_size=16,\n",
    "    layers=[3, 9, 11],  # Only collect specific layers, default is all layers\n",
    "    idx=-1,  # Last token (default)\n",
    ")\n",
    "print(f\"Batched activations shape: {batch_activations.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78591b6e",
   "metadata": {},
   "source": [
    "## 6. Prompt Utilities\n",
    "\n",
    "`nnterp` provides utilities for working with prompts and tracking probabilities of first tokens of certain strings. It tracks both the first token of \"string\" and \" string\".\n",
    "\n",
    "You can provide multiple string per category, the probabilities returned will be the sum of the probabilities of all the first tokens of the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6785fc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: ['Paris', 'ĠParis']\n",
      "traps: ['London', 'ĠLondon', 'Mad', 'ĠMadrid']\n",
      "longstring: ['the', 'Ġthe']\n",
      "target: ['J', 'ĠJupiter']\n",
      "traps: ['Earth', 'ĠEarth', 'Ne', 'ĠNeptune']\n",
      "longstring: ['Pal', 'ĠPalace']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84f8e7238fe4c2faa7e2e35d8d37e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target token probabilities:\n",
      "  target: shape torch.Size([2, 1])\n",
      "  traps: shape torch.Size([2, 1])\n",
      "  longstring: shape torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "from nnterp.prompt_utils import Prompt, run_prompts\n",
    "\n",
    "# Create prompts with target tokens to track\n",
    "prompt1 = Prompt.from_strings(\n",
    "    \"The capital of France (not England or Spain) is\",\n",
    "    {\n",
    "        \"target\": \"Paris\",\n",
    "        \"traps\": [\"London\", \"Madrid\"],\n",
    "        \"longstring\": \"the country of France\",\n",
    "    },\n",
    "    nnterp_gpt2.tokenizer,\n",
    ")\n",
    "for name, tokens in prompt1.target_tokens.items():\n",
    "    print(f\"{name}: {nnterp_gpt2.tokenizer.convert_ids_to_tokens(tokens)}\")\n",
    "\n",
    "prompt2 = Prompt.from_strings(\n",
    "    \"The largest planet (not Earth or Neptune) is\",\n",
    "    {\"target\": \"Jupiter\", \"traps\": [\"Earth\", \"Neptune\"], \"longstring\": \"Palace planet\"},\n",
    "    nnterp_gpt2.tokenizer,\n",
    ")\n",
    "for name, tokens in prompt2.target_tokens.items():\n",
    "    print(f\"{name}: {nnterp_gpt2.tokenizer.convert_ids_to_tokens(tokens)}\")\n",
    "\n",
    "# Run prompts and get probabilities for target tokens\n",
    "results = run_prompts(nnterp_gpt2, [prompt1, prompt2], batch_size=2)\n",
    "print(\"Target token probabilities:\")\n",
    "for target, probs in results.items():\n",
    "    print(f\"  {target}: shape {probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da692d08",
   "metadata": {},
   "source": [
    "## 7. Interventions\n",
    "\n",
    "`nnterp` provides several intervention methods inspired by mechanistic interpretability research:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79776001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit lens output shape: torch.Size([2, 12, 50257])\n",
      "repeat_prompt: TargetPrompt(prompt='car:car\\n\\ncross:cross\\n\\nazdrfa:azdrfa\\n\\n*', index_to_patch=-1)\n",
      "custom_repeat_prompt: TargetPrompt(prompt='car:car\\n\\ncross:cross\\n\\nazdrfa:azdrfa\\n\\n*', index_to_patch=-1)\n",
      "patchscope_probs: torch.Size([2, 12, 50257])\n"
     ]
    }
   ],
   "source": [
    "from nnterp.interventions import (\n",
    "    logit_lens,\n",
    "    patchscope_lens,\n",
    "    TargetPrompt,\n",
    "    repeat_prompt,\n",
    "    steer,\n",
    ")\n",
    "\n",
    "# Logit Lens: See predictions at each layer\n",
    "prompts = [\"The capital of France is\", \"The sun rises in the\"]\n",
    "probs = logit_lens(nnterp_gpt2, prompts)\n",
    "print(f\"Logit lens output shape: {probs.shape}\")  # (batch, layers, vocab)\n",
    "\n",
    "# Patchscope: Replace activations from one context into another\n",
    "source_prompts = [\"Paris is beautiful\", \"London is foggy\"]\n",
    "custom_target_prompt = TargetPrompt(\"city: Paris\\nfood: croissant\\n?\", -1)\n",
    "target_prompt = repeat_prompt()  # Creates a repetition task\n",
    "custom_repeat_prompt = repeat_prompt(\n",
    "    words=[\"car\", \"cross\", \"azdrfa\"],\n",
    "    rel=\":\",\n",
    "    sep=\"\\n\\n\",\n",
    "    placeholder=\"*\",\n",
    ")\n",
    "print(f\"repeat_prompt: {custom_repeat_prompt}\")\n",
    "print(f\"custom_repeat_prompt: {custom_repeat_prompt}\")\n",
    "patchscope_probs = patchscope_lens(\n",
    "    nnterp_gpt2, source_prompts=source_prompts, target_patch_prompts=target_prompt\n",
    ")\n",
    "print(f\"patchscope_probs: {patchscope_probs.shape}\")\n",
    "\n",
    "# Steering with intervention function\n",
    "with nnterp_gpt2.trace(\"The weather is\"):\n",
    "    steer(nnterp_gpt2, layers=[5, 10], steering_vector=steering_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02998b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "You can use a combination of run_prompts and interventions to get the probabilities of certain tokens according to your custom intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "069c856f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-10 17:00:18.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m184\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
      "\u001b[32m2025-07-10 17:00:20.568\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m213\u001b[0m - \u001b[33m\u001b[1mgoogle/gemma-2-2b's architecture has failed tests for this transformer version. Use at your own risks. If you want to be safe use only the renaming feature of nnterp, and do not use model.layers_output and other accessors\u001b[0m\n",
      "\u001b[32m2025-07-10 17:00:20.569\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m217\u001b[0m - \u001b[33m\u001b[1mgoogle/gemma-2-2b's architecture is not tested. This may cause unexpected behavior. It is recommended to check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()` if you plan on using it (prettier in a notebook).\n",
      "Feel free to open an issue on github (https://github.com/butanium/nnterp/issues) or run the tests yourself with a toy model if you want to add test coverage for this model.\u001b[0m\n",
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a7a62f2d8e4e8f86ca3e5850dd3e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f2f716dceb4bd6ab606ab65662680f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: torch.Size([2, 26])\n",
      "english: torch.Size([2, 26])\n",
      "format: torch.Size([2, 26])\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "target",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25
         ],
         "y": [
          0,
          0,
          1.0477929007295955e-39,
          3.3572008169903216e-37,
          3.3725472632050863e-25,
          3.6082769559333792e-37,
          5.974479717547129e-23,
          7.064204322083499e-18,
          2.661365939744436e-17,
          1.1582176732451155e-13,
          7.272823937868027e-16,
          5.24414403029372e-16,
          2.400891383828028e-15,
          8.364434091103629e-14,
          1.771075473777639e-11,
          3.0354375697011493e-13,
          3.620844596374795e-11,
          2.011593203208456e-13,
          3.741373327142451e-15,
          2.67894092326193e-11,
          1.154006225431367e-13,
          1.9552523662647037e-13,
          8.380037982980149e-12,
          4.287754979087448e-14,
          1.202039336123395e-12,
          0.0002228509692940861
         ]
        },
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "english",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25
         ],
         "y": [
          0,
          1.401298464324817e-45,
          4.751985777003652e-36,
          1.249700094947333e-33,
          1.2111213844915835e-25,
          1.113362701513488e-34,
          4.810917581628571e-22,
          2.584157728549349e-18,
          4.2512577005248434e-17,
          9.211833125997732e-15,
          2.55039481660078e-15,
          1.8504258604660294e-17,
          9.279078019506512e-18,
          4.034329497532539e-14,
          1.0548329570925219e-11,
          8.504329375045791e-14,
          5.178877882094923e-10,
          6.9792739296192785e-12,
          5.079090925619312e-8,
          0.8435060977935791,
          0.9909697771072388,
          0.702079176902771,
          0.2419319450855255,
          2.0453693139188545e-7,
          4.922426910525246e-9,
          0.0000037258014344843104
         ]
        },
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "format",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25
         ],
         "y": [
          2.4922373447709737e-40,
          3.3020952868270977e-32,
          7.28434178113041e-40,
          1.864506922709312e-29,
          2.5811363221150282e-20,
          2.2197348907458836e-28,
          1.914033071721152e-18,
          5.4384453415808574e-15,
          1.5282396116818309e-13,
          8.418319210126701e-13,
          9.86899689574272e-13,
          7.13409637354695e-13,
          3.191941719618563e-12,
          7.809057933627628e-8,
          4.6338300307979807e-7,
          0.0000022637991605733987,
          0.00007198070670710877,
          0.005840994417667389,
          0.0015627413522452116,
          0.0005657284636981785,
          0.008743060752749443,
          0.062422383576631546,
          0.13925188779830933,
          0.12920856475830078,
          0.9999398589134216,
          0.9995877146720886
         ]
        }
       ],
       "layout": {
        "hovermode": "x unified",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Mean Token Probabilities Across Layers"
        },
        "xaxis": {
         "title": {
          "text": "Layer"
         }
        },
        "yaxis": {
         "title": {
          "text": "Mean Probability"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_model = StandardizedTransformer(\"google/gemma-2-2b\")\n",
    "# uncomment if you don't have a GPU\n",
    "# demo_model = nnterp_gpt2\n",
    "\n",
    "prompts_str = [\n",
    "    \"The translation of 'car' in French is\",\n",
    "    \"The translation of 'cat' in Spanish is\",\n",
    "]\n",
    "tokens = [\n",
    "    {\"target\": [\"voiture\", \"bagnole\"], \"english\": \"car\", \"format\": \"'\"},\n",
    "    {\"target\": [\"gato\", \"minino\"], \"english\": \"cat\", \"format\": \"'\"},\n",
    "]\n",
    "prompts = [\n",
    "    Prompt.from_strings(prompt, tokens, demo_model.tokenizer)\n",
    "    for prompt, tokens in zip(prompts_str, tokens)\n",
    "]\n",
    "results = run_prompts(demo_model, prompts, batch_size=2, get_probs_func=logit_lens)\n",
    "for category, probs in results.items():\n",
    "    print(f\"{category}: {probs.shape}\")  # (batch, layers)\n",
    "\n",
    "# Create a plotly plot showing mean probabilities for each category across layers\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calculate mean probabilities across batches for each category and layer\n",
    "mean_probs = {category: probs.mean(dim=0) for category, probs in results.items()}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add a line for each category\n",
    "for category, probs in mean_probs.items():\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(probs))),\n",
    "            y=probs.tolist(),\n",
    "            mode=\"lines+markers\",\n",
    "            name=category,\n",
    "            line=dict(width=2),\n",
    "            marker=dict(size=6),\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Mean Token Probabilities Across Layers\",\n",
    "    xaxis_title=\"Layer\",\n",
    "    yaxis_title=\"Mean Probability\",\n",
    "    hovermode=\"x unified\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7930aa",
   "metadata": {},
   "source": [
    "## 8. Visualization\n",
    "\n",
    "Finally, `nnterp` provides visualization utilities for analyzing model probabilities and prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e63b784e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorbar": {
          "len": 0.9,
          "thickness": 15,
          "title": {
           "text": "Probability"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(5,48,97)"
          ],
          [
           0.1,
           "rgb(33,102,172)"
          ],
          [
           0.2,
           "rgb(67,147,195)"
          ],
          [
           0.3,
           "rgb(146,197,222)"
          ],
          [
           0.4,
           "rgb(209,229,240)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(253,219,199)"
          ],
          [
           0.7,
           "rgb(244,165,130)"
          ],
          [
           0.8,
           "rgb(214,96,77)"
          ],
          [
           0.9,
           "rgb(178,24,43)"
          ],
          [
           1,
           "rgb(103,0,31)"
          ]
         ],
         "hovertemplate": "Layer: %{y}<br>%{hovertext}<br>Probability: %{z}<extra></extra>",
         "hovertext": [
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 919<br>Token: '▁has'",
           "ID: 2125<br>Token: '▁Is'"
          ],
          [
           "ID: 2<br>Token: '<bos>'",
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 919<br>Token: '▁has'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 2<br>Token: '<bos>'",
           "ID: 708<br>Token: '▁are'",
           "ID: 729<br>Token: '▁was'",
           "ID: 919<br>Token: '▁has'"
          ],
          [
           "ID: 2<br>Token: '<bos>'",
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 919<br>Token: '▁has'",
           "ID: 708<br>Token: '▁are'"
          ],
          [
           "ID: 2<br>Token: '<bos>'",
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 919<br>Token: '▁has'"
          ],
          [
           "ID: 2<br>Token: '<bos>'",
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 780<br>Token: '▁not'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 212549<br>Token: '▁nahilalakip'",
           "ID: 708<br>Token: '▁are'",
           "ID: 780<br>Token: '▁not'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 780<br>Token: '▁not'",
           "ID: 614<br>Token: '▁be'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 212549<br>Token: '▁nahilalakip'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 919<br>Token: '▁has'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 780<br>Token: '▁not'",
           "ID: 28707<br>Token: '▁translated'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 6947<br>Token: '▁sometimes'",
           "ID: 729<br>Token: '▁was'",
           "ID: 708<br>Token: '▁are'",
           "ID: 212549<br>Token: '▁nahilalakip'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 708<br>Token: '▁are'",
           "ID: 729<br>Token: '▁was'",
           "ID: 1170<br>Token: '▁also'",
           "ID: 476<br>Token: '▁a'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 708<br>Token: '▁are'",
           "ID: 235248<br>Token: '▁'",
           "ID: 729<br>Token: '▁was'",
           "ID: 476<br>Token: '▁a'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 6574<br>Token: '▁simply'",
           "ID: 780<br>Token: '▁not'",
           "ID: 1170<br>Token: '▁also'",
           "ID: 575<br>Token: '▁in'"
          ],
          [
           "ID: 603<br>Token: '▁is'",
           "ID: 6574<br>Token: '▁simply'",
           "ID: 5250<br>Token: '▁usually'",
           "ID: 1508<br>Token: '▁very'",
           "ID: 1170<br>Token: '▁also'"
          ],
          [
           "ID: 5250<br>Token: '▁usually'",
           "ID: 1508<br>Token: '▁very'",
           "ID: 6574<br>Token: '▁simply'",
           "ID: 780<br>Token: '▁not'",
           "ID: 15208<br>Token: '▁basically'"
          ],
          [
           "ID: 6574<br>Token: '▁simply'",
           "ID: 1170<br>Token: '▁also'",
           "ID: 3906<br>Token: '▁either'",
           "ID: 5250<br>Token: '▁usually'",
           "ID: 780<br>Token: '▁not'"
          ],
          [
           "ID: 664<br>Token: '▁\"'",
           "ID: 6574<br>Token: '▁simply'",
           "ID: 3906<br>Token: '▁either'",
           "ID: 1170<br>Token: '▁also'",
           "ID: 5250<br>Token: '▁usually'"
          ],
          [
           "ID: 6574<br>Token: '▁simply'",
           "ID: 664<br>Token: '▁\"'",
           "ID: 3906<br>Token: '▁either'",
           "ID: 2167<br>Token: '▁different'",
           "ID: 777<br>Token: '▁''"
          ],
          [
           "ID: 1226<br>Token: '▁car'",
           "ID: 6574<br>Token: '▁simply'",
           "ID: 664<br>Token: '▁\"'",
           "ID: 1080<br>Token: '▁“'",
           "ID: 1170<br>Token: '▁also'"
          ],
          [
           "ID: 1226<br>Token: '▁car'",
           "ID: 777<br>Token: '▁''",
           "ID: 664<br>Token: '▁\"'",
           "ID: 3031<br>Token: '▁‘'",
           "ID: 5250<br>Token: '▁usually'"
          ],
          [
           "ID: 664<br>Token: '▁\"'",
           "ID: 1226<br>Token: '▁car'",
           "ID: 777<br>Token: '▁''",
           "ID: 3031<br>Token: '▁‘'",
           "ID: 5250<br>Token: '▁usually'"
          ],
          [
           "ID: 664<br>Token: '▁\"'",
           "ID: 777<br>Token: '▁''",
           "ID: 1226<br>Token: '▁car'",
           "ID: 3031<br>Token: '▁‘'",
           "ID: 1080<br>Token: '▁“'"
          ],
          [
           "ID: 664<br>Token: '▁\"'",
           "ID: 777<br>Token: '▁''",
           "ID: 235248<br>Token: '▁'",
           "ID: 476<br>Token: '▁a'",
           "ID: 3031<br>Token: '▁‘'"
          ],
          [
           "ID: 777<br>Token: '▁''",
           "ID: 664<br>Token: '▁\"'",
           "ID: 3031<br>Token: '▁‘'",
           "ID: 476<br>Token: '▁a'",
           "ID: 1508<br>Token: '▁very'"
          ],
          [
           "ID: 777<br>Token: '▁''",
           "ID: 573<br>Token: '▁the'",
           "ID: 476<br>Token: '▁a'",
           "ID: 235248<br>Token: '▁'",
           "ID: 664<br>Token: '▁\"'"
          ]
         ],
         "text": [
          [
           "'▁is'",
           "'▁was'",
           "'▁are'",
           "'▁has'",
           "'▁Is'"
          ],
          [
           "'<bos>'",
           "'▁is'",
           "'▁was'",
           "'▁are'",
           "'▁has'"
          ],
          [
           "'▁is'",
           "'<bos>'",
           "'▁are'",
           "'▁was'",
           "'▁has'"
          ],
          [
           "'<bos>'",
           "'▁is'",
           "'▁was'",
           "'▁has'",
           "'▁are'"
          ],
          [
           "'<bos>'",
           "'▁is'",
           "'▁was'",
           "'▁are'",
           "'▁has'"
          ],
          [
           "'<bos>'",
           "'▁is'",
           "'▁was'",
           "'▁are'",
           "'▁not'"
          ],
          [
           "'▁is'",
           "'▁was'",
           "'▁nahilalakip'",
           "'▁are'",
           "'▁not'"
          ],
          [
           "'▁is'",
           "'▁was'",
           "'▁are'",
           "'▁not'",
           "'▁be'"
          ],
          [
           "'▁is'",
           "'▁nahilalakip'",
           "'▁was'",
           "'▁are'",
           "'▁has'"
          ],
          [
           "'▁is'",
           "'▁was'",
           "'▁are'",
           "'▁not'",
           "'▁translated'"
          ],
          [
           "'▁is'",
           "'▁sometimes'",
           "'▁was'",
           "'▁are'",
           "'▁nahilalakip'"
          ],
          [
           "'▁is'",
           "'▁are'",
           "'▁was'",
           "'▁also'",
           "'▁a'"
          ],
          [
           "'▁is'",
           "'▁are'",
           "'▁'",
           "'▁was'",
           "'▁a'"
          ],
          [
           "'▁is'",
           "'▁simply'",
           "'▁not'",
           "'▁also'",
           "'▁in'"
          ],
          [
           "'▁is'",
           "'▁simply'",
           "'▁usually'",
           "'▁very'",
           "'▁also'"
          ],
          [
           "'▁usually'",
           "'▁very'",
           "'▁simply'",
           "'▁not'",
           "'▁basically'"
          ],
          [
           "'▁simply'",
           "'▁also'",
           "'▁either'",
           "'▁usually'",
           "'▁not'"
          ],
          [
           "'▁\"'",
           "'▁simply'",
           "'▁either'",
           "'▁also'",
           "'▁usually'"
          ],
          [
           "'▁simply'",
           "'▁\"'",
           "'▁either'",
           "'▁different'",
           "'▁''"
          ],
          [
           "'▁car'",
           "'▁simply'",
           "'▁\"'",
           "'▁“'",
           "'▁also'"
          ],
          [
           "'▁car'",
           "'▁''",
           "'▁\"'",
           "'▁‘'",
           "'▁usually'"
          ],
          [
           "'▁\"'",
           "'▁car'",
           "'▁''",
           "'▁‘'",
           "'▁usually'"
          ],
          [
           "'▁\"'",
           "'▁''",
           "'▁car'",
           "'▁‘'",
           "'▁“'"
          ],
          [
           "'▁\"'",
           "'▁''",
           "'▁'",
           "'▁a'",
           "'▁‘'"
          ],
          [
           "'▁''",
           "'▁\"'",
           "'▁‘'",
           "'▁a'",
           "'▁very'"
          ],
          [
           "'▁''",
           "'▁the'",
           "'▁a'",
           "'▁'",
           "'▁\"'"
          ]
         ],
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          0,
          1,
          2,
          3,
          4
         ],
         "xaxis": "x",
         "y": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25
         ],
         "yaxis": "y",
         "z": [
          [
           1,
           2.3151466180599098e-24,
           8.42044016187807e-28,
           2.0047263587963015e-29,
           8.650025455723612e-30
          ],
          [
           0.9992285966873169,
           0.0007713921368122101,
           1.7992664663191533e-21,
           1.7071412450799772e-21,
           1.0732710967478325e-24
          ],
          [
           1,
           8.535231188488979e-9,
           1.6947293330645376e-15,
           2.240163123377821e-16,
           1.1926126290028268e-17
          ],
          [
           1,
           2.987542657706399e-9,
           1.3279856372922591e-18,
           6.402782484178761e-20,
           2.0227336708856153e-20
          ],
          [
           0.9999854564666748,
           0.000014573995940736495,
           2.1106125180914148e-10,
           1.82993550867927e-12,
           1.0954423132480962e-12
          ],
          [
           1,
           7.569549948000187e-19,
           8.413709383389827e-25,
           3.975750000901366e-25,
           7.895352118131407e-27
          ],
          [
           0.9999974966049194,
           0.000002450321971991798,
           2.4282755362037278e-8,
           1.404038485475212e-8,
           8.987405486493572e-9
          ],
          [
           0.9998475313186646,
           0.00015150685794651508,
           3.738690850241255e-7,
           2.4958359290394583e-7,
           1.4379475032910705e-7
          ],
          [
           0.9978814721107483,
           0.002040071180090308,
           0.00007608687883475795,
           0.0000017660672710917424,
           2.2895261508892872e-7
          ],
          [
           0.9993451237678528,
           0.0004691276990342885,
           0.00009598900942364708,
           0.00001783434890967328,
           0.00001303147291764617
          ],
          [
           0.9998301267623901,
           0.00005989967394270934,
           0.00004061039362568408,
           0.000021555966668529436,
           0.00001177772537630517
          ],
          [
           0.9999406337738037,
           0.00003853342423099093,
           0.000015297280697268434,
           0.0000010349804142606445,
           9.18196576549235e-7
          ],
          [
           0.9998801946640015,
           0.00005662661715177819,
           0.000030496132239932194,
           0.000013081919860269409,
           0.000005348958893591771
          ],
          [
           0.9877098798751831,
           0.00826561264693737,
           0.002280906541272998,
           0.0006596657913178205,
           0.00020977204258088022
          ],
          [
           0.4871099293231964,
           0.36218827962875366,
           0.05066414549946785,
           0.030026275664567947,
           0.02534375712275505
          ],
          [
           0.5050486326217651,
           0.223493292927742,
           0.10977663844823837,
           0.10093633830547333,
           0.0150370541960001
          ],
          [
           0.3661052882671356,
           0.2514839768409729,
           0.1304149329662323,
           0.10803717374801636,
           0.07067609578371048
          ],
          [
           0.46861255168914795,
           0.3733682930469513,
           0.10170669108629227,
           0.023432526737451553,
           0.011813758872449398
          ],
          [
           0.7756552696228027,
           0.20536597073078156,
           0.006097967736423016,
           0.0056435284204781055,
           0.0024093182291835546
          ],
          [
           0.6871451139450073,
           0.15660768747329712,
           0.14415711164474487,
           0.002450173022225499,
           0.0018776600481942296
          ],
          [
           0.981977641582489,
           0.017486538738012314,
           0.0005235348362475634,
           0.000009828595466387924,
           0.000001020675426843809
          ],
          [
           0.4706916809082031,
           0.4045005142688751,
           0.1247280165553093,
           0.0000411471955885645,
           0.000014091730918153189
          ],
          [
           0.797338604927063,
           0.1997614949941635,
           0.0027032806538045406,
           0.00015175306180026382,
           0.000042159281292697415
          ],
          [
           0.8389861583709717,
           0.15910100936889648,
           0.0011611240915954113,
           0.00028001455939374864,
           0.0002583090972620994
          ],
          [
           0.9999836683273315,
           0.000015939473087200895,
           3.3427886592107825e-7,
           1.2600869681023141e-8,
           1.1745839856303064e-8
          ],
          [
           0.9999192953109741,
           0.00002427246727165766,
           0.000012648476513277274,
           0.000010786672646645457,
           0.000007066440048220102
          ]
         ],
         "zmax": 1,
         "zmin": 0
        }
       ],
       "layout": {
        "height": 1000,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Top 5 tokens at each layer for 'The translation of 'car' in French is"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Tokens"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Layers"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompts DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prompt</th>\n",
       "      <td>The translation of 'car' in French is</td>\n",
       "      <td>The translation of 'cat' in Spanish is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_string</th>\n",
       "      <td>[voiture, bagnole]</td>\n",
       "      <td>[gato, minino]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_string</th>\n",
       "      <td>car</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format_string</th>\n",
       "      <td>'</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_tokens</th>\n",
       "      <td>[voiture, ▁voiture, bagno, ▁bagno]</td>\n",
       "      <td>[gato, ▁gato, min, ▁min]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_tokens</th>\n",
       "      <td>[car, ▁car]</td>\n",
       "      <td>[cat, ▁cat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format_tokens</th>\n",
       "      <td>[', ▁']</td>\n",
       "      <td>[', ▁']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  \\\n",
       "prompt          The translation of 'car' in French is   \n",
       "target_string                      [voiture, bagnole]   \n",
       "english_string                                    car   \n",
       "format_string                                       '   \n",
       "target_tokens      [voiture, ▁voiture, bagno, ▁bagno]   \n",
       "english_tokens                            [car, ▁car]   \n",
       "format_tokens                                 [', ▁']   \n",
       "\n",
       "                                                     1  \n",
       "prompt          The translation of 'cat' in Spanish is  \n",
       "target_string                           [gato, minino]  \n",
       "english_string                                     cat  \n",
       "format_string                                        '  \n",
       "target_tokens                 [gato, ▁gato, min, ▁min]  \n",
       "english_tokens                             [cat, ▁cat]  \n",
       "format_tokens                                  [', ▁']  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nnterp.display import plot_topk_tokens, prompts_to_df\n",
    "\n",
    "probs = logit_lens(demo_model, prompts_str[0])\n",
    "# Visualize top tokens from logit lens\n",
    "plot_topk_tokens(\n",
    "    probs[0],\n",
    "    demo_model.tokenizer,\n",
    "    k=5,\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    title=\"Top 5 tokens at each layer for 'The translation of 'car' in French is\",\n",
    ")\n",
    "\n",
    "# Convert prompts to DataFrame for analysis\n",
    "df = prompts_to_df(prompts, demo_model.tokenizer)\n",
    "print(\"\\nPrompts DataFrame:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4546107d",
   "metadata": {},
   "source": [
    "# Advanced usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636df112",
   "metadata": {},
   "source": [
    "## Adding support for new models\n",
    "\n",
    "Sometime, your model might not be supported yet by nnterp. In this case, you'll be able to use a `RenameConfig` to properly initialize your model.\n",
    "\n",
    "In this section, I'll show you the steps I took to add support for the `gpt2` to `nnterp`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad5f1b",
   "metadata": {},
   "source": [
    "###  Renaming a module not automatically renamed\n",
    "\n",
    "Let's say that you load a `gpt2` model that is a bit special: every module is called \"super_module\" instead of \"module\".\n",
    "\n",
    "First, let's build such a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5361b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (super_transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (super_h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (super_mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (super_attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "for layer in model.transformer.h:\n",
    "    layer.super_mlp = layer.mlp\n",
    "    delattr(layer, \"mlp\")\n",
    "    layer.super_attn = layer.attn\n",
    "    delattr(layer, \"attn\")\n",
    "model.transformer.super_h = model.transformer.h\n",
    "delattr(model.transformer, \"h\")\n",
    "# Let's keep the final layer norm as is\n",
    "# model.transformer.super_ln_f = model.transformer.ln_f\n",
    "# delattr(model.transformer, \"ln_f\")\n",
    "model.super_transformer = model.transformer\n",
    "delattr(model, \"transformer\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6cb2b1",
   "metadata": {},
   "source": [
    "now if we try to use nnterp, the renaming check automatically performed will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9e05a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-11 14:40:36.681\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m68\u001b[0m - \u001b[33m\u001b[1mStatus file data/status.json not found. Can't access tested models.\u001b[0m\n",
      "\u001b[32m2025-07-11 14:40:36.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_81816/3127552474.py\", line 5, in <module>\n",
      "    StandardizedTransformer(model)\n",
      "  File \"/workspace/nnterp/nnterp/standardized_transformer.py\", line 196, in __init__\n",
      "    self.num_layers = len(self.layers)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 1061, in __getattr__\n",
      "    raise AttributeError(f\"{self} has no attribute {name}\")\n",
      "AttributeError: GPT2LMHeadModel(\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (super_transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (norm/ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (super_h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (super_mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (super_attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): WrapperModule()\n",
      ") has no attribute layers\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "from traceback import print_exc \n",
    "\n",
    "try:\n",
    "    StandardizedTransformer(model)\n",
    "except Exception as e:\n",
    "    print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4039b096",
   "metadata": {},
   "source": [
    "`nnterp` can't find the layers because they're located under `super_transformer`, that nnterp doesn't know about. We have 2 choices in this case:\n",
    "1. Rename `super_transformer` to `model` and `super_h` to `layers` such that it matches the `model.model.layers` Llama architecture and let `nnterp` do the rest.\n",
    "2. Rename `super_transformer.super_h` directly to `layers`, matching the StandardizedTransformer architecture.\n",
    "\n",
    "Let's try the second option first. And let's not forget that we still need to rename\n",
    "\n",
    "In order to do that we can instantiate a `StandardizedTransformer` with a `RenameConfig` with the correct aliases provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5405d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-11 14:40:36.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_81816/4123434686.py\", line 9, in <module>\n",
      "    StandardizedTransformer(model, rename_config=rename_cfg)\n",
      "  File \"/workspace/nnterp/nnterp/standardized_transformer.py\", line 198, in __init__\n",
      "    check_model_renaming(self, repo_id, ignores, allow_dispatch)\n",
      "  File \"/workspace/nnterp/nnterp/rename_utils.py\", line 450, in check_model_renaming\n",
      "    raise RenamingError(\n",
      "nnterp.rename_utils.RenamingError: Could not find norm module in GPT2LMHeadModel(\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (super_transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (super_h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (super_mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (super_attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): WrapperModule()\n",
      ") architecture. This means that it was not properly renamed.\n",
      "Please pass the name of the norm module to the ln_final_rename argument.\n"
     ]
    }
   ],
   "source": [
    "from nnterp.rename_utils import RenameConfig\n",
    "\n",
    "rename_cfg = RenameConfig(\n",
    "    layers_name=\".super_transformer.super_h\",\n",
    "    attn_name=\"super_attn\",\n",
    "    mlp_name=\"super_mlp\",\n",
    ")\n",
    "try:\n",
    "    StandardizedTransformer(model, rename_config=rename_cfg)\n",
    "except Exception as e:\n",
    "    print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee40cd9b",
   "metadata": {},
   "source": [
    "We're still getting an error because `nnterp` doesn't find the `ln_f`. This is because `nnterp` will automatically rename the `ln_f` to `ln_final`, but fails to rename `model.ln_final` to `ln_final`. Again, we can either rename `super_transformer` to `model` or directly rename `super_transformer.ln_f` to `ln_final`.\n",
    "\n",
    "⚠️ The code will still fail, because our \"super_gpt2\" model can't run its forward pass as we deleted its modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc45cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-11 15:43:36.299\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.utils\u001b[0m:\u001b[36mtry_with_scan\u001b[0m:\u001b[36m82\u001b[0m - \u001b[33m\u001b[1mError when trying to scan the model - using .trace() instead (which will dispatch the model)...\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/nnterp/nnterp/utils.py\", line 86, in try_with_scan\n",
      "    with model.trace(dummy_inputs()) as tracer:\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py\", line 408, in __exit__\n",
      "    self.backend(self)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 24, in __call__\n",
      "    raise wrap_exception(e, tracer.info) from None\n",
      "nnsight.NNsightException: \n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 21, in __call__\n",
      "    tracer.execute(fn)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 331, in execute\n",
      "    self.model.interleave(interleaver, self.fn, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py\", line 76, in interleave\n",
      "    return super().interleave(interleaver, fn, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 725, in interleave\n",
      "    interleaver(fn, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 317, in __call__\n",
      "    fn(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 387, in __call__\n",
      "    else self._module(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 132, in inner\n",
      "    value = fn(module, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n",
      "    return inner()\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1189, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1940, in __getattr__\n",
      "    raise AttributeError(\n",
      "\n",
      "AttributeError: 'GPT2LMHeadModel' object has no attribute 'transformer'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_81816/2632821507.py\", line 11, in <module>\n",
      "    StandardizedTransformer(\n",
      "  File \"/workspace/nnterp/nnterp/standardized_transformer.py\", line 198, in __init__\n",
      "    returns_tuple=mlp_returns_tuple(self._model, rename_config),\n",
      "  File \"/workspace/nnterp/nnterp/rename_utils.py\", line 472, in check_model_renaming\n",
      "    try_with_scan(\n",
      "  File \"/workspace/nnterp/nnterp/utils.py\", line 90, in try_with_scan\n",
      "    raise error_to_throw from e2\n",
      "nnterp.rename_utils.RenamingError: Could not check the IO of GPT2LMHeadModel(\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (super_transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (super_h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (super_mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (super_attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): WrapperModule()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rename_cfg = RenameConfig(\n",
    "    model_name=\"super_transformer\",\n",
    "    layers_name=\"super_h\",\n",
    "    attn_name=\"super_attn\",\n",
    "    mlp_name=\"super_mlp\",\n",
    "    ln_final_name=\".super_transformer.ln_f\",\n",
    ")\n",
    "from transformers import AutoConfig\n",
    "\n",
    "try:\n",
    "    StandardizedTransformer(\n",
    "        model, rename_config=rename_cfg, config=AutoConfig.from_pretrained(\"gpt2\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c685c3",
   "metadata": {},
   "source": [
    "## Adding attention probabilities support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169bfa74",
   "metadata": {},
   "source": [
    "To access the attention probabilities, `nnterp` uses the `NNsight` ability to hook on most intermediate variables of the forward pass. This is very architecture dependent, as even 2 equivalent models, if they use different names for the intermediate variables, will need different hooks.\n",
    "\n",
    "As I'm writing this tutorial, I'm adding support for attention probabilities for `GPTJ` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9b4448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-11 17:36:15.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
      "\u001b[32m2025-07-11 17:36:16.983\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m171\u001b[0m - \u001b[33m\u001b[1myujiepan/gptj-tiny-random's architecture is not tested. This may cause unexpected behavior. It is recommended to check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()` if you plan on using it (prettier in a notebook).\n",
      "Feel free to open an issue on github (https://github.com/butanium/nnterp/issues) or run the tests yourself with a toy model if you want to add test coverage for this model.\u001b[0m\n",
      "\u001b[32m2025-07-11 17:36:17.823\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m213\u001b[0m - \u001b[31m\u001b[1mAttention probabilities is not available for yujiepan/gptj-tiny-random architecture. Disabling it. Error:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/nnterp/nnterp/rename_utils.py\", line 309, in check_source\n",
      "    test_prob_source()\n",
      "  File \"/workspace/nnterp/nnterp/rename_utils.py\", line 296, in test_prob_source\n",
      "    probs = self[layer]\n",
      "  File \"/workspace/nnterp/nnterp/rename_utils.py\", line 282, in __getitem__\n",
      "    return self.source_attr(self.model.layers[layer].self_attn).output\n",
      "  File \"/workspace/nnterp/nnterp/rename_utils.py\", line 250, in default_attention_prob_source\n",
      "    attention_module.source.attention_interface_0.source.nn_functional_dropout_0\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 1449, in __getattr__\n",
      "    return super().__getattr__(name)\n",
      "\n",
      "AttributeError: 'super' object has no attribute '__getattr__'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "gptj = StandardizedTransformer(\"yujiepan/gptj-tiny-random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe40784",
   "metadata": {},
   "source": [
    "As you can see, when you load a model,`nnterp` will automatically test if the attention probabilities hook is working and returns a tensor of shape `(batch_size, num_heads, seq_len, seq_len)` where the last dimension sums to 1. In this case, the test failed and `nnterp` logs the error.\n",
    "\n",
    "Now let's look at the `yujiepan/gptj-tiny-random` forward pass and try to understand where are the attention probabilities computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d43ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```py\n",
       "                                 * def forward(\n",
       "                                 0     self,\n",
       "                                 1     hidden_states: torch.FloatTensor,\n",
       "                                 2     layer_past: Optional[Cache] = None,\n",
       "                                 3     attention_mask: Optional[torch.FloatTensor] = None,\n",
       "                                 4     position_ids: Optional[torch.LongTensor] = None,\n",
       "                                 5     head_mask: Optional[torch.FloatTensor] = None,\n",
       "                                 6     use_cache: Optional[bool] = False,\n",
       "                                 7     output_attentions: Optional[bool] = False,\n",
       "                                 8     cache_position: Optional[torch.LongTensor] = None,\n",
       "                                 9 ) -> Union[\n",
       "                                10     tuple[torch.Tensor, tuple[torch.Tensor]],\n",
       "                                11     Optional[tuple[torch.Tensor, tuple[torch.Tensor], tuple[torch.Tensor, ...]]],\n",
       "                                12 ]:\n",
       " self_q_proj_0               -> 13     query = self.q_proj(hidden_states)\n",
       " self_k_proj_0               -> 14     key = self.k_proj(hidden_states)\n",
       " self_v_proj_0               -> 15     value = self.v_proj(hidden_states)\n",
       "                                16 \n",
       " self__split_heads_0         -> 17     query = self._split_heads(query, self.num_attention_heads, self.head_dim, True)\n",
       " self__split_heads_1         -> 18     key = self._split_heads(key, self.num_attention_heads, self.head_dim, True)\n",
       " self__split_heads_2         -> 19     value = self._split_heads(value, self.num_attention_heads, self.head_dim, False)\n",
       "                                20 \n",
       " is_torch_fx_proxy_0         -> 21     if is_torch_fx_proxy(position_ids) or torch.jit.is_tracing():\n",
       " torch_jit_is_tracing_0      ->  +     ...\n",
       "                                22         # The logic to conditionally copy to GPU could not be traced, so we do this\n",
       "                                23         # every time in the torch.fx case\n",
       " get_embed_positions_0       -> 24         embed_positions = get_embed_positions(self.embed_positions, position_ids)\n",
       "                                25     else:\n",
       " self__get_embed_positions_0 -> 26         embed_positions = self._get_embed_positions(position_ids)\n",
       "                                27 \n",
       " position_ids_unsqueeze_0    -> 28     repeated_position_ids = position_ids.unsqueeze(-1).repeat(1, 1, embed_positions.shape[-1])\n",
       " repeat_0                    ->  +     ...\n",
       " torch_gather_0              -> 29     sincos = torch.gather(embed_positions, 1, repeated_position_ids)\n",
       " torch_split_0               -> 30     sin, cos = torch.split(sincos, sincos.shape[-1] // 2, dim=-1)\n",
       "                                31 \n",
       "                                32     if self.rotary_dim is not None:\n",
       "                                33         k_rot = key[:, :, :, : self.rotary_dim]\n",
       "                                34         k_pass = key[:, :, :, self.rotary_dim :]\n",
       "                                35 \n",
       "                                36         q_rot = query[:, :, :, : self.rotary_dim]\n",
       "                                37         q_pass = query[:, :, :, self.rotary_dim :]\n",
       "                                38 \n",
       " apply_rotary_pos_emb_0      -> 39         k_rot = apply_rotary_pos_emb(k_rot, sin, cos)\n",
       " apply_rotary_pos_emb_1      -> 40         q_rot = apply_rotary_pos_emb(q_rot, sin, cos)\n",
       "                                41 \n",
       " torch_cat_0                 -> 42         key = torch.cat([k_rot, k_pass], dim=-1)\n",
       " torch_cat_1                 -> 43         query = torch.cat([q_rot, q_pass], dim=-1)\n",
       "                                44     else:\n",
       " apply_rotary_pos_emb_2      -> 45         key = apply_rotary_pos_emb(key, sin, cos)\n",
       " apply_rotary_pos_emb_3      -> 46         query = apply_rotary_pos_emb(query, sin, cos)\n",
       "                                47 \n",
       " key_permute_0               -> 48     key = key.permute(0, 2, 1, 3)\n",
       " query_permute_0             -> 49     query = query.permute(0, 2, 1, 3)\n",
       "                                50 \n",
       "                                51     if layer_past is not None:\n",
       "                                52         cache_kwargs = {\n",
       "                                53             \"sin\": sin,\n",
       "                                54             \"cos\": cos,\n",
       "                                55             \"partial_rotation_size\": self.rotary_dim,\n",
       "                                56             \"cache_position\": cache_position,\n",
       "                                57         }\n",
       " layer_past_update_0         -> 58         key, value = layer_past.update(key, value, self.layer_idx, cache_kwargs)\n",
       "                                59 \n",
       "                                60     # compute self-attention: V x Softmax(QK^T)\n",
       " self__attn_0                -> 61     attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
       "                                62 \n",
       " self__merge_heads_0         -> 63     attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_dim)\n",
       " self_out_proj_0             -> 64     attn_output = self.out_proj(attn_output)\n",
       " self_resid_dropout_0        -> 65     attn_output = self.resid_dropout(attn_output)\n",
       "                                66 \n",
       "                                67     outputs = (attn_output, layer_past)\n",
       "                                68     if output_attentions:\n",
       "                                69         outputs += (attn_weights,)\n",
       "                                70 \n",
       "                                71     return outputs  # a, present, (attentions)\n",
       "                                72 \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nnterp.utils import display_source\n",
    "\n",
    "display_source(gptj.attentions[0].source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f10ad",
   "metadata": {},
   "source": [
    "Lines 60-61:\n",
    "```py\n",
    "                                60     # compute self-attention: V x Softmax(QK^T)\n",
    " self__attn_0                -> 61     attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
    " ```\n",
    "⚠️ Be careful! if you set the hook here, you'll be able to successfully access the attention probabilities, but not to edit them! ⚠️\n",
    "\n",
    "We need to check the source of `self__attn_0` to see where `attn_weights` is used. In order to access a deeper variable like this, we have to actually run the model with `trace` or `scan`. I'd advise to start with `scan` first, but switch to `trace` if you encounter an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f88b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```py\n",
       "                             * def _attn(\n",
       "                             0     self,\n",
       "                             1     query,\n",
       "                             2     key,\n",
       "                             3     value,\n",
       "                             4     attention_mask=None,\n",
       "                             5     head_mask=None,\n",
       "                             6 ):\n",
       "                             7     # Keep the attention weights computation in fp32 to avoid overflow issues\n",
       " query_to_0              ->  8     query = query.to(torch.float32)\n",
       " key_to_0                ->  9     key = key.to(torch.float32)\n",
       "                            10 \n",
       " key_transpose_0         -> 11     attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
       " torch_matmul_0          ->  +     ...\n",
       "                            12     attn_weights = attn_weights / self.scale_attn\n",
       "                            13 \n",
       "                            14     if attention_mask is not None:  # no matter the length, we just slice it\n",
       "                            15         causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n",
       "                            16         attn_weights = attn_weights + causal_mask\n",
       "                            17 \n",
       " nn_functional_softmax_0 -> 18     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       " attn_weights_to_0       -> 19     attn_weights = attn_weights.to(value.dtype)\n",
       " self_attn_dropout_0     -> 20     attn_weights = self.attn_dropout(attn_weights)\n",
       "                            21 \n",
       "                            22     # Mask heads if we want to\n",
       "                            23     if head_mask is not None:\n",
       "                            24         attn_weights = attn_weights * head_mask\n",
       "                            25 \n",
       " torch_matmul_1          -> 26     attn_output = torch.matmul(attn_weights, value)\n",
       "                            27 \n",
       "                            28     return attn_output, attn_weights\n",
       "                            29 \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with gptj.scan(\"a\"):\n",
    "    display_source(gptj.attentions[0].source.self__attn_0.source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60af356",
   "metadata": {},
   "source": [
    "Here, line 20-24:\n",
    "```py\n",
    " self_attn_dropout_0     -> 20     attn_weights = self.attn_dropout(attn_weights)\n",
    "                            21 \n",
    "                            22     # Mask heads if we want to\n",
    "                            23     if head_mask is not None:\n",
    "                            24         attn_weights = attn_weights * head_mask\n",
    "```\n",
    "\n",
    "In the current `NNsight` version, the results of operators like `*` are not hooked. But even if they were, I'd be careful to use line 24 here, as it's inside a `if` statement. Therefore, we'll use `self_attn_dropout_0` instead.\n",
    "\n",
    "Note that we could also look at `torch_matmul_1` input and edit the value here. However, this looks less robust to me as it assumes this is the only place where `attn_weights` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df59d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "with gptj.scan(th.tensor([[1, 2, 3]])):\n",
    "    print(gptj.attentions[0].source.self__attn_0.source.self_attn_dropout_0.output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab6818",
   "metadata": {},
   "source": [
    "Nice! The shape looks good. Now we can initialize our model with the right RenameConfig, and let `nnterp` run the tests for us.\n",
    "\n",
    "To do this, we'll need to create a `AttnProbFunction` and implement the `get_attention_prob_source` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e814dce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-11 17:44:27.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
      "\u001b[32m2025-07-11 17:44:27.859\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m173\u001b[0m - \u001b[33m\u001b[1myujiepan/gptj-tiny-random's architecture is not tested. This may cause unexpected behavior. It is recommended to check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()` if you plan on using it (prettier in a notebook).\n",
      "Feel free to open an issue on github (https://github.com/butanium/nnterp/issues) or run the tests yourself with a toy model if you want to add test coverage for this model.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_probs.shape: torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "from nnterp.rename_utils import AttnProbFunction, RenameConfig\n",
    "\n",
    "\n",
    "class GPTJAttnProbFunction(AttnProbFunction):\n",
    "\n",
    "    def get_attention_prob_source(\n",
    "        self, attention_module, return_module_source: bool = False\n",
    "    ):\n",
    "        if return_module_source:\n",
    "            # in this case, return source of the module from where the attention probabilities are computed\n",
    "            return attention_module.source.self__attn_0.source\n",
    "        else:\n",
    "            # in this case, return the attention probabilities hook\n",
    "            return attention_module.source.self__attn_0.source.self_attn_dropout_0\n",
    "\n",
    "\n",
    "gptj = StandardizedTransformer(\n",
    "    \"yujiepan/gptj-tiny-random\",\n",
    "    rename_config=RenameConfig(attn_prob_source=GPTJAttnProbFunction()),\n",
    ")\n",
    "\n",
    "with gptj.trace(\"Hello world!\"):\n",
    "    batch_size, seq_len = gptj.input_size\n",
    "    attn_probs = gptj.attention_probabilities[0].save()\n",
    "    print(f\"attn_probs.shape: {attn_probs.shape}\")\n",
    "    assert attn_probs.shape == (batch_size, gptj.num_heads, seq_len, seq_len)\n",
    "    gptj.attention_probabilities[0] = attn_probs / 2\n",
    "    corrupt_logits = gptj.logits.save()\n",
    "\n",
    "with gptj.trace(\"Hello world!\"):\n",
    "    clean_logits = gptj.logits.save()\n",
    "\n",
    "assert gptj.attention_probabilities.enabled\n",
    "assert not th.allclose(clean_logits, corrupt_logits)\n",
    "summed_attn_probs = attn_probs.sum(dim=-1)\n",
    "assert th.allclose(summed_attn_probs, th.ones_like(summed_attn_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f28a6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "`nnterp` provides a unified, standardized interface for working with transformer models, built on top of `nnsight`. Key features include:\n",
    "\n",
    "1. **Standardized naming** across all transformer architectures\n",
    "2. **Easy access** to layer/attention/MLP inputs and outputs\n",
    "3. **Built-in methods** for common operations (steering, skipping layers, projecting to vocab)\n",
    "4. **Efficient activation collection** with batching support\n",
    "5. **Prompt utilities** for tracking target tokens\n",
    "6. **Intervention methods** from mechanistic interpretability research\n",
    "7. **Visualization tools** for analyzing model behavior\n",
    "\n",
    "All of this while maintaining the full power and flexibility of `nnsight` under the hood!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7778f384",
   "metadata": {},
   "source": [
    "# Appendix: `NNsight` cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322dceff",
   "metadata": {},
   "source": [
    "## 1) You must execute your interventions in order\n",
    "In the new `NNsight` versions, it is enforced that you must access to model internals *in the same order* as the model execute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74bc15e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-10 17:00:30.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m184\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
      "\u001b[32m2025-07-10 17:00:31.055\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m213\u001b[0m - \u001b[33m\u001b[1mgpt2's architecture has failed tests for this transformer version. Use at your own risks. If you want to be safe use only the renaming feature of nnterp, and do not use model.layers_output and other accessors\u001b[0m\n",
      "\u001b[32m2025-07-10 17:00:31.056\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m217\u001b[0m - \u001b[33m\u001b[1mgpt2's architecture is not tested. This may cause unexpected behavior. It is recommended to check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()` if you plan on using it (prettier in a notebook).\n",
      "Feel free to open an issue on github (https://github.com/butanium/nnterp/issues) or run the tests yourself with a toy model if you want to add test coverage for this model.\u001b[0m\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_571945/2722117291.py\", line 6, in <module>\n",
      "    with nnterp_gpt2.trace(\"My tailor is rich\"):\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py\", line 408, in __exit__\n",
      "    self.backend(self)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 24, in __call__\n",
      "    raise wrap_exception(e, tracer.info) from None\n",
      "nnsight.NNsightException: \n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_571945/2722117291.py\", line 8, in <module>\n",
      "    l1 = nnterp_gpt2.layers_output[1]  # will fail! You need to collect l1 before l2\n",
      "  File \"/workspace/nnterp/nnterp/rename_utils.py\", line 153, in __getitem__\n",
      "    target = module.output\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 157, in output\n",
      "    return self._interleaver.current.request(\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 824, in request\n",
      "    value = self.send(Events.VALUE, requester)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 808, in send\n",
      "    raise response\n",
      "\n",
      "OutOfOrderError: Value was missed for model.transformer.h.1.output.i0. Did you call an Envoy out of order?\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "from traceback import print_exc\n",
    "\n",
    "nnterp_gpt2 = StandardizedTransformer(\"gpt2\")\n",
    "try:\n",
    "    with nnterp_gpt2.trace(\"My tailor is rich\"):\n",
    "        l2 = nnterp_gpt2.layers_output[2]\n",
    "        l1 = nnterp_gpt2.layers_output[1]  # will fail! You need to collect l1 before l2\n",
    "except Exception as e:\n",
    "    print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6252452",
   "metadata": {},
   "source": [
    "## 2) Gradient computation\n",
    "To compute gradients, you need to open a `.backward()` context, and save the gradients *inside it*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba59f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nnterp_gpt2.trace(\"My tailor is rich\"):\n",
    "    l1_out = nnterp_gpt2.layers_output[1]  # get l1 before accessing logits\n",
    "    logits = nnterp_gpt2.output.logits\n",
    "    with logits.sum().backward(\n",
    "        retain_graph=True\n",
    "    ):  # use retain_graph if you want to do multiple backprops\n",
    "        if False:\n",
    "            l1_grad = nnterp_gpt2.layers_output[1].grad.save()\n",
    "            # this would fail as we'd access nnterp_gpt2.layers_output[1] after nnterp_gpt2.output\n",
    "        l1_grad = l1_out.grad.save()\n",
    "    with (logits.sum() ** 2).backward():\n",
    "        l1_grad_2 = l1_out.grad.save()\n",
    "\n",
    "assert not th.allclose(l1_grad, l1_grad_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7396d7f8",
   "metadata": {},
   "source": [
    "## 3) Use tracer.stop() to save useless computations\n",
    "If you're just computing activations, don't forget to call `tracer.stop()` at the end of your trace. This will stop the model from executing the rest of its computations, and save you some time, as demonstrated below (with the contribution of Claude 4 Sonnet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc97afc4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎭 Welcome to the Theatrical Performance Comparison! 🎭\n",
      "============================================================\n",
      "\n",
      "🐌 ACT I: 'The Tragedy of the Unstoppable Tracer' 🐌\n",
      "In which our hero forgets to call tracer.stop()...\n",
      "⏰ Duration of suffering: 0.6250 seconds\n",
      "\n",
      "⚡ ACT II: 'The Redemption of the Stopped Tracer' ⚡\n",
      "Our hero learns the ancient art of tracer.stop()...\n",
      "⏰ Duration of enlightenment: 0.3877 seconds\n",
      "\n",
      "============================================================\n",
      "🎉 THE GRAND RESULTS SPECTACULAR! 🎉\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>🎭 Performance Type</th>\n",
       "      <th>⏱️ Time (seconds)</th>\n",
       "      <th>🎯 Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Without tracer.stop() 🐌</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>Tragic 😭</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With tracer.stop() ⚡</td>\n",
       "      <td>0.3877</td>\n",
       "      <td>Magnificent! 🌟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Time Saved 💰</td>\n",
       "      <td>0.2373</td>\n",
       "      <td>PROFIT! 📈</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        🎭 Performance Type ⏱️ Time (seconds)        🎯 Rating\n",
       "0  Without tracer.stop() 🐌            0.6250        Tragic 😭\n",
       "1     With tracer.stop() ⚡            0.3877  Magnificent! 🌟\n",
       "2             Time Saved 💰            0.2373       PROFIT! 📈"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏎️ SPEEDUP METER 🏎️\n",
      "┌──────────────────────────────────────────────────┐\n",
      "│████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░│\n",
      "└──────────────────────────────────────────────────┘\n",
      "   💫 COSMIC SPEEDUP: 1.61x FASTER! 💫\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(\n",
    "    \"🎭 Welcome to the Theatrical Performance Comparison! 🎭\\n\"\n",
    "    + \"=\" * 60\n",
    "    + \"\\n\\n🐌 ACT I: 'The Tragedy of the Unstoppable Tracer' 🐌\\nIn which our hero forgets to call tracer.stop()...\"\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(30):\n",
    "    with nnterp_gpt2.trace([\"Neel Samba\", \"Chris Aloha\"]):\n",
    "        out5 = nnterp_gpt2.layers_output[5].save()\n",
    "end_time = time.time()\n",
    "nostop_time = end_time - start_time\n",
    "\n",
    "print(\n",
    "    f\"⏰ Duration of suffering: {nostop_time:.4f} seconds\\n\\n⚡ ACT II: 'The Redemption of the Stopped Tracer' ⚡\\nOur hero learns the ancient art of tracer.stop()...\"\n",
    ")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(30):\n",
    "    with nnterp_gpt2.trace([\"Neel Samba\", \"Chris Aloha\"]) as tracer:\n",
    "        out5 = nnterp_gpt2.layers_output[5].save()\n",
    "        tracer.stop()\n",
    "end_time = time.time()\n",
    "stop_time = end_time - start_time\n",
    "\n",
    "print(f\"⏰ Duration of enlightenment: {stop_time:.4f} seconds\")\n",
    "\n",
    "speedup = nostop_time / stop_time\n",
    "time_saved = nostop_time - stop_time\n",
    "\n",
    "# fun display\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n🎉 THE GRAND RESULTS SPECTACULAR! 🎉\\n\" + \"=\" * 60)\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"🎭 Performance Type\": [\n",
    "            \"Without tracer.stop() 🐌\",\n",
    "            \"With tracer.stop() ⚡\",\n",
    "            \"Time Saved 💰\",\n",
    "        ],\n",
    "        \"⏱️ Time (seconds)\": [\n",
    "            f\"{nostop_time:.4f}\",\n",
    "            f\"{stop_time:.4f}\",\n",
    "            f\"{time_saved:.4f}\",\n",
    "        ],\n",
    "        \"🎯 Rating\": [\"Tragic 😭\", \"Magnificent! 🌟\", \"PROFIT! 📈\"],\n",
    "    }\n",
    ")\n",
    "display(results_df)\n",
    "speedup_bars = int(speedup * 10)\n",
    "meter = \"█\" * min(speedup_bars, 48) + \"░\" * (50 - min(speedup_bars, 48))\n",
    "print(\n",
    "    f\"\\n🏎️ SPEEDUP METER 🏎️\\n┌{'─' * 50}┐\\n│{meter}│\\n└{'─' * 50}┘\\n   💫 COSMIC SPEEDUP: {speedup:.2f}x FASTER! 💫\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aaf904",
   "metadata": {},
   "source": [
    "## 4) Using NNsight builtin cache to collect activations\n",
    "\n",
    "`NNsight 0.5` introduces a builtin way to cache activations during the forward pass. Be careful not to call `tracer.stop()` before all the module of the cache have been accessed.\n",
    "\n",
    "NOTE: Currently doesn't work with renamed names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26dfa488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model.transformer.h.0', 'model.transformer.h.2', 'model.transformer.h.4', 'model.transformer.h.6', 'model.transformer.h.8', 'model.transformer.h.10'])\n",
      "(tensor([[[ 6.7070e-01, -1.8503e+00,  3.1870e-01, -1.2145e+00,  9.2637e-01,\n",
      "          -4.4384e-01,  3.1514e+00, -2.7403e+00, -1.2991e+00, -2.4684e-01,\n",
      "           1.6393e+00,  1.1666e+00, -7.5172e-01, -9.1627e-01,  9.2812e-01,\n",
      "           5.5949e-01, -1.3898e+00, -1.1977e+00,  6.4158e-01,  1.5681e-01,\n",
      "           1.6228e+00, -9.9746e-01, -4.3505e-01, -1.3621e+00,  1.2481e+00,\n",
      "           5.5103e-01,  6.9309e-01, -1.7744e+00,  5.7165e-01, -1.3165e+00,\n",
      "           4.5355e-01,  2.3892e-01,  1.1452e+00, -1.5457e+00, -6.0431e-01,\n",
      "          -7.4857e-01,  3.8172e+00, -6.3845e-01, -2.1072e-01,  4.3352e-01,\n",
      "          -1.7843e-01,  1.2019e+00,  2.0452e+00, -9.5562e-01,  3.7681e-01,\n",
      "           3.6692e+00, -3.3548e-01,  1.7966e-01, -9.9919e-01,  1.0720e+00,\n",
      "           8.2490e-01,  1.3279e-01,  1.3782e+00,  5.9792e-02, -8.1983e-01,\n",
      "           3.9198e+00,  2.8448e+00, -2.7525e-01, -5.6255e-01,  9.4997e-01,\n",
      "           1.7219e-01, -4.2022e-01,  4.3993e-01,  3.5795e-01,  3.2357e+00,\n",
      "          -5.3500e-01,  7.3815e-01, -1.9114e+00, -2.0742e-01,  2.4846e-01,\n",
      "          -7.3719e-01, -5.8241e-01,  1.1861e-01, -6.5108e-01,  5.3989e-01,\n",
      "           7.3264e-01, -1.2914e+00,  6.9464e-01,  2.9651e-01, -4.8049e-01,\n",
      "          -2.2145e+00, -1.1202e+00, -3.6351e-01, -3.6644e-02, -1.5592e+00,\n",
      "           5.4680e-01, -4.0526e+00, -2.7868e+00,  2.1119e+00, -2.5027e-01,\n",
      "           1.1373e+00, -1.3301e+00,  3.2010e-01,  6.5666e-01, -2.3427e+00,\n",
      "          -4.4932e-01, -1.1223e-01,  3.7575e-01, -1.1890e+00,  2.0191e+00,\n",
      "          -1.7279e+00,  2.3713e+00, -5.9065e-01, -2.1408e+00, -4.8267e-01,\n",
      "           1.3969e+00,  3.3323e-01,  1.8863e+00, -1.0902e-01, -1.0459e+00,\n",
      "          -5.7396e-02, -1.3300e+00,  1.0879e-01, -1.7075e+00,  1.4237e-01,\n",
      "           6.1561e-01, -2.8082e+00,  9.7018e-01, -1.4957e-01,  1.4489e+00,\n",
      "          -6.5531e-01,  1.0456e+00,  1.1509e+00,  5.0312e-01, -3.7214e-01,\n",
      "          -1.6162e+00, -1.1885e+00,  3.7258e-01, -1.6372e+00,  1.5417e+00,\n",
      "           1.5101e-02,  1.4883e+00,  1.9463e+00,  5.6884e-01, -1.7985e-01,\n",
      "          -2.5918e-01, -1.2781e+00, -3.0756e-01,  7.6530e+02,  2.0898e-01,\n",
      "          -1.3170e+00,  1.2122e+00, -2.0991e-01,  3.0064e-02,  2.3993e+00,\n",
      "           2.2991e-01, -1.5891e-01,  8.4298e-01, -9.6606e-01, -6.2524e-01,\n",
      "           6.6568e-01, -2.7300e+00, -1.5833e+00,  6.5256e-01, -1.9610e+00,\n",
      "           3.3100e-02, -1.4516e+00, -2.6707e+00,  9.3928e-01, -3.0071e-01,\n",
      "          -5.3841e-01,  5.4358e-02,  5.1179e-01,  6.1382e-02, -2.7793e+00,\n",
      "           8.0466e-02,  3.1516e-01, -1.4427e-01,  7.8287e-01, -1.4529e+00,\n",
      "           2.2420e-01,  6.9308e-01, -6.7370e-01,  1.7873e+00, -7.9461e-02,\n",
      "           1.7510e+00,  2.8172e-01, -5.6474e-01,  1.4139e+00,  1.3163e+00,\n",
      "          -1.3676e+00,  3.0171e-01, -1.5021e+00, -1.6115e+00, -4.1069e-01,\n",
      "          -5.1928e-01, -3.4226e-01, -2.6883e-01,  5.8532e-01,  3.9217e-01,\n",
      "          -1.9042e-01,  1.8857e-01, -5.7744e-01, -5.9709e-02, -1.9403e+00,\n",
      "           3.9040e-01,  1.0001e+00, -2.0011e+00, -1.2953e-01, -4.4173e-01,\n",
      "          -1.0358e+00,  2.6437e+00, -2.6533e+00,  7.0766e-01,  1.3952e-01,\n",
      "          -1.5293e+00, -4.5749e-01, -3.9643e-02, -7.2787e-01,  8.3490e-01,\n",
      "           2.0656e+00,  1.4437e-01, -4.3841e-01, -9.6474e-01, -1.1369e+00,\n",
      "           7.4735e-01, -1.2066e+00, -1.0632e+00, -1.3866e+00, -1.0919e+00,\n",
      "          -6.1489e-01,  1.2769e-01,  1.3306e+00, -1.3653e+00, -8.6203e-01,\n",
      "           3.7239e-01,  7.4592e-02,  1.0630e+00, -6.5921e-01,  4.8581e-01,\n",
      "           1.5681e+00,  6.2412e-01,  1.9397e+00,  7.9313e-02,  3.2770e-01,\n",
      "          -1.3567e+00, -1.5840e+00,  3.5070e-01, -7.0539e-01, -1.0851e+00,\n",
      "           1.9995e+00, -3.2235e-01,  1.1203e+00,  6.1648e-01, -1.8907e-01,\n",
      "          -1.6023e+00, -3.7048e-01, -1.1197e+00,  5.6089e-01,  1.7384e+00,\n",
      "           2.7699e-01, -2.3180e+00,  3.5222e-02,  6.4616e-01, -1.2939e+00,\n",
      "           5.3911e-01,  1.6613e+00,  1.1968e+00,  8.9091e-01,  2.7491e-02,\n",
      "           4.5551e-01,  2.9158e-01,  3.1225e-02,  1.9923e-01, -3.7503e-01,\n",
      "          -2.2575e-01, -5.8265e-01,  1.0568e+00, -1.1642e+00,  1.6608e+00,\n",
      "           1.7657e+00,  6.6096e-01, -1.5173e-01,  1.4592e+00, -5.0406e-01,\n",
      "           1.7819e+00,  2.0715e+00,  1.3527e+00, -2.1049e-01,  4.6036e-01,\n",
      "           4.0097e-01, -9.9179e-02,  6.6876e-01, -1.1445e+00,  1.0098e+00,\n",
      "          -2.1958e+00, -7.7329e-01,  3.9126e-01,  5.6261e-01,  7.4934e-01,\n",
      "          -9.0725e-01, -4.4566e-01, -2.8130e+00, -3.1077e-01,  5.2906e-01,\n",
      "          -1.4334e+00, -6.5204e-01, -5.0126e-01,  1.1375e-01,  1.9851e-02,\n",
      "           7.4748e-01,  1.8061e-02,  2.9800e+00,  4.3116e-01,  6.4445e-01,\n",
      "           6.3735e-02,  1.1720e+00,  1.7393e+00,  2.5535e-01, -6.5180e-01,\n",
      "           3.6528e-01,  2.2047e+00, -2.2385e+00, -2.9612e-01, -2.2521e+00,\n",
      "          -1.0492e+00, -1.7240e+00,  1.8167e-01, -1.3623e-01,  1.3082e+00,\n",
      "           6.9062e-01, -7.8701e-01, -9.2673e-01, -3.0565e-01,  2.0558e+00,\n",
      "           7.0405e-01,  5.2608e+00,  4.5207e-01,  1.1442e+00, -1.3327e+00,\n",
      "          -1.2365e+00, -1.0942e+00,  1.8048e+00,  1.6979e-01,  1.7717e+00,\n",
      "          -1.2586e+00,  2.1460e+00, -2.9819e-01,  6.1754e-01, -2.6056e-02,\n",
      "           6.6182e-01, -1.0177e+00, -4.6371e-01, -5.1776e-01, -2.8611e-01,\n",
      "           1.9613e+00, -3.0997e-01, -2.4044e+00,  8.3413e-01, -1.1928e+00,\n",
      "          -1.8856e+00,  4.0521e-01,  1.5359e+00,  2.8738e-02,  2.7128e+00,\n",
      "          -1.2465e+00, -7.1987e-01, -1.3038e+00,  7.6399e-01,  3.4811e-01,\n",
      "          -1.1512e+00, -1.4190e+00, -1.7722e-01, -2.7177e+00, -8.9881e-01,\n",
      "          -2.5211e+00, -2.9427e-01,  1.2716e-01,  2.1785e+00,  2.5464e-02,\n",
      "           1.0543e+00,  1.8231e+00,  3.2076e-01, -2.3150e+01, -4.0466e+00,\n",
      "          -7.7405e-02,  7.1021e-01,  1.7659e+00, -2.7069e+01,  3.1768e-01,\n",
      "          -3.4601e-01,  5.4489e-01,  4.7308e-01, -1.4979e-01, -1.0113e+00,\n",
      "           1.4596e+00, -1.1350e+00,  3.0098e-01, -1.0799e+00, -4.2611e-01,\n",
      "           9.3423e-01, -2.7592e-01,  1.0919e+00, -1.4730e-01, -4.6314e-01,\n",
      "           1.3346e+00, -1.0850e+00, -6.1100e-01, -8.4926e-01,  8.6312e-01,\n",
      "          -2.3617e-01, -1.0347e+00, -1.3727e+00,  1.2794e-01, -8.9640e-01,\n",
      "          -9.3483e-02, -1.4472e-01, -7.8979e-01,  5.1324e-01,  2.1478e-01,\n",
      "          -2.3167e+00,  8.5952e-01, -1.3199e+00, -2.2933e+00, -1.4877e+00,\n",
      "          -3.5944e-02,  1.7191e-01,  9.2098e-01,  1.2463e-01, -1.3087e-01,\n",
      "          -6.1069e-01, -1.7719e-01,  2.2941e+00, -6.6404e-01, -1.0775e-02,\n",
      "          -9.1542e-01, -1.1385e+00, -1.0924e+00,  1.9747e-01,  3.4964e-01,\n",
      "           7.7548e+00, -1.6463e-01,  1.6228e+00,  7.6341e-01, -1.0173e+00,\n",
      "           1.3214e+00, -3.1024e-01, -1.2294e+00, -1.2705e+00,  3.9518e-01,\n",
      "           5.8329e-01, -3.6427e-02, -1.5458e+00,  6.5028e-01,  1.6354e+00,\n",
      "          -4.3070e-01, -4.0658e-01,  2.9948e+03, -1.2733e+00,  9.4257e-01,\n",
      "          -1.9124e+00,  5.4859e-01,  1.3982e-01, -7.0577e-01, -1.7842e-01,\n",
      "          -2.4869e+00, -3.5670e-01,  2.5547e-01, -3.7422e-01, -7.0479e-01,\n",
      "           1.1601e+00,  6.7565e-01,  8.8273e-01, -4.5967e-02,  9.5866e-01,\n",
      "          -1.1638e+00,  4.4426e-01, -4.0961e-01, -8.8361e-01, -6.0962e-01,\n",
      "          -3.1402e-01, -1.2213e+00, -5.5185e-01,  2.5272e-01, -8.0324e-01,\n",
      "          -2.4436e-01, -2.6337e-01, -2.4478e-01, -1.0643e+00, -1.0095e-01,\n",
      "          -1.0392e+00,  3.0175e+01, -1.1100e+00,  1.2143e+00,  5.2001e-01,\n",
      "           3.0244e-01,  2.6807e+00,  1.5390e-01,  1.8569e-01, -1.3597e+00,\n",
      "          -4.7872e-01,  2.2627e+00,  2.3467e-02, -2.3953e-01,  6.2778e-01,\n",
      "          -2.2188e-01,  1.8766e+01, -6.3737e-01,  1.3625e+00,  4.2327e-01,\n",
      "           7.5684e-01, -4.5134e-01, -1.1301e-01, -2.6335e+00,  8.3828e-02,\n",
      "          -2.0662e+00, -7.2458e-02,  1.3657e+00, -1.4022e-01, -2.2398e-02,\n",
      "           6.2300e-01, -5.2762e-01, -1.1461e+00, -1.5115e+00,  1.0680e+00,\n",
      "          -2.0126e-01, -2.6040e-01, -1.1355e+00,  1.8544e-01, -7.4071e-01,\n",
      "          -1.1752e+00,  2.5302e-01, -1.1494e-02, -8.2354e-02,  1.0829e+00,\n",
      "           1.9406e-01, -2.4287e+00, -6.4597e-01, -2.9770e+00,  4.9268e-02,\n",
      "          -3.2653e-01,  7.0100e-01,  5.4269e-01, -3.7603e-01,  8.5870e-01,\n",
      "          -1.4478e-01,  1.5827e-01, -1.1897e+00, -2.3323e-01, -8.1286e-01,\n",
      "           1.5230e+00, -2.0399e+00,  1.4396e+00,  6.8726e-01, -8.9858e-01,\n",
      "          -2.2911e+00, -7.6701e-01, -5.8170e-01, -1.1820e-01, -7.9593e-01,\n",
      "           2.4525e-01,  2.7768e-01,  2.0428e-01,  4.5014e-01, -1.5629e+00,\n",
      "           4.4538e-01, -7.6472e-01,  1.8057e-01, -1.2592e+00, -1.2883e+00,\n",
      "           2.9984e-01,  1.3820e+00, -6.3348e-01,  3.6206e-01, -5.5680e-01,\n",
      "          -5.4283e-01,  4.2635e-01, -1.5599e+00, -9.2857e-01, -1.6660e+00,\n",
      "          -2.4450e+00,  3.4790e-01,  2.1044e+00, -4.7817e-01,  5.4932e-01,\n",
      "          -5.0217e-01,  9.0348e-02,  1.1421e-01, -1.8518e+00,  1.9762e+00,\n",
      "           1.0345e+00, -2.3354e-01,  2.7523e-01,  1.3418e-01, -1.3010e+00,\n",
      "           2.5380e+00, -2.9936e-01, -1.4240e+00,  1.1892e+00,  9.1980e-01,\n",
      "           1.7094e+00,  4.5100e-01, -1.4179e-01, -4.5492e-01,  3.5378e-01,\n",
      "           3.2356e-01, -5.4683e-01,  6.1319e-01,  3.2880e-01, -1.7111e-01,\n",
      "          -1.8562e-01,  2.6664e-01,  6.5863e-01, -1.3924e+00,  3.9588e-01,\n",
      "           4.5106e-01, -6.5210e-02,  1.8174e+00, -2.0621e+00,  1.4510e+00,\n",
      "           2.1972e+00, -2.3321e-01, -4.6185e-02, -3.2126e-01, -1.5128e+00,\n",
      "          -6.0722e-01, -1.3608e+00, -3.4804e-01,  1.5682e+00, -1.2698e+00,\n",
      "          -5.1578e-01,  1.8829e+00,  2.2275e+00,  2.6412e-01, -1.0753e-01,\n",
      "           5.7711e-01, -1.7214e+00, -1.4754e+00,  1.3297e+00, -8.0692e-01,\n",
      "           1.3654e+00, -1.2928e+00, -3.7130e-01, -6.6547e-01,  6.5901e-01,\n",
      "          -6.0256e-01, -1.6869e+00, -3.6666e-01, -1.9289e+00,  1.2031e+00,\n",
      "           9.4492e-01,  1.5833e+00, -2.2415e-01,  5.0332e-01,  1.6932e-01,\n",
      "          -3.6213e-01, -3.6567e-01, -3.8481e+00, -8.0853e-01,  2.2981e+00,\n",
      "           5.9735e-01,  5.6679e-01,  6.7471e-01,  1.1544e-01,  1.0323e+00,\n",
      "           6.6981e-01,  7.9609e-01,  1.2044e+00, -4.8087e-03,  4.5822e-01,\n",
      "           1.0083e+00, -9.7270e-01, -1.0483e+00,  1.1974e-01, -8.7776e-01,\n",
      "           1.3382e+00, -2.9068e-01, -1.1088e+00, -3.8450e-01,  6.6675e-01,\n",
      "           1.6795e+00, -8.3295e-02, -3.5360e-01,  1.3371e+00, -9.5376e+00,\n",
      "           1.5765e+00, -8.3575e-01, -1.9004e+00, -8.5495e-01, -6.6476e-01,\n",
      "          -1.8385e+00,  1.2753e+00, -2.0135e-01,  7.6233e-01, -6.9069e-01,\n",
      "          -6.2178e-01, -3.6600e-01,  8.7826e-01,  1.2405e+00, -7.8938e-01,\n",
      "          -2.0538e+00,  1.2143e+00,  8.0982e-01,  1.7810e+00, -2.0187e-02,\n",
      "           7.4798e-01,  2.1311e+00, -2.2152e-01,  1.1284e+00, -8.9606e-01,\n",
      "           9.9577e-01, -1.0531e+00,  3.3025e-02, -2.5823e+00, -1.0266e+00,\n",
      "          -1.1472e-01,  5.6418e-02, -4.2177e-02, -2.3806e+00,  2.7581e+00,\n",
      "          -3.5021e-01, -5.3082e-01, -1.5315e+00, -8.7174e-01, -1.5640e+00,\n",
      "           9.1119e-02,  1.0829e-01, -2.0930e-02, -1.0351e+00,  2.4476e-01,\n",
      "           8.2697e-01,  1.4717e+00,  1.3922e+00, -2.0491e-01,  2.0728e+00,\n",
      "          -5.9806e-01,  1.7311e+00,  1.5899e+00,  7.9312e-01, -4.4519e-01,\n",
      "           4.0304e-01, -1.0444e+00, -1.6759e+00, -6.2883e-01, -7.7553e-01,\n",
      "           1.0457e+00,  2.3277e+00, -3.9998e-02, -6.1748e-01,  2.7222e+00,\n",
      "          -2.3519e-01,  1.2101e-01, -3.6167e-01, -7.0045e-01,  6.8359e-02,\n",
      "          -4.6685e+00, -2.6207e+00, -7.7318e-01, -1.9568e+00, -3.6817e-01,\n",
      "          -6.6098e-01, -3.2718e-01,  1.0890e+00, -6.8582e-01, -3.3971e-01,\n",
      "           7.8769e-01, -5.3604e+00, -8.8528e-01, -1.5225e+00, -3.1073e-01,\n",
      "           4.6432e-01, -6.0514e-01,  7.8786e-01, -7.8606e-01,  1.9946e-01,\n",
      "          -1.0331e+00, -4.7309e-01, -2.0984e+00]]]),)\n"
     ]
    }
   ],
   "source": [
    "with nnterp_gpt2.trace(\"Hello\") as tracer:\n",
    "    cache = tracer.cache(modules=[layer for layer in nnterp_gpt2.layers[::2]]).save()\n",
    "\n",
    "print(cache.keys())\n",
    "print(cache[\"model.transformer.h.10\"].output)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
