{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Butanium/nnterp/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef12d930",
      "metadata": {
        "id": "ef12d930"
      },
      "source": [
        "# Demo: nnterp Features Showcase\n",
        "\n",
        "This notebook demonstrates the key features of `nnterp`, which aims to offer a unified interface for all transformer models and give best `NNsight` practices for LLMs in everyone's hands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d40bbfdb",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  !pip install git+https://github.com/Butanium/nnterp.git\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67aacf2a",
      "metadata": {
        "id": "67aacf2a"
      },
      "source": [
        "## 1. Standardized Interface\n",
        "\n",
        "Similar to [`transformer_lens`](https://github.com/TransformerLensOrg/TransformerLens), `nnterp` provides a standardized interface for all transformer models.\n",
        "The main difference is that `nnterp` still uses the huggingface implementation under the hood through `NNsight`, while transformer_lens uses its own implementation of the transformer architecture. However, each transformer implementation has its own quirks, such that `transformer_lens` is not able to support all models, and can sometimes have significant difference with the huggingface implementation.\n",
        "\n",
        "Note that `nnterp` doesn't support all models either, since `NNsight` itself doesn't support all architectures. Additionally, because different models use different naming conventions, `nnterp` doesn't support all HuggingFace models, but it does support a good portion of them. When a model is loaded in `nnterp`, automatic tests are performed to verify that the model has been correctly renamed and that `nnterp`'s hooks return the expected shapes. This means that even if an architecture hasn't been officially tested, the simple fact that it loads successfully indicates it's probably working correctly.\n",
        "\n",
        "The way it's implemented is based on the `NNsight` built-in renaming feature, to make all models look like the llama naming convention, without having to write `model.model`, namely:\n",
        "```ocaml\n",
        "StandardizedTransformer\n",
        "├── layers\n",
        "│   ├── self_attn\n",
        "│   └── mlp\n",
        "├── ln_final\n",
        "└── lm_head\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b13faa7f",
      "metadata": {
        "id": "b13faa7f",
        "outputId": "1973563c-f064-417e-acb8-26af6e451f5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 64, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-7): 8 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=64, out_features=256, bias=False)\n",
            "          (up_proj): Linear(in_features=64, out_features=256, bias=False)\n",
            "          (down_proj): Linear(in_features=256, out_features=64, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((64,), eps=1e-06)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((64,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((64,), eps=1e-06)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=64, out_features=32000, bias=False)\n",
            ")\n",
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n",
            "Qwen3MoeForCausalLM(\n",
            "  (model): Qwen3MoeModel(\n",
            "    (embed_tokens): Embedding(151936, 64)\n",
            "    (layers): ModuleList(\n",
            "      (0): Qwen3MoeDecoderLayer(\n",
            "        (self_attn): Qwen3MoeAttention(\n",
            "          (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (k_proj): Linear(in_features=64, out_features=32, bias=False)\n",
            "          (v_proj): Linear(in_features=64, out_features=32, bias=False)\n",
            "          (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (q_norm): Qwen3MoeRMSNorm((32,), eps=1e-06)\n",
            "          (k_norm): Qwen3MoeRMSNorm((32,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Qwen3MoeMLP(\n",
            "          (gate_proj): Linear(in_features=64, out_features=128, bias=False)\n",
            "          (up_proj): Linear(in_features=64, out_features=128, bias=False)\n",
            "          (down_proj): Linear(in_features=128, out_features=64, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Qwen3MoeRMSNorm((64,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3MoeRMSNorm((64,), eps=1e-06)\n",
            "      )\n",
            "      (1): Qwen3MoeDecoderLayer(\n",
            "        (self_attn): Qwen3MoeAttention(\n",
            "          (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (k_proj): Linear(in_features=64, out_features=32, bias=False)\n",
            "          (v_proj): Linear(in_features=64, out_features=32, bias=False)\n",
            "          (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (q_norm): Qwen3MoeRMSNorm((32,), eps=1e-06)\n",
            "          (k_norm): Qwen3MoeRMSNorm((32,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Qwen3MoeSparseMoeBlock(\n",
            "          (gate): Linear(in_features=64, out_features=8, bias=False)\n",
            "          (experts): ModuleList(\n",
            "            (0-7): 8 x Qwen3MoeMLP(\n",
            "              (gate_proj): Linear(in_features=64, out_features=128, bias=False)\n",
            "              (up_proj): Linear(in_features=64, out_features=128, bias=False)\n",
            "              (down_proj): Linear(in_features=128, out_features=64, bias=False)\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (input_layernorm): Qwen3MoeRMSNorm((64,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3MoeRMSNorm((64,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Qwen3MoeRMSNorm((64,), eps=1e-06)\n",
            "    (rotary_emb): Qwen3MoeRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=64, out_features=151936, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "print(AutoModelForCausalLM.from_pretrained(\"Maykeye/TinyLLama-v0\"))\n",
        "print(AutoModelForCausalLM.from_pretrained(\"gpt2\"))\n",
        "print(AutoModelForCausalLM.from_pretrained(\"yujiepan/qwen3-moe-tiny-random\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5259defb",
      "metadata": {
        "id": "5259defb"
      },
      "source": [
        "As you can see, the naming scheme of gpt2 is different from the llama naming convention.\n",
        "A simple way to fix this is to use the `rename` feature of `NNsight` to rename the gpt2 modules to the llama naming convention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b6cbba1f",
      "metadata": {
        "id": "b6cbba1f",
        "outputId": "b9df09da-e2c6-49f8-d71f-5ad0b695f9e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (model/transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (layers/h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attn/attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_final/ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "  (generator): Generator(\n",
            "    (streamer): Streamer()\n",
            "  )\n",
            ")\n",
            "GPT2Attention(\n",
            "  (c_attn): Conv1D()\n",
            "  (c_proj): Conv1D()\n",
            "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from nnsight import LanguageModel\n",
        "\n",
        "model = LanguageModel(\n",
        "    \"gpt2\", rename=dict(transformer=\"model\", h=\"layers\", ln_f=\"ln_final\", attn=\"self_attn\")\n",
        ")\n",
        "print(model)\n",
        "# Access the attn module as if it was a llama model\n",
        "print(model.model.layers[0].self_attn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eebe2e5c",
      "metadata": {
        "id": "eebe2e5c"
      },
      "source": [
        "You can see the that renamed modules are displayed like `(new_name)/old_name`. However, many models family have their own naming convention, `nnterp` has a global renaming scheme that should transform any model to the llama naming convention. The easiest way to use it is to load your model using the `StandardizedTransformer` class that inherits from `nnsight.LanguageModel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f830ab98",
      "metadata": {
        "id": "f830ab98",
        "outputId": "19534ed2-d33e-4982-f8d1-d63fd90497a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-09-18 16:32:41.241\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m130\u001b[0m - \u001b[33m\u001b[1mnnterp was not tested with Transformers version 4.57.0.dev0. Closest below: 4.53.2, closest above: None\n",
            "This is most likely okay, but you may want to at least check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()`. It is recommended to switch to 4.53.2 if possible or:\n",
            "  - run the nnterp tests with your version of transformers to ensure everything works as expected using `python -m nnterp run_tests` to update the status file locally.\n",
            "  - check if the attention probabilities hook makes sense before using them by calling `model.attention_probabilities.print_source()` (prettier in a notebook).\n",
            "Using test status from 4.53.2.\u001b[0m\n",
            "\u001b[32m2025-09-18 16:32:41.242\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m181\u001b[0m - \u001b[33m\u001b[1mnnterp was not tested with NNsight version 0.5.0 for transformers version 4.53.2. Closest below: 0.5.0.dev9, closest above: None\n",
            "This is most likely okay, but you may want to at least check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()`. It is recommended to switch to NNsight 0.5.0.dev9 if possible.\n",
            "Otherwise, consider:\n",
            "  - run the nnterp tests with your version of transformers to ensure everything works as expected using `python -m nnterp run_tests` to update the status file locally.\n",
            "  - check if the attention probabilities hook makes sense before using them by calling `model.attention_probabilities.print_source()` (prettier in a notebook).\n",
            "Using test results from NNsight 0.5.0.dev9.\u001b[0m\n",
            "\u001b[32m2025-09-18 16:32:41.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
            "\u001b[32m2025-09-18 16:32:41.957\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.utils\u001b[0m:\u001b[36mtry_with_scan\u001b[0m:\u001b[36m274\u001b[0m - \u001b[33m\u001b[1mError when trying to scan the model - using .trace() instead (which will dispatch the model)...\u001b[0m\n"
          ]
        },
        {
          "ename": "RenamingError",
          "evalue": "Could not check the IO of gpt2",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
            "File \u001b[0;32m~/github/nnterp/nnterp/utils.py:278\u001b[0m, in \u001b[0;36mtry_with_scan\u001b[0;34m(model, function, error_to_throw, allow_dispatch, warn_if_scan_fails, errors_to_raise)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(dummy_inputs()) \u001b[38;5;28;01mas\u001b[39;00m tracer:\n\u001b[1;32m    279\u001b[0m         function()\n",
            "File \u001b[0;32m~/github/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:416\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/github/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/Users/d0rb/github/nnterp/nnterp/utils.py\", line 323, in try_with_scan\n    function()\n  File \"/Users/d0rb/github/nnterp/nnterp/rename_utils.py\", line 1003, in <lambda>\n    lambda: check_io(std_model, model_name, ignores),\n  File \"/Users/d0rb/github/nnterp/nnterp/rename_utils.py\", line 883, in check_io\n    token_embeddings = std_model.token_embeddings\n  File \"/Users/d0rb/github/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 1048, in __getattr__\n    raise AttributeError(f\"{self} has no attribute {name}\")\n\nAttributeError: GPT2LMHeadModel(\n  (model/transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (layers/h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (self_attn/attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_final/ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  (generator): Generator(\n    (streamer): Streamer()\n  )\n  (layers): ModuleList(\n    (0-11): 12 x GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (self_attn/attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n) has no attribute token_embeddings",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRenamingError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnnterp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardizedTransformer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# You will see the `layers` module printed two times, it'll be explained later.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m nnterp_gpt2 \u001b[38;5;241m=\u001b[39m \u001b[43mStandardizedTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(nnterp_gpt2)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# StandardizedTransformer also use `device_map=\"auto\"` by default:\u001b[39;00m\n",
            "File \u001b[0;32m~/github/nnterp/nnterp/standardized_transformer.py:158\u001b[0m, in \u001b[0;36mStandardizedTransformer.__init__\u001b[0;34m(self, model, trust_remote_code, check_renaming, allow_dispatch, check_attn_probs_with_trace, rename_config, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m get_hidden_size(\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model, raise_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rename_config\u001b[38;5;241m=\u001b[39mrename_config\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_renaming:\n\u001b[0;32m--> 158\u001b[0m     \u001b[43mcheck_model_renaming\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_dispatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_probabilities \u001b[38;5;241m=\u001b[39m AttentionProbabilitiesAccessor(\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m, rename_config\u001b[38;5;241m=\u001b[39mrename_config\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_renaming:\n",
            "File \u001b[0;32m~/github/nnterp/nnterp/rename_utils.py:1001\u001b[0m, in \u001b[0;36mcheck_model_renaming\u001b[0;34m(std_model, model_name, ignores, allow_dispatch)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(std_model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlp\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m RenamingError(\n\u001b[1;32m    997\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find mlp module in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m architecture. This means that it was not properly renamed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    998\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass the name of the mlp module to the mlp_rename argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m         )\n\u001b[0;32m-> 1001\u001b[0m \u001b[43mtry_with_scan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstd_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_io\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mRenamingError\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCould not check the IO of \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors_to_raise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRenamingError\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/github/nnterp/nnterp/utils.py:284\u001b[0m, in \u001b[0;36mtry_with_scan\u001b[0;34m(model, function, error_to_throw, allow_dispatch, warn_if_scan_fails, errors_to_raise)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors_to_raise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e2, errors_to_raise):\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e2\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_throw \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me2\u001b[39;00m\n\u001b[1;32m    285\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing trace() succeed! Error when trying to scan the model:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[0;31mRenamingError\u001b[0m: Could not check the IO of gpt2"
          ]
        }
      ],
      "source": [
        "from nnterp import StandardizedTransformer\n",
        "\n",
        "# You will see the `layers` module printed two times, it'll be explained later.\n",
        "nnterp_gpt2 = StandardizedTransformer(\"gpt2\")\n",
        "print(nnterp_gpt2)\n",
        "# StandardizedTransformer also use `device_map=\"auto\"` by default:\n",
        "nnterp_gpt2.dispatch()\n",
        "print(nnterp_gpt2.model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19fd4f50",
      "metadata": {
        "id": "19fd4f50"
      },
      "source": [
        "Great! But I can see you at the back of the classroom, asking yourself:\n",
        "> \"Why would you create a package that just pass the right dict to the `NNsight` `rename` feature?\"\n",
        "\n",
        "And actually, I'm glad you asked! `StandardizedTransformer` and `nnterp` have a lot of other features, so bear with me!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4541f3b8",
      "metadata": {
        "id": "4541f3b8"
      },
      "source": [
        "## 2. Accessing Modules I/O\n",
        "With `NNsight`, the most robust way to set the residual stream after layer 1 to be the residual stream after layer 0 for a LLama-like model would be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8452f8d",
      "metadata": {
        "id": "e8452f8d",
        "outputId": "254a2890-c288-4e91-9903-a46c557d9c1c"
      },
      "outputs": [],
      "source": [
        "llama = LanguageModel(\"Maykeye/TinyLLama-v0\")\n",
        "with llama.trace(\"hello\"):\n",
        "    llama.model.layers[1].output = (\n",
        "        llama.model.layers[0].output[0],\n",
        "        *llama.model.layers[1].output[1:],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdf0ef20",
      "metadata": {
        "id": "bdf0ef20"
      },
      "source": [
        "Note that the following can cause issues:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60409efd",
      "metadata": {
        "id": "60409efd"
      },
      "outputs": [],
      "source": [
        "with llama.trace(\"hello\"):\n",
        "    # can't do this because .output is a tuple\n",
        "    # llama.model.layers[1].output[0] = llama.model.layers[0].output[0]\n",
        "\n",
        "    # Can cause errors with gradient computation\n",
        "    llama.model.layers[1].output[0][:] = llama.model.layers[0].output[0]\n",
        "\n",
        "with llama.trace(\"hello\"):\n",
        "    # Can cause errors with opt if you do this at its last layer (thanks pytest)\n",
        "    llama.model.layers[1].output = (llama.model.layers[0].output[0],)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91fbd600",
      "metadata": {
        "id": "91fbd600"
      },
      "source": [
        "`nnterp` makes this much cleaner:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f9b56b",
      "metadata": {
        "id": "d8f9b56b"
      },
      "outputs": [],
      "source": [
        "# First, you can access layer inputs and outputs directly:\n",
        "with nnterp_gpt2.trace(\"hello\"):\n",
        "    # Access layer 5's output\n",
        "    layer_5_output = nnterp_gpt2.layers_output[5]\n",
        "    # Set layer 10's output to be layer 5's output\n",
        "    nnterp_gpt2.layers_output[10] = layer_5_output\n",
        "\n",
        "# You can also access attention and MLP outputs:\n",
        "with nnterp_gpt2.trace(\"hello\"):\n",
        "    attn_output = nnterp_gpt2.attentions_output[3]\n",
        "    mlp_output = nnterp_gpt2.mlps_output[3]\n",
        "\n",
        "# And expert router outputs for MoE models:\n",
        "nnterp_qwen3_moe = StandardizedTransformer(\"yujiepan/qwen3-moe-tiny-random\")\n",
        "\n",
        "with nnterp_qwen3_moe.trace(\"hello\"):\n",
        "    router_output = nnterp_qwen3_moe.routers_output[1]\n",
        "    router_probabilities = nnterp_qwen3_moe.router_probabilities[1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f1e53e",
      "metadata": {
        "id": "a2f1e53e"
      },
      "source": [
        "## 3. `nnterp` Guarantees\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29c88a95",
      "metadata": {
        "id": "29c88a95"
      },
      "source": [
        "When designing, `nnterp` I was very worried about silent failures, where you load a model, and then get an unexpected failure in your code downstream, or worst, it doesn't fail but give you fake results. To avoid this, when you load an `nnterp` model, a series of fast tests are run to ensure that:\n",
        "- The model has been correctly renamed\n",
        "- The model module output are of the expected shape\n",
        "- Attention probabilities have the right shape, sum to 1, and changing them changes the output\n",
        "\n",
        "This comes with the trade-off that `nnterp` will dispatch your model when you load it, which can be annoying if you don't want to load the model's weights. Also to be able to access the attention probabibilties, `nnterp` loads your model with the `eager` attention implementation, which can be slower than the default hf implementation. If you don't need the attention probabilities, you can force to use the default hf implementation / another one by passing `attn_implementation=None` or `attn_implementation=\"your_implementation\"`.\n",
        "\n",
        "What `nnterp` can NOT guarantee:\n",
        "- The attention probabilities won't be modified by the model before being multiplied by the values. To ensure this, you can check `model.attention_probabilities.print_source()` (preferably in a notebook for markdown display) to understand where the attention probabilities are computed.\n",
        "- Huggingface's transformers sheringan w\n",
        "\n",
        "If youe model is not properly renamed, you can pass a `RenameConfig` to the `nnterp` constructor to rename the model. See more in the advanced usage section of this demo.\n",
        "\n",
        "On top of that, before releasing a new version of `nnterp`, a series of tests covering most architectures are performed. When you load a model, `nnterp` will check if tests were run for your `nnsight` and `transformers` versions, and will check the tests results for the class of your model. I chose to include the tests in the `nnterp` package, so that if your model architecture has not been tested / you use a different version of `nnsight` or `transformers`, you can run `python -m nnterp run_tests --model-names foo bar --class-names LlamaForCausalLM` to run the tests for your model. `--class-names` allow you to run the tests on a toy model of the same class as your model to make it cheaper and faster."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2111a4ac",
      "metadata": {
        "id": "2111a4ac"
      },
      "source": [
        "## 4. Attention Probabilities\n",
        "\n",
        "For models that support it, you can access attention probabilities directly. You can check if a model supports it by calling `model.supports_attention_probabilities`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "296ac73f",
      "metadata": {
        "id": "296ac73f",
        "outputId": "7a75f4aa-234f-4e85-fee2-f80910524fb7"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "\n",
        "nnterp_gpt2.tokenizer.padding_side = (\n",
        "    \"left\"  # ensure left padding for easy access to the first token\n",
        ")\n",
        "\n",
        "with th.no_grad():\n",
        "    with nnterp_gpt2.trace(\"The cat sat on the mat\"):\n",
        "        # Access attention probabilities for layer 5\n",
        "        attn_probs_l2 = nnterp_gpt2.attention_probabilities[2].save()\n",
        "        attn_probs = nnterp_gpt2.attention_probabilities[5].save()\n",
        "        print(\n",
        "            f\"Attention probs shape will be: (batch, heads, seq_len, seq_len): {attn_probs.shape}\"\n",
        "        )\n",
        "        # knock out the attention to the first token\n",
        "        attn_probs[:, :, :, 0] = 0\n",
        "        attn_probs /= attn_probs.sum(dim=-1, keepdim=True)\n",
        "        corr_logits = nnterp_gpt2.logits.save()\n",
        "    with nnterp_gpt2.trace(\"The cat sat on the mat\"):\n",
        "        baseline_logits = nnterp_gpt2.logits.save()\n",
        "\n",
        "assert not th.allclose(corr_logits, baseline_logits)\n",
        "\n",
        "sums = attn_probs_l2.sum(dim=-1)\n",
        "# last dimension is the attention of token i to all other tokens, so should sum to 1\n",
        "assert th.allclose(sums, th.ones_like(sums))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b5363fb",
      "metadata": {
        "id": "8b5363fb"
      },
      "source": [
        "Under the hood this uses the new tracing system implemented in `NNsight v0.5` which allow to access most model intermediate variables during the forward pass. This means that if the `transformers` implementation were to change, this could break or give unexpected results, so it is recommended to use one of the tested versions of `transformers` and to check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()` if you want to use a different version of `transformers` / a architecture that has not been tested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cb2c8d0",
      "metadata": {
        "id": "4cb2c8d0",
        "outputId": "c06dd067-54a9-4aa8-8c8c-3b03bc0bccc9"
      },
      "outputs": [],
      "source": [
        "nnterp_gpt2.attention_probabilities.print_source()  # pretty markdown display in a notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3317e47b",
      "metadata": {
        "id": "3317e47b"
      },
      "source": [
        "## 5. Builtin interventions\n",
        "\n",
        "`StandardizedTransformer` also provides convenient methods for common operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "665f3981",
      "metadata": {
        "id": "665f3981"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "\n",
        "# Project hidden states to vocabulary using the unembed norm and lm_head\n",
        "with nnterp_gpt2.trace(\"The capital of France is\"):\n",
        "    hidden = nnterp_gpt2.layers_output[5]\n",
        "    logits = nnterp_gpt2.project_on_vocab(hidden)\n",
        "\n",
        "# Skip layers entirely\n",
        "with nnterp_gpt2.trace(\"Hello world\"):\n",
        "    # Skip layer 1\n",
        "    nnterp_gpt2.skip_layer(1)\n",
        "    # Skip layers 2 through 3 (inclusive)\n",
        "    nnterp_gpt2.skip_layers(2, 3)\n",
        "\n",
        "# This is useful if you want to start at a later layer than the first one\n",
        "with nnterp_gpt2.trace(\"Hello world\") as tracer:\n",
        "    layer_6_out = nnterp_gpt2.layers_output[6].save()\n",
        "    tracer.stop()  # avoid computations after layer 6\n",
        "\n",
        "with nnterp_gpt2.trace(\"Hello world\"):\n",
        "    nnterp_gpt2.skip_layers(0, 6, skip_with=layer_6_out)\n",
        "    half_half_logits = nnterp_gpt2.logits.save()\n",
        "\n",
        "with nnterp_gpt2.trace(\"Hello world\"):\n",
        "    vanilla_logits = nnterp_gpt2.logits.save()\n",
        "\n",
        "assert th.allclose(vanilla_logits, half_half_logits)  # they should be the same\n",
        "\n",
        "# Direct steering\n",
        "steering_vector = th.randn(768)  # gpt2 hidden size\n",
        "with nnterp_gpt2.trace(\"The weather today is\"):\n",
        "    nnterp_gpt2.steer(layers=[1, 3], steering_vector=steering_vector, factor=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf6d1c5f",
      "metadata": {
        "id": "cf6d1c5f"
      },
      "source": [
        "## 6. Specific Token Activation Collection\n",
        "\n",
        "`nnterp` provides utilities for collecting activations efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "611a2f3d",
      "metadata": {
        "id": "611a2f3d",
        "outputId": "90a1f5d2-dec6-4c93-d2fe-2a34bc6d8fe3"
      },
      "outputs": [],
      "source": [
        "from nnterp.nnsight_utils import (\n",
        "    get_token_activations,\n",
        "    collect_token_activations_batched,\n",
        ")\n",
        "\n",
        "# Collect activations for specific tokens\n",
        "prompts = [\"The capital of France is\", \"The weather today is\"]\n",
        "with nnterp_gpt2.trace(prompts) as tracer:\n",
        "    # Get last token activations for all layers\n",
        "    activations = get_token_activations(nnterp_gpt2, prompts, idx=-1, tracer=tracer)\n",
        "    # activations shape: (num_layers, batch_size, hidden_size)\n",
        "\n",
        "# For large datasets, use batched collection\n",
        "large_prompts = [\"Sample text \" + str(i) for i in range(100)]\n",
        "batch_activations = collect_token_activations_batched(\n",
        "    nnterp_gpt2,\n",
        "    large_prompts,\n",
        "    batch_size=16,\n",
        "    layers=[3, 9, 11],  # Only collect specific layers, default is all layers\n",
        "    idx=-1,  # Last token (default)\n",
        ")\n",
        "print(f\"Batched activations shape: {batch_activations.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78591b6e",
      "metadata": {
        "id": "78591b6e"
      },
      "source": [
        "## 7. Prompt Utilities\n",
        "\n",
        "`nnterp` provides utilities for working with prompts and tracking probabilities of first tokens of certain strings. It tracks both the first token of \"string\" and \" string\".\n",
        "\n",
        "You can provide multiple string per category, the probabilities returned will be the sum of the probabilities of all the first tokens of the strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6785fc77",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "fd48b349fa8c42f09311e96724480d2e"
          ]
        },
        "id": "6785fc77",
        "outputId": "1429c035-8da2-48f5-8dc5-6d6994f47d27"
      },
      "outputs": [],
      "source": [
        "from nnterp.prompt_utils import Prompt, run_prompts\n",
        "\n",
        "# Create prompts with target tokens to track\n",
        "prompt1 = Prompt.from_strings(\n",
        "    \"The capital of France (not England or Spain) is\",\n",
        "    {\n",
        "        \"target\": \"Paris\",\n",
        "        \"traps\": [\"London\", \"Madrid\"],\n",
        "        \"longstring\": \"the country of France\",\n",
        "    },\n",
        "    nnterp_gpt2.tokenizer,\n",
        ")\n",
        "for name, tokens in prompt1.target_tokens.items():\n",
        "    print(f\"{name}: {nnterp_gpt2.tokenizer.convert_ids_to_tokens(tokens)}\")\n",
        "\n",
        "prompt2 = Prompt.from_strings(\n",
        "    \"The largest planet (not Earth or Neptune) is\",\n",
        "    {\"target\": \"Jupiter\", \"traps\": [\"Earth\", \"Neptune\"], \"longstring\": \"Palace planet\"},\n",
        "    nnterp_gpt2.tokenizer,\n",
        ")\n",
        "for name, tokens in prompt2.target_tokens.items():\n",
        "    print(f\"{name}: {nnterp_gpt2.tokenizer.convert_ids_to_tokens(tokens)}\")\n",
        "\n",
        "# Run prompts and get probabilities for target tokens\n",
        "results = run_prompts(nnterp_gpt2, [prompt1, prompt2], batch_size=2)\n",
        "print(\"Target token probabilities:\")\n",
        "for target, probs in results.items():\n",
        "    print(f\"  {target}: shape {probs.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da692d08",
      "metadata": {
        "id": "da692d08"
      },
      "source": [
        "## 8. Interventions\n",
        "\n",
        "`nnterp` provides several intervention methods inspired by mechanistic interpretability research:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79776001",
      "metadata": {
        "id": "79776001",
        "outputId": "2dfc7fa7-fdd7-4bac-edd5-fca17a115730"
      },
      "outputs": [],
      "source": [
        "from nnterp.interventions import (\n",
        "    logit_lens,\n",
        "    patchscope_lens,\n",
        "    TargetPrompt,\n",
        "    repeat_prompt,\n",
        "    steer,\n",
        ")\n",
        "\n",
        "# Logit Lens: See predictions at each layer\n",
        "prompts = [\"The capital of France is\", \"The sun rises in the\"]\n",
        "probs = logit_lens(nnterp_gpt2, prompts)\n",
        "print(f\"Logit lens output shape: {probs.shape}\")  # (batch, layers, vocab)\n",
        "\n",
        "# Patchscope: Replace activations from one context into another\n",
        "source_prompts = [\"Paris is beautiful\", \"London is foggy\"]\n",
        "custom_target_prompt = TargetPrompt(\"city: Paris\\nfood: croissant\\n?\", -1)\n",
        "target_prompt = repeat_prompt()  # Creates a repetition task\n",
        "custom_repeat_prompt = repeat_prompt(\n",
        "    words=[\"car\", \"cross\", \"azdrfa\"],\n",
        "    rel=\":\",\n",
        "    sep=\"\\n\\n\",\n",
        "    placeholder=\"*\",\n",
        ")\n",
        "print(f\"repeat_prompt: {custom_repeat_prompt}\")\n",
        "print(f\"custom_repeat_prompt: {custom_repeat_prompt}\")\n",
        "patchscope_probs = patchscope_lens(\n",
        "    nnterp_gpt2, source_prompts=source_prompts, target_patch_prompts=target_prompt\n",
        ")\n",
        "print(f\"patchscope_probs: {patchscope_probs.shape}\")\n",
        "\n",
        "# Steering with intervention function\n",
        "with nnterp_gpt2.trace(\"The weather is\"):\n",
        "    steer(nnterp_gpt2, layers=[5, 10], steering_vector=steering_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd02998b",
      "metadata": {
        "id": "fd02998b",
        "lines_to_next_cell": 0
      },
      "source": [
        "You can use a combination of run_prompts and interventions to get the probabilities of certain tokens according to your custom intervention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "069c856f",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f18faf6e738c47f3a0e620e6d5ac79a7",
            "f5c5f1ee0be542a6840b8ada4f8446e6",
            "a7d35f5ccecf44139c136bd57e1adc3e",
            "1d4a4184938c4803806aa230dcc973c6",
            "76e6363859504c99a277dc8254242ebf",
            "6a2e3c12795a4367b3e6af565adb4f11"
          ]
        },
        "id": "069c856f",
        "lines_to_next_cell": 2,
        "outputId": "b92039e0-6db9-4521-f712-04701c4fe570"
      },
      "outputs": [],
      "source": [
        "demo_model = StandardizedTransformer(\"google/gemma-2-2b\")\n",
        "# uncomment if you don't have a GPU\n",
        "# demo_model = nnterp_gpt2\n",
        "try:\n",
        "  import google.colab\n",
        "  print(\"using gpt2 because colab is slow to download gemma\")\n",
        "  demo_model = nnterp_gpt2\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "prompts_str = [\n",
        "    \"The translation of 'car' in French is\",\n",
        "    \"The translation of 'cat' in Spanish is\",\n",
        "]\n",
        "tokens = [\n",
        "    {\"target\": [\"voiture\", \"bagnole\"], \"english\": \"car\", \"format\": \"'\"},\n",
        "    {\"target\": [\"gato\", \"minino\"], \"english\": \"cat\", \"format\": \"'\"},\n",
        "]\n",
        "prompts = [\n",
        "    Prompt.from_strings(prompt, tokens, demo_model.tokenizer)\n",
        "    for prompt, tokens in zip(prompts_str, tokens)\n",
        "]\n",
        "results = run_prompts(demo_model, prompts, batch_size=2, get_probs_func=logit_lens)\n",
        "for category, probs in results.items():\n",
        "    print(f\"{category}: {probs.shape}\")  # (batch, layers)\n",
        "\n",
        "# Create a plotly plot showing mean probabilities for each category across layers\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Calculate mean probabilities across batches for each category and layer\n",
        "mean_probs = {category: probs.mean(dim=0) for category, probs in results.items()}\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add a line for each category\n",
        "for category, probs in mean_probs.items():\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=list(range(len(probs))),\n",
        "            y=probs.tolist(),\n",
        "            mode=\"lines+markers\",\n",
        "            name=category,\n",
        "            line=dict(width=2),\n",
        "            marker=dict(size=6),\n",
        "        )\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Mean Token Probabilities Across Layers\",\n",
        "    xaxis_title=\"Layer\",\n",
        "    yaxis_title=\"Mean Probability\",\n",
        "    hovermode=\"x unified\",\n",
        "    template=\"plotly_white\",\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e7930aa",
      "metadata": {
        "id": "6e7930aa"
      },
      "source": [
        "## 9. Visualization\n",
        "\n",
        "Finally, `nnterp` provides visualization utilities for analyzing model probabilities and prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e63b784e",
      "metadata": {
        "id": "e63b784e",
        "outputId": "adae7b76-a052-4888-8708-cef19c8daaf9"
      },
      "outputs": [],
      "source": [
        "from nnterp.display import plot_topk_tokens, prompts_to_df\n",
        "\n",
        "probs = logit_lens(demo_model, prompts_str[0])\n",
        "# Visualize top tokens from logit lens\n",
        "plot_topk_tokens(\n",
        "    probs[0],\n",
        "    demo_model.tokenizer,\n",
        "    k=5,\n",
        "    width=1000,\n",
        "    height=1000,\n",
        "    title=\"Top 5 tokens at each layer for 'The translation of 'car' in French is\",\n",
        ")\n",
        "\n",
        "# Convert prompts to DataFrame for analysis\n",
        "df = prompts_to_df(prompts, demo_model.tokenizer)\n",
        "print(\"\\nPrompts DataFrame:\")\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4546107d",
      "metadata": {
        "id": "4546107d"
      },
      "source": [
        "# Advanced usage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0557f145",
      "metadata": {
        "id": "0557f145"
      },
      "source": [
        "Sometime, your model might not be supported yet by nnterp. In this case, you'll be able to use a `RenameConfig` to properly initialize your model.\n",
        "\n",
        "In this section, I'll show you the steps I took to add support for the `gpt2` to `nnterp`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36ad5f1b",
      "metadata": {
        "id": "36ad5f1b"
      },
      "source": [
        "###  Renaming a module not automatically renamed\n",
        "\n",
        "Let's say that you load a `gpt2` model that is a bit special: every module is called \"super_module\" instead of \"module\".\n",
        "\n",
        "First, let's build such a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f5361b1",
      "metadata": {
        "id": "1f5361b1",
        "outputId": "6720cdae-69b8-4269-f238-60e29009993e"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "for layer in model.transformer.h:\n",
        "    layer.super_mlp = layer.mlp\n",
        "    delattr(layer, \"mlp\")\n",
        "    layer.super_attn = layer.attn\n",
        "    delattr(layer, \"attn\")\n",
        "model.transformer.super_h = model.transformer.h\n",
        "delattr(model.transformer, \"h\")\n",
        "# Let's keep the final layer norm as is\n",
        "# model.transformer.super_ln_f = model.transformer.ln_f\n",
        "# delattr(model.transformer, \"ln_f\")\n",
        "model.super_transformer = model.transformer\n",
        "delattr(model, \"transformer\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d6cb2b1",
      "metadata": {
        "id": "6d6cb2b1"
      },
      "source": [
        "now if we try to use nnterp, the renaming check automatically performed will fail:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a9e05a4",
      "metadata": {
        "id": "0a9e05a4",
        "outputId": "539ce07a-df03-4833-914c-7cbf9a6dd29e"
      },
      "outputs": [],
      "source": [
        "from nnterp import StandardizedTransformer\n",
        "from traceback import print_exc\n",
        "\n",
        "try:\n",
        "    StandardizedTransformer(model)\n",
        "except Exception as e:\n",
        "    print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4039b096",
      "metadata": {
        "id": "4039b096"
      },
      "source": [
        "`nnterp` can't find the layers because they're located under `super_transformer`, that nnterp doesn't know about. We have 2 choices in this case:\n",
        "1. Rename `super_transformer` to `model` and `super_h` to `layers` such that it matches the `model.model.layers` Llama architecture and let `nnterp` do the rest.\n",
        "2. Rename `super_transformer.super_h` directly to `layers`, matching the StandardizedTransformer architecture.\n",
        "\n",
        "Let's try the second option first. And let's not forget that we still need to rename\n",
        "\n",
        "In order to do that we can instantiate a `StandardizedTransformer` with a `RenameConfig` with the correct aliases provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5405d65b",
      "metadata": {
        "id": "5405d65b",
        "outputId": "583a34cd-32ae-4fce-fefd-a1bb2f3b24ab"
      },
      "outputs": [],
      "source": [
        "from nnterp.rename_utils import RenameConfig\n",
        "\n",
        "rename_cfg = RenameConfig(\n",
        "    layers_name=\".super_transformer.super_h\",\n",
        "    attn_name=\"super_attn\",\n",
        "    mlp_name=\"super_mlp\",\n",
        ")\n",
        "try:\n",
        "    StandardizedTransformer(model, rename_config=rename_cfg)\n",
        "except Exception as e:\n",
        "    print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee40cd9b",
      "metadata": {
        "id": "ee40cd9b"
      },
      "source": [
        "We're still getting an error because `nnterp` doesn't find the `ln_f`. This is because `nnterp` will automatically rename the `ln_f` to `ln_final`, but fails to rename `model.ln_final` to `ln_final`. Again, we can either rename `super_transformer` to `model` or directly rename `super_transformer.ln_f` to `ln_final`.\n",
        "\n",
        "⚠️ The code will still fail, because our \"super_gpt2\" model can't run its forward pass as we deleted its modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbc45cfb",
      "metadata": {
        "id": "fbc45cfb",
        "outputId": "ad5b00b9-418b-474d-f580-fc89c2bf1f06"
      },
      "outputs": [],
      "source": [
        "rename_cfg = RenameConfig(\n",
        "    model_name=\"super_transformer\",\n",
        "    layers_name=\"super_h\",\n",
        "    attn_name=\"super_attn\",\n",
        "    mlp_name=\"super_mlp\",\n",
        "    ln_final_name=\".super_transformer.ln_f\",\n",
        ")\n",
        "from transformers import AutoConfig\n",
        "\n",
        "try:\n",
        "    StandardizedTransformer(\n",
        "        model, rename_config=rename_cfg, config=AutoConfig.from_pretrained(\"gpt2\")\n",
        "    )\n",
        "except Exception as e:\n",
        "    print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91c685c3",
      "metadata": {
        "id": "91c685c3"
      },
      "source": [
        "## Adding attention probabilities support"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169bfa74",
      "metadata": {
        "id": "169bfa74"
      },
      "source": [
        "To access the attention probabilities, `nnterp` uses the `NNsight` ability to hook on most intermediate variables of the forward pass. This is very architecture dependent, as even 2 equivalent models, if they use different names for the intermediate variables, will need different hooks.\n",
        "\n",
        "As I'm writing this tutorial, I'm adding support for attention probabilities for `GPTJ` models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9b4448",
      "metadata": {
        "id": "2a9b4448",
        "outputId": "828fc401-8844-42b1-c7bf-60689939c7db"
      },
      "outputs": [],
      "source": [
        "from nnterp import StandardizedTransformer\n",
        "gptj = StandardizedTransformer(\"yujiepan/gptj-tiny-random\")  # In the current version of nnterp, this will work out of the box"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfe40784",
      "metadata": {
        "id": "cfe40784"
      },
      "source": [
        "As you can see, when you load a model,`nnterp` will automatically test if the attention probabilities hook is working and returns a tensor of shape `(batch_size, num_heads, seq_len, seq_len)` where the last dimension sums to 1. In this case, the test failed and `nnterp` logs the error.\n",
        "\n",
        "Now let's look at the `yujiepan/gptj-tiny-random` forward pass and try to understand where are the attention probabilities computed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29d43ea2",
      "metadata": {
        "id": "29d43ea2",
        "outputId": "3a2e78fe-61c7-4e73-86d0-3ef8d757cca8"
      },
      "outputs": [],
      "source": [
        "from nnterp.utils import display_source\n",
        "\n",
        "display_source(gptj.attentions[0].source)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a14f10ad",
      "metadata": {
        "id": "a14f10ad"
      },
      "source": [
        "Lines 60-61:\n",
        "```py\n",
        "                                60     # compute self-attention: V x Softmax(QK^T)\n",
        " self__attn_0                -> 61     attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
        " ```\n",
        "⚠️ Be careful! if you set the hook here, you'll be able to successfully access the attention probabilities, but not to edit them! ⚠️\n",
        "\n",
        "We need to check the source of `self__attn_0` to see where `attn_weights` is used. In order to access a deeper variable like this, we have to actually run the model with `trace` or `scan`. I'd advise to start with `scan` first, but switch to `trace` if you encounter an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1f88b6d",
      "metadata": {
        "id": "a1f88b6d",
        "outputId": "d874b7d8-1b34-4118-975b-612e3b0b3c57"
      },
      "outputs": [],
      "source": [
        "with gptj.scan(\"a\"):\n",
        "    display_source(gptj.attentions[0].source.self__attn_0.source)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60af356",
      "metadata": {
        "id": "f60af356"
      },
      "source": [
        "Here, line 20-24:\n",
        "```py\n",
        " self_attn_dropout_0     -> 20     attn_weights = self.attn_dropout(attn_weights)\n",
        "                            21\n",
        "                            22     # Mask heads if we want to\n",
        "                            23     if head_mask is not None:\n",
        "                            24         attn_weights = attn_weights * head_mask\n",
        "```\n",
        "\n",
        "In the current `NNsight` version, the results of operators like `*` are not hooked. But even if they were, I'd be careful to use line 24 here, as it's inside a `if` statement. Therefore, we'll use `self_attn_dropout_0` instead.\n",
        "\n",
        "Note that we could also look at `torch_matmul_1` input and edit the value here. However, this looks less robust to me as it assumes this is the only place where `attn_weights` is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2df59d43",
      "metadata": {
        "id": "2df59d43",
        "outputId": "b1e04a79-26a5-4351-936f-f2da57e30947"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "with gptj.scan(th.tensor([[1, 2, 3]])):\n",
        "    print(gptj.attentions[0].source.self__attn_0.source.self_attn_dropout_0.output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2ab6818",
      "metadata": {
        "id": "e2ab6818"
      },
      "source": [
        "Nice! The shape looks good. Now we can initialize our model with the right RenameConfig, and let `nnterp` run the tests for us.\n",
        "\n",
        "To do this, we'll need to create a `AttnProbFunction` and implement the `get_attention_prob_source` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e814dce9",
      "metadata": {
        "id": "e814dce9",
        "outputId": "63322d64-6de3-42ab-c57e-8e1144e43163"
      },
      "outputs": [],
      "source": [
        "from nnterp.rename_utils import AttnProbFunction, RenameConfig\n",
        "\n",
        "\n",
        "class GPTJAttnProbFunction(AttnProbFunction):\n",
        "\n",
        "    def get_attention_prob_source(\n",
        "        self, attention_module, return_module_source: bool = False\n",
        "    ):\n",
        "        if return_module_source:\n",
        "            # in this case, return source of the module from where the attention probabilities are computed\n",
        "            return attention_module.source.self__attn_0.source\n",
        "        else:\n",
        "            # in this case, return the attention probabilities hook\n",
        "            return attention_module.source.self__attn_0.source.self_attn_dropout_0\n",
        "\n",
        "\n",
        "gptj = StandardizedTransformer(\n",
        "    \"yujiepan/gptj-tiny-random\",\n",
        "    rename_config=RenameConfig(attn_prob_source=GPTJAttnProbFunction()),\n",
        ")\n",
        "\n",
        "with gptj.trace(\"Hello world!\"):\n",
        "    batch_size, seq_len = gptj.input_size\n",
        "    attn_probs = gptj.attention_probabilities[0].save()\n",
        "    print(f\"attn_probs.shape: {attn_probs.shape}\")\n",
        "    assert attn_probs.shape == (batch_size, gptj.num_heads, seq_len, seq_len)\n",
        "    gptj.attention_probabilities[0] = attn_probs / 2\n",
        "    corrupt_logits = gptj.logits.save()\n",
        "\n",
        "with gptj.trace(\"Hello world!\"):\n",
        "    clean_logits = gptj.logits.save()\n",
        "\n",
        "assert gptj.attention_probabilities.enabled\n",
        "assert not th.allclose(clean_logits, corrupt_logits)\n",
        "summed_attn_probs = attn_probs.sum(dim=-1)\n",
        "assert th.allclose(summed_attn_probs, th.ones_like(summed_attn_probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e45f28a6",
      "metadata": {
        "id": "e45f28a6"
      },
      "source": [
        "## Summary\n",
        "\n",
        "`nnterp` provides a unified, standardized interface for working with transformer models, built on top of `nnsight`. Key features include:\n",
        "\n",
        "1. **Standardized naming** across all transformer architectures\n",
        "2. **Easy access** to layer/attention/MLP inputs and outputs\n",
        "3. **Built-in methods** for common operations (steering, skipping layers, projecting to vocab)\n",
        "4. **Efficient activation collection** with batching support\n",
        "5. **Prompt utilities** for tracking target tokens\n",
        "6. **Intervention methods** from mechanistic interpretability research\n",
        "7. **Visualization tools** for analyzing model behavior\n",
        "\n",
        "All of this while maintaining the full power and flexibility of `nnsight` under the hood!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7778f384",
      "metadata": {
        "id": "7778f384"
      },
      "source": [
        "# Appendix: `NNsight` cheatsheet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "322dceff",
      "metadata": {
        "id": "322dceff"
      },
      "source": [
        "## 1) You must execute your interventions in order\n",
        "In the new `NNsight` versions, it is enforced that you must access to model internals *in the same order* as the model execute them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74bc15e1",
      "metadata": {
        "id": "74bc15e1",
        "outputId": "71b9e75b-fba3-42a7-adc3-c861f7a78a8e"
      },
      "outputs": [],
      "source": [
        "from nnterp import StandardizedTransformer\n",
        "from traceback import print_exc\n",
        "\n",
        "nnterp_gpt2 = StandardizedTransformer(\"gpt2\")\n",
        "try:\n",
        "    with nnterp_gpt2.trace(\"My tailor is rich\"):\n",
        "        l2 = nnterp_gpt2.layers_output[2]\n",
        "        l1 = nnterp_gpt2.layers_output[1]  # will fail! You need to collect l1 before l2\n",
        "except Exception as e:\n",
        "    print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6252452",
      "metadata": {
        "id": "b6252452"
      },
      "source": [
        "## 2) Gradient computation\n",
        "To compute gradients, you need to open a `.backward()` context, and save the gradients *inside it*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba59f2f9",
      "metadata": {
        "id": "ba59f2f9"
      },
      "outputs": [],
      "source": [
        "with nnterp_gpt2.trace(\"My tailor is rich\"):\n",
        "    l1_out = nnterp_gpt2.layers_output[1]  # get l1 before accessing logits\n",
        "    logits = nnterp_gpt2.output.logits\n",
        "    with logits.sum().backward(\n",
        "        retain_graph=True\n",
        "    ):  # use retain_graph if you want to do multiple backprops\n",
        "        if False:\n",
        "            l1_grad = nnterp_gpt2.layers_output[1].grad.save()\n",
        "            # this would fail as we'd access nnterp_gpt2.layers_output[1] after nnterp_gpt2.output\n",
        "        l1_grad = l1_out.grad.save()\n",
        "    with (logits.sum() ** 2).backward():\n",
        "        l1_grad_2 = l1_out.grad.save()\n",
        "\n",
        "assert not th.allclose(l1_grad, l1_grad_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7396d7f8",
      "metadata": {
        "id": "7396d7f8"
      },
      "source": [
        "## 3) Use tracer.stop() to save useless computations\n",
        "If you're just computing activations, don't forget to call `tracer.stop()` at the end of your trace. This will stop the model from executing the rest of its computations, and save you some time, as demonstrated below (with the contribution of Claude 4 Sonnet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc97afc4",
      "metadata": {
        "id": "dc97afc4",
        "lines_to_next_cell": 2,
        "outputId": "113ce68d-bab2-415f-cff1-646bf6279c12"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "print(\n",
        "    \"🎭 Welcome to the Theatrical Performance Comparison! 🎭\\n\"\n",
        "    + \"=\" * 60\n",
        "    + \"\\n\\n🐌 ACT I: 'The Tragedy of the Unstoppable Tracer' 🐌\\nIn which our hero forgets to call tracer.stop()...\"\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "for _ in range(30):\n",
        "    with nnterp_gpt2.trace([\"Neel Samba\", \"Chris Aloha\"]):\n",
        "        out5 = nnterp_gpt2.layers_output[5].save()\n",
        "end_time = time.time()\n",
        "nostop_time = end_time - start_time\n",
        "\n",
        "print(\n",
        "    f\"⏰ Duration of suffering: {nostop_time:.4f} seconds\\n\\n⚡ ACT II: 'The Redemption of the Stopped Tracer' ⚡\\nOur hero learns the ancient art of tracer.stop()...\"\n",
        ")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "for _ in range(30):\n",
        "    with nnterp_gpt2.trace([\"Neel Samba\", \"Chris Aloha\"]) as tracer:\n",
        "        out5 = nnterp_gpt2.layers_output[5].save()\n",
        "        tracer.stop()\n",
        "end_time = time.time()\n",
        "stop_time = end_time - start_time\n",
        "\n",
        "print(f\"⏰ Duration of enlightenment: {stop_time:.4f} seconds\")\n",
        "\n",
        "speedup = nostop_time / stop_time\n",
        "time_saved = nostop_time - stop_time\n",
        "\n",
        "# fun display\n",
        "print(\"\\n\" + \"=\" * 60 + \"\\n🎉 THE GRAND RESULTS SPECTACULAR! 🎉\\n\" + \"=\" * 60)\n",
        "results_df = pd.DataFrame(\n",
        "    {\n",
        "        \"🎭 Performance Type\": [\n",
        "            \"Without tracer.stop() 🐌\",\n",
        "            \"With tracer.stop() ⚡\",\n",
        "            \"Time Saved 💰\",\n",
        "        ],\n",
        "        \"⏱️ Time (seconds)\": [\n",
        "            f\"{nostop_time:.4f}\",\n",
        "            f\"{stop_time:.4f}\",\n",
        "            f\"{time_saved:.4f}\",\n",
        "        ],\n",
        "        \"🎯 Rating\": [\"Tragic 😭\", \"Magnificent! 🌟\", \"PROFIT! 📈\"],\n",
        "    }\n",
        ")\n",
        "display(results_df)\n",
        "speedup_bars = int(speedup * 10)\n",
        "meter = \"█\" * min(speedup_bars, 48) + \"░\" * (50 - min(speedup_bars, 48))\n",
        "print(\n",
        "    f\"\\n🏎️ SPEEDUP METER 🏎️\\n┌{'─' * 50}┐\\n│{meter}│\\n└{'─' * 50}┘\\n   💫 COSMIC SPEEDUP: {speedup:.2f}x FASTER! 💫\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4aaf904",
      "metadata": {
        "id": "e4aaf904"
      },
      "source": [
        "## 4) Using NNsight builtin cache to collect activations\n",
        "\n",
        "`NNsight 0.5` introduces a builtin way to cache activations during the forward pass. Be careful not to call `tracer.stop()` before all the module of the cache have been accessed.\n",
        "\n",
        "NOTE: Currently the cache doesn't use the renamed names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26dfa488",
      "metadata": {
        "id": "26dfa488",
        "outputId": "cae0a1d0-a42b-42e1-ae7d-05923bd4bca5"
      },
      "outputs": [],
      "source": [
        "with nnterp_gpt2.trace(\"Hello\") as tracer:\n",
        "    cache = tracer.cache(modules=[layer for layer in nnterp_gpt2.layers[::2]]).save()\n",
        "\n",
        "print(cache.keys())\n",
        "print(cache[\"model.transformer.h.10\"].output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
