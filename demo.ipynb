{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9699fa29",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Butanium/nnterp/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12d930",
   "metadata": {
    "id": "ef12d930"
   },
   "source": [
    "# Demo: nnterp Features Showcase\n",
    "\n",
    "This notebook demonstrates the key features of `nnterp`, which aims to offer a unified interface for all transformer models and give best `NNsight` practices for LLMs in everyone's hands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d40bbfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  !pip install git+https://github.com/Butanium/nnterp.git\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8075c",
   "metadata": {},
   "source": [
    "## 1. Standardized Interface\n",
    "\n",
    "Similar to [`transformer_lens`](https://github.com/TransformerLensOrg/TransformerLens), `nnterp` provides a standardized interface for all transformer models.\n",
    "The main difference is that `nnterp` still uses the huggingface implementation under the hood through `NNsight`, while transformer_lens uses its own implementation of the transformer architecture. However, each transformer implementation has its own quirks, such that `transformer_lens` is not able to support all models, and can sometimes have significant difference with the huggingface implementation.\n",
    "\n",
    "Note that `nnterp` doesn't support all models either, since `NNsight` itself doesn't support all architectures. Additionally, because different models use different naming conventions, `nnterp` doesn't support all HuggingFace models, but it does support a good portion of them. When a model is loaded in `nnterp`, automatic tests are performed to verify that the model has been correctly renamed and that `nnterp`'s hooks return the expected shapes. This means that even if an architecture hasn't been officially tested, the simple fact that it loads successfully indicates it's probably working correctly.\n",
    "\n",
    "The way it's implemented is based on the `NNsight` built-in renaming feature, to make all models look like the llama naming convention, without having to write `model.model`, namely:\n",
    "```ocaml\n",
    "StandardizedTransformer\n",
    "├── embed_tokens\n",
    "├── layers\n",
    "│   ├── self_attn\n",
    "│   └── mlp\n",
    "├── ln_final\n",
    "└── lm_head\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "506925b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 64, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=64, out_features=256, bias=False)\n",
      "          (up_proj): Linear(in_features=64, out_features=256, bias=False)\n",
      "          (down_proj): Linear(in_features=256, out_features=64, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((64,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((64,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((64,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=64, out_features=32000, bias=False)\n",
      ")\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "print(AutoModelForCausalLM.from_pretrained(\"Maykeye/TinyLLama-v0\"))\n",
    "print(AutoModelForCausalLM.from_pretrained(\"gpt2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bae472",
   "metadata": {},
   "source": [
    "As you can see, the naming scheme of gpt2 is different from the llama naming convention.\n",
    "A simple way to fix this is to use the `rename` feature of `NNsight` to rename the gpt2 modules to the llama naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e3bbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (model/transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (layers/h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn/attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_final/ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n",
      "GPT2Attention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel(\n",
    "    \"gpt2\",\n",
    "    rename=dict(transformer=\"model\", h=\"layers\", ln_f=\"ln_final\", attn=\"self_attn\"),\n",
    ")\n",
    "print(model)\n",
    "# Access the attn module as if it was a llama model\n",
    "print(model.model.layers[0].self_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d5f88c",
   "metadata": {},
   "source": [
    "You can see the that renamed modules are displayed like `(new_name)/old_name`. However, many models family have their own naming convention, `nnterp` has a global renaming scheme that should transform any model to the llama naming convention. The easiest way to use it is to load your model using the `StandardizedTransformer` class that inherits from `nnsight.LanguageModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b95cf9",
   "metadata": {},
   "outputs": [],
   "source": "from nnterp import StandardizedTransformer\n\n# You will see the `layers` module printed two times, it'll be explained later.\n# Note: We use enable_attention_probs=True to enable attention pattern access\nnnterp_gpt2 = StandardizedTransformer(\"gpt2\", enable_attention_probs=True)\nprint(nnterp_gpt2)\n# StandardizedTransformer also use `device_map=\"auto\"` by default:\nnnterp_gpt2.dispatch()\nprint(nnterp_gpt2.model.device)"
  },
  {
   "cell_type": "markdown",
   "id": "9243fdb9",
   "metadata": {},
   "source": [
    "Great! But I can see you at the back of the classroom, asking yourself:\n",
    "> \"Why would you create a package that just pass the right dict to the `NNsight` `rename` feature?\"\n",
    "\n",
    "And actually, I'm glad you asked! `StandardizedTransformer` and `nnterp` have a lot of other features, so bear with me!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e849e",
   "metadata": {},
   "source": [
    "## 2. Accessing Modules I/O\n",
    "With `NNsight`, the most robust way to set the residual stream after layer 1 to be the residual stream after layer 0 for a LLama-like model would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd93a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from transformers import __version__ as TRANSFORMERS_VERSION\n",
    "from packaging.version import parse\n",
    "llama = LanguageModel(\"Maykeye/TinyLLama-v0\")\n",
    "# code for transformer \"<4.54\"\n",
    "is_old_transformers = parse(TRANSFORMERS_VERSION) < parse(\"4.54\")\n",
    "if is_old_transformers:\n",
    "    with llama.trace(\"hello\"):\n",
    "        llama.model.layers[1].output = (\n",
    "            llama.model.layers[0].output[0],\n",
    "            *llama.model.layers[1].output[1:],\n",
    "        )\n",
    "else:\n",
    "    with llama.trace(\"hello\"):\n",
    "        llama.model.layers[1].output = llama.model.layers[0].output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b6e8b",
   "metadata": {},
   "source": [
    "Note that the following can cause issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b6f1153",
   "metadata": {},
   "outputs": [],
   "source": [
    "with llama.trace(\"hello\"):\n",
    "    # can't do this because .output is a tuple\n",
    "\n",
    "    # Can cause errors with gradient computation\n",
    "    if is_old_transformers:\n",
    "        # llama.model.layers[1].output[0] = llama.model.layers[0].output[0]\n",
    "        llama.model.layers[1].output[0][:] = llama.model.layers[0].output[0]\n",
    "    else:\n",
    "        llama.model.layers[1].output[:] = llama.model.layers[0].output\n",
    "\n",
    "if is_old_transformers:\n",
    "    with llama.trace(\"hello\"):\n",
    "        # Can cause errors with opt if you do this at its last layer (thanks pytest)\n",
    "        llama.model.layers[1].output = (llama.model.layers[0].output[0],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af609a",
   "metadata": {},
   "source": [
    "`nnterp` makes this much cleaner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda6cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the version of transformers does not matter, the tuple vs not tuple stuff is handled internally\n",
    "# First, you can access layer inputs and outputs directly:\n",
    "with nnterp_gpt2.trace(\"hello\"):\n",
    "    # Access layer 5's output\n",
    "    layer_5_output = nnterp_gpt2.layers_output[5]\n",
    "    # Set layer 10's output to be layer 5's output\n",
    "    nnterp_gpt2.layers_output[10] = layer_5_output\n",
    "\n",
    "# You can also access attention and MLP outputs:\n",
    "with nnterp_gpt2.trace(\"hello\"):\n",
    "    attn_output = nnterp_gpt2.attentions_output[3]\n",
    "    mlp_output = nnterp_gpt2.mlps_output[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2085327c",
   "metadata": {},
   "source": [
    "## 3. `nnterp` Guarantees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf83bee",
   "metadata": {},
   "source": [
    "When designing, `nnterp` I was very worried about silent failures, where you load a model, and then get an unexpected failure in your code downstream, or worst, it doesn't fail but give you fake results. To avoid this, when you load an `nnterp` model, a series of fast tests are run to ensure that:\n",
    "- The model has been correctly renamed\n",
    "- The model module output are of the expected shape\n",
    "- Attention probabilities have the right shape, sum to 1, and changing them changes the output\n",
    "\n",
    "This comes with the trade-off that `nnterp` will dispatch your model when you load it, which can be annoying if you don't want to load the model's weights. Also to be able to access the attention probabibilties, `nnterp` loads your model with the `eager` attention implementation, which can be slower than the default hf implementation. If you don't need the attention probabilities, you can force to use the default hf implementation / another one by passing `attn_implementation=None` or `attn_implementation=\"your_implementation\"`.\n",
    "\n",
    "What `nnterp` can NOT guarantee:\n",
    "- The attention probabilities won't be modified by the model before being multiplied by the values. To ensure this, you can check `model.attention_probabilities.print_source()` (preferably in a notebook for markdown display) to understand where the attention probabilities are computed.\n",
    "- Huggingface's transformers sheringan w\n",
    "\n",
    "If youe model is not properly renamed, you can pass a `RenameConfig` to the `nnterp` constructor to rename the model. See more in the advanced usage section of this demo.\n",
    "\n",
    "On top of that, before releasing a new version of `nnterp`, a series of tests covering most architectures are performed. When you load a model, `nnterp` will check if tests were run for your `nnsight` and `transformers` versions, and will check the tests results for the class of your model. I chose to include the tests in the `nnterp` package, so that if your model architecture has not been tested / you use a different version of `nnsight` or `transformers`, you can run `python -m nnterp run_tests --model-names foo bar --class-names LlamaForCausalLM` to run the tests for your model. `--class-names` allow you to run the tests on a toy model of the same class as your model to make it cheaper and faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe6199",
   "metadata": {},
   "source": [
    "## 4. Attention Probabilities\n",
    "\n",
    "For models that support it, you can access attention probabilities directly. You can check if a model supports it by calling `model.supports_attention_probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141813bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention probs shape will be: (batch, heads, seq_len, seq_len): torch.Size([1, 12, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "nnterp_gpt2.tokenizer.padding_side = (\n",
    "    \"left\"  # ensure left padding for easy access to the first token\n",
    ")\n",
    "\n",
    "with th.no_grad():\n",
    "    with nnterp_gpt2.trace(\"The cat sat on the mat\"):\n",
    "        # Access attention probabilities for layer 5\n",
    "        attn_probs_l2 = nnterp_gpt2.attention_probabilities[2].save()\n",
    "        attn_probs = nnterp_gpt2.attention_probabilities[5].save()\n",
    "        print(\n",
    "            f\"Attention probs shape will be: (batch, heads, seq_len, seq_len): {attn_probs.shape}\"\n",
    "        )\n",
    "        # knock out the attention to the first token\n",
    "        attn_probs[:, :, :, 0] = 0\n",
    "        attn_probs /= attn_probs.sum(dim=-1, keepdim=True)\n",
    "        corr_logits = nnterp_gpt2.logits.save()\n",
    "    with nnterp_gpt2.trace(\"The cat sat on the mat\"):\n",
    "        baseline_logits = nnterp_gpt2.logits.save()\n",
    "\n",
    "assert not th.allclose(corr_logits, baseline_logits)\n",
    "\n",
    "sums = attn_probs_l2.sum(dim=-1)\n",
    "# last dimension is the attention of token i to all other tokens, so should sum to 1\n",
    "assert th.allclose(sums, th.ones_like(sums))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98920c",
   "metadata": {},
   "source": [
    "Under the hood this uses the new tracing system implemented in `NNsight v0.5` which allow to access most model intermediate variables during the forward pass. This means that if the `transformers` implementation were to change, this could break or give unexpected results, so it is recommended to use one of the tested versions of `transformers` and to check that the attention probabilities hook makes sense by calling `model.attention_probabilities.print_source()` if you want to use a different version of `transformers` / a architecture that has not been tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83968d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Accessing attention probabilities from:\n",
       "```py\n",
       "model.transformer.h.0.attn.attention_interface_0.module_attn_dropout_0:\n",
       "\n",
       "    ....\n",
       "            attn_weights = attn_weights + causal_mask\n",
       "    \n",
       "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "    \n",
       "        # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
       "        attn_weights = attn_weights.type(value.dtype)\n",
       "    --> attn_weights = module.attn_dropout(attn_weights) <--\n",
       "    \n",
       "        # Mask heads if we want to\n",
       "        if head_mask is not None:\n",
       "            attn_weights = attn_weights * head_mask\n",
       "    \n",
       "        attn_output = torch.matmul(attn_weights, value)\n",
       "    ....\n",
       "```\n",
       "\n",
       "## Full module source:\n",
       "```py\n",
       "                             * def eager_attention_forward(module, query, key, value, attention_mask, head_mask=None, **kwargs):\n",
       " key_transpose_0         ->  0     attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
       " torch_matmul_0          ->  +     ...\n",
       "                             1 \n",
       "                             2     if module.scale_attn_weights:\n",
       " torch_full_0            ->  3         attn_weights = attn_weights / torch.full(\n",
       " value_size_0            ->  4             [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n",
       "                             5         )\n",
       "                             6 \n",
       "                             7     # Layer-wise attention scaling\n",
       "                             8     if module.scale_attn_by_inverse_layer_idx:\n",
       " float_0                 ->  9         attn_weights = attn_weights / float(module.layer_idx + 1)\n",
       "                            10 \n",
       "                            11     if not module.is_cross_attention:\n",
       "                            12         # if only \"normal\" attention layer implements causal mask\n",
       " query_size_0            -> 13         query_length, key_length = query.size(-2), key.size(-2)\n",
       " key_size_0              ->  +         ...\n",
       "                            14         causal_mask = module.bias[:, :, key_length - query_length : key_length, :key_length]\n",
       " torch_finfo_0           -> 15         mask_value = torch.finfo(attn_weights.dtype).min\n",
       "                            16         # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
       "                            17         # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
       " torch_full_1            -> 18         mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
       " attn_weights_to_0       -> 19         attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
       " torch_where_0           ->  +         ...\n",
       "                            20 \n",
       "                            21     if attention_mask is not None:\n",
       "                            22         # Apply the attention mask\n",
       "                            23         causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n",
       "                            24         attn_weights = attn_weights + causal_mask\n",
       "                            25 \n",
       " nn_functional_softmax_0 -> 26     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                            27 \n",
       "                            28     # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
       " attn_weights_type_0     -> 29     attn_weights = attn_weights.type(value.dtype)\n",
       " module_attn_dropout_0   -> 30     attn_weights = module.attn_dropout(attn_weights)\n",
       "                            31 \n",
       "                            32     # Mask heads if we want to\n",
       "                            33     if head_mask is not None:\n",
       "                            34         attn_weights = attn_weights * head_mask\n",
       "                            35 \n",
       " torch_matmul_1          -> 36     attn_output = torch.matmul(attn_weights, value)\n",
       " attn_output_transpose_0 -> 37     attn_output = attn_output.transpose(1, 2)\n",
       "                            38 \n",
       "                            39     return attn_output, attn_weights\n",
       "                            40 \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nnterp_gpt2.attention_probabilities.print_source()  # pretty markdown display in a notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f355f",
   "metadata": {},
   "source": [
    "## 5. Builtin interventions\n",
    "\n",
    "`StandardizedTransformer` also provides convenient methods for common operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8651c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "# Project hidden states to vocabulary using the unembed norm and lm_head\n",
    "with nnterp_gpt2.trace(\"The capital of France is\"):\n",
    "    hidden = nnterp_gpt2.layers_output[5]\n",
    "    logits = nnterp_gpt2.project_on_vocab(hidden)\n",
    "\n",
    "# Skip layers entirely\n",
    "with nnterp_gpt2.trace(\"Hello world\"):\n",
    "    # Skip layer 1\n",
    "    nnterp_gpt2.skip_layer(1)\n",
    "    # Skip layers 2 through 3 (inclusive)\n",
    "    nnterp_gpt2.skip_layers(2, 3)\n",
    "\n",
    "# This is useful if you want to start at a later layer than the first one\n",
    "with nnterp_gpt2.trace(\"Hello world\") as tracer:\n",
    "    layer_6_out = nnterp_gpt2.layers_output[6].save()\n",
    "    tracer.stop()  # avoid computations after layer 6\n",
    "\n",
    "with nnterp_gpt2.trace(\"Hello world\"):\n",
    "    nnterp_gpt2.skip_layers(0, 6, skip_with=layer_6_out)\n",
    "    half_half_logits = nnterp_gpt2.logits.save()\n",
    "\n",
    "with nnterp_gpt2.trace(\"Hello world\"):\n",
    "    vanilla_logits = nnterp_gpt2.logits.save()\n",
    "\n",
    "assert th.allclose(vanilla_logits, half_half_logits)  # they should be the same\n",
    "\n",
    "# Direct steering\n",
    "steering_vector = th.randn(768)  # gpt2 hidden size\n",
    "with nnterp_gpt2.trace(\"The weather today is\"):\n",
    "    nnterp_gpt2.steer(layers=[1, 3], steering_vector=steering_vector, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783b615",
   "metadata": {},
   "source": [
    "## 6. Specific Token Activation Collection\n",
    "\n",
    "`nnterp` provides utilities for collecting activations efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c33979c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched activations shape: torch.Size([3, 100, 768])\n"
     ]
    }
   ],
   "source": [
    "from nnterp.nnsight_utils import (\n",
    "    get_token_activations,\n",
    "    collect_token_activations_batched,\n",
    ")\n",
    "\n",
    "# Collect activations for specific tokens\n",
    "prompts = [\"The capital of France is\", \"The weather today is\"]\n",
    "with nnterp_gpt2.trace(prompts) as tracer:\n",
    "    # Get last token activations for all layers\n",
    "    activations = get_token_activations(nnterp_gpt2, prompts, idx=-1, tracer=tracer)\n",
    "    # activations shape: (num_layers, batch_size, hidden_size)\n",
    "\n",
    "# For large datasets, use batched collection\n",
    "large_prompts = [\"Sample text \" + str(i) for i in range(100)]\n",
    "batch_activations = collect_token_activations_batched(\n",
    "    nnterp_gpt2,\n",
    "    large_prompts,\n",
    "    batch_size=16,\n",
    "    layers=[3, 9, 11],  # Only collect specific layers, default is all layers\n",
    "    idx=-1,  # Last token (default)\n",
    ")\n",
    "print(f\"Batched activations shape: {batch_activations.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a92f9a",
   "metadata": {},
   "source": [
    "## 7. Prompt Utilities\n",
    "\n",
    "`nnterp` provides utilities for working with prompts and tracking probabilities of first tokens of certain strings. It tracks both the first token of \"string\" and \" string\".\n",
    "\n",
    "You can provide multiple string per category, the probabilities returned will be the sum of the probabilities of all the first tokens of the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "149f0c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: ['Paris', 'ĠParis']\n",
      "traps: ['London', 'ĠLondon', 'Mad', 'ĠMadrid']\n",
      "longstring: ['the', 'Ġthe']\n",
      "target: ['J', 'ĠJupiter']\n",
      "traps: ['Earth', 'ĠEarth', 'Ne', 'ĠNeptune']\n",
      "longstring: ['Pal', 'ĠPalace']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d665362e14f54c6c9cf3b44f007726fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target token probabilities:\n",
      "  target: shape torch.Size([2, 1])\n",
      "  traps: shape torch.Size([2, 1])\n",
      "  longstring: shape torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "from nnterp.prompt_utils import Prompt, run_prompts\n",
    "\n",
    "# Create prompts with target tokens to track\n",
    "prompt1 = Prompt.from_strings(\n",
    "    \"The capital of France (not England or Spain) is\",\n",
    "    {\n",
    "        \"target\": \"Paris\",\n",
    "        \"traps\": [\"London\", \"Madrid\"],\n",
    "        \"longstring\": \"the country of France\",\n",
    "    },\n",
    "    nnterp_gpt2.tokenizer,\n",
    ")\n",
    "for name, tokens in prompt1.target_tokens.items():\n",
    "    print(f\"{name}: {nnterp_gpt2.tokenizer.convert_ids_to_tokens(tokens)}\")\n",
    "\n",
    "prompt2 = Prompt.from_strings(\n",
    "    \"The largest planet (not Earth or Neptune) is\",\n",
    "    {\"target\": \"Jupiter\", \"traps\": [\"Earth\", \"Neptune\"], \"longstring\": \"Palace planet\"},\n",
    "    nnterp_gpt2.tokenizer,\n",
    ")\n",
    "for name, tokens in prompt2.target_tokens.items():\n",
    "    print(f\"{name}: {nnterp_gpt2.tokenizer.convert_ids_to_tokens(tokens)}\")\n",
    "\n",
    "# Run prompts and get probabilities for target tokens\n",
    "results = run_prompts(nnterp_gpt2, [prompt1, prompt2], batch_size=2)\n",
    "print(\"Target token probabilities:\")\n",
    "for target, probs in results.items():\n",
    "    print(f\"  {target}: shape {probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661dcb8",
   "metadata": {},
   "source": [
    "## 8. Interventions\n",
    "\n",
    "`nnterp` provides several intervention methods inspired by mechanistic interpretability research:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34179bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit lens output shape: torch.Size([2, 12, 50257])\n",
      "repeat_prompt: TargetPrompt(prompt='car:car\\n\\ncross:cross\\n\\nazdrfa:azdrfa\\n\\n*', index_to_patch=-1)\n",
      "custom_repeat_prompt: TargetPrompt(prompt='car:car\\n\\ncross:cross\\n\\nazdrfa:azdrfa\\n\\n*', index_to_patch=-1)\n",
      "patchscope_probs: torch.Size([2, 12, 50257])\n"
     ]
    }
   ],
   "source": [
    "from nnterp.interventions import (\n",
    "    logit_lens,\n",
    "    patchscope_lens,\n",
    "    TargetPrompt,\n",
    "    repeat_prompt,\n",
    "    steer,\n",
    ")\n",
    "\n",
    "# Logit Lens: See predictions at each layer\n",
    "prompts = [\"The capital of France is\", \"The sun rises in the\"]\n",
    "probs = logit_lens(nnterp_gpt2, prompts)\n",
    "print(f\"Logit lens output shape: {probs.shape}\")  # (batch, layers, vocab)\n",
    "\n",
    "# Patchscope: Replace activations from one context into another\n",
    "source_prompts = [\"Paris is beautiful\", \"London is foggy\"]\n",
    "custom_target_prompt = TargetPrompt(\"city: Paris\\nfood: croissant\\n?\", -1)\n",
    "target_prompt = repeat_prompt()  # Creates a repetition task\n",
    "custom_repeat_prompt = repeat_prompt(\n",
    "    words=[\"car\", \"cross\", \"azdrfa\"],\n",
    "    rel=\":\",\n",
    "    sep=\"\\n\\n\",\n",
    "    placeholder=\"*\",\n",
    ")\n",
    "print(f\"repeat_prompt: {custom_repeat_prompt}\")\n",
    "print(f\"custom_repeat_prompt: {custom_repeat_prompt}\")\n",
    "patchscope_probs = patchscope_lens(\n",
    "    nnterp_gpt2, source_prompts=source_prompts, target_patch_prompts=target_prompt\n",
    ")\n",
    "print(f\"patchscope_probs: {patchscope_probs.shape}\")\n",
    "\n",
    "# Steering with intervention function\n",
    "with nnterp_gpt2.trace(\"The weather is\"):\n",
    "    steer(nnterp_gpt2, layers=[5, 10], steering_vector=steering_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f50694e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "You can use a combination of run_prompts and interventions to get the probabilities of certain tokens according to your custom intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "074c3253",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7898cf6e59149818ed1f0621bf41d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: torch.Size([2, 12])\n",
      "english: torch.Size([2, 12])\n",
      "format: torch.Size([2, 12])\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "target",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11
         ],
         "y": [
          0.00007567817374365404,
          0.000027838676032843068,
          0.00005731204873882234,
          0.00003011445005540736,
          0.0000073779065132839605,
          0.0000012276200322958175,
          4.4267795829000534e-7,
          2.6624027782418125e-7,
          9.068317012861371e-7,
          0.000009268691428587772,
          0.000039329381252173334,
          0.00032726803328841925
         ]
        },
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "english",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11
         ],
         "y": [
          0.00001638935282244347,
          0.000003013349669345189,
          0.000004992575668438803,
          0.0000025822341740422416,
          0.0000018991563592862803,
          7.224385853987769e-7,
          6.758828021702357e-7,
          7.550120244559366e-7,
          7.243799018397112e-7,
          0.000004083875865035225,
          0.000022598127543460578,
          0.00019179985974915326
         ]
        },
        {
         "line": {
          "width": 2
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "name": "format",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11
         ],
         "y": [
          0.0002455353387631476,
          0.00004637414531316608,
          0.000029440583602990955,
          0.000022731657736585476,
          0.000020229383153491654,
          0.000015257579434546642,
          0.000006756861239409773,
          0.000011153537343489006,
          0.0005351840518414974,
          0.043851301074028015,
          0.07962892949581146,
          0.06841540336608887
         ]
        }
       ],
       "layout": {
        "hovermode": "x unified",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Mean Token Probabilities Across Layers"
        },
        "xaxis": {
         "title": {
          "text": "Layer"
         }
        },
        "yaxis": {
         "title": {
          "text": "Mean Probability"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_model = nnterp_gpt2\n",
    "# uncomment if you have a GPU for cooler results\n",
    "# demo_model = StandardizedTransformer(\"google/gemma-2-2b\")\n",
    "\n",
    "prompts_str = [\n",
    "    \"The translation of 'car' in French is\",\n",
    "    \"The translation of 'cat' in Spanish is\",\n",
    "]\n",
    "tokens = [\n",
    "    {\"target\": [\"voiture\", \"bagnole\"], \"english\": \"car\", \"format\": \"'\"},\n",
    "    {\"target\": [\"gato\", \"minino\"], \"english\": \"cat\", \"format\": \"'\"},\n",
    "]\n",
    "prompts = [\n",
    "    Prompt.from_strings(prompt, tokens, demo_model.tokenizer)\n",
    "    for prompt, tokens in zip(prompts_str, tokens)\n",
    "]\n",
    "results = run_prompts(demo_model, prompts, batch_size=2, get_probs_func=logit_lens)\n",
    "for category, probs in results.items():\n",
    "    print(f\"{category}: {probs.shape}\")  # (batch, layers)\n",
    "\n",
    "# Create a plotly plot showing mean probabilities for each category across layers\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calculate mean probabilities across batches for each category and layer\n",
    "mean_probs = {category: probs.mean(dim=0) for category, probs in results.items()}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add a line for each category\n",
    "for category, probs in mean_probs.items():\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(len(probs))),\n",
    "            y=probs.tolist(),\n",
    "            mode=\"lines+markers\",\n",
    "            name=category,\n",
    "            line=dict(width=2),\n",
    "            marker=dict(size=6),\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Mean Token Probabilities Across Layers\",\n",
    "    xaxis_title=\"Layer\",\n",
    "    yaxis_title=\"Mean Probability\",\n",
    "    hovermode=\"x unified\",\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec6bce",
   "metadata": {},
   "source": [
    "## 9. Visualization\n",
    "\n",
    "Finally, `nnterp` provides visualization utilities for analyzing model probabilities and prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09369de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorbar": {
          "len": 0.9,
          "thickness": 15,
          "title": {
           "text": "Probability"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(5,48,97)"
          ],
          [
           0.1,
           "rgb(33,102,172)"
          ],
          [
           0.2,
           "rgb(67,147,195)"
          ],
          [
           0.3,
           "rgb(146,197,222)"
          ],
          [
           0.4,
           "rgb(209,229,240)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(253,219,199)"
          ],
          [
           0.7,
           "rgb(244,165,130)"
          ],
          [
           0.8,
           "rgb(214,96,77)"
          ],
          [
           0.9,
           "rgb(178,24,43)"
          ],
          [
           1,
           "rgb(103,0,31)"
          ]
         ],
         "hovertemplate": "Layer: %{y}<br>%{hovertext}<br>Probability: %{z}<extra></extra>",
         "hovertext": [
          [
           "ID: 407<br>Token: 'Ġnot'",
           "ID: 783<br>Token: 'Ġnow'",
           "ID: 635<br>Token: 'Ġalso'",
           "ID: 991<br>Token: 'Ġstill'",
           "ID: 257<br>Token: 'Ġa'"
          ],
          [
           "ID: 407<br>Token: 'Ġnot'",
           "ID: 783<br>Token: 'Ġnow'",
           "ID: 635<br>Token: 'Ġalso'",
           "ID: 991<br>Token: 'Ġstill'",
           "ID: 257<br>Token: 'Ġa'"
          ],
          [
           "ID: 407<br>Token: 'Ġnot'",
           "ID: 783<br>Token: 'Ġnow'",
           "ID: 635<br>Token: 'Ġalso'",
           "ID: 991<br>Token: 'Ġstill'",
           "ID: 257<br>Token: 'Ġa'"
          ],
          [
           "ID: 635<br>Token: 'Ġalso'",
           "ID: 407<br>Token: 'Ġnot'",
           "ID: 783<br>Token: 'Ġnow'",
           "ID: 991<br>Token: 'Ġstill'",
           "ID: 257<br>Token: 'Ġa'"
          ],
          [
           "ID: 635<br>Token: 'Ġalso'",
           "ID: 783<br>Token: 'Ġnow'",
           "ID: 407<br>Token: 'Ġnot'",
           "ID: 991<br>Token: 'Ġstill'",
           "ID: 5600<br>Token: 'Ġindeed'"
          ],
          [
           "ID: 635<br>Token: 'Ġalso'",
           "ID: 407<br>Token: 'Ġnot'",
           "ID: 783<br>Token: 'Ġnow'",
           "ID: 2192<br>Token: 'Ġprobably'",
           "ID: 1690<br>Token: 'Ġoften'"
          ],
          [
           "ID: 635<br>Token: 'Ġalso'",
           "ID: 407<br>Token: 'Ġnot'",
           "ID: 783<br>Token: 'Ġnow'",
           "ID: 1690<br>Token: 'Ġoften'",
           "ID: 2192<br>Token: 'Ġprobably'"
          ],
          [
           "ID: 2192<br>Token: 'Ġprobably'",
           "ID: 635<br>Token: 'Ġalso'",
           "ID: 3221<br>Token: 'Ġusually'",
           "ID: 783<br>Token: 'Ġnow'",
           "ID: 407<br>Token: 'Ġnot'"
          ],
          [
           "ID: 2192<br>Token: 'Ġprobably'",
           "ID: 635<br>Token: 'Ġalso'",
           "ID: 783<br>Token: 'Ġnow'",
           "ID: 991<br>Token: 'Ġstill'",
           "ID: 2407<br>Token: 'Ġquite'"
          ],
          [
           "ID: 11491<br>Token: 'Ġincorrect'",
           "ID: 25<br>Token: ':'",
           "ID: 2407<br>Token: 'Ġquite'",
           "ID: 407<br>Token: 'Ġnot'",
           "ID: 705<br>Token: 'Ġ''"
          ],
          [
           "ID: 25<br>Token: ':'",
           "ID: 705<br>Token: 'Ġ''",
           "ID: 407<br>Token: 'Ġnot'",
           "ID: 2407<br>Token: 'Ġquite'",
           "ID: 635<br>Token: 'Ġalso'"
          ],
          [
           "ID: 25<br>Token: ':'",
           "ID: 705<br>Token: 'Ġ''",
           "ID: 422<br>Token: 'Ġfrom'",
           "ID: 257<br>Token: 'Ġa'",
           "ID: 366<br>Token: 'Ġ\"'"
          ]
         ],
         "text": [
          [
           "'Ġnot'",
           "'Ġnow'",
           "'Ġalso'",
           "'Ġstill'",
           "'Ġa'"
          ],
          [
           "'Ġnot'",
           "'Ġnow'",
           "'Ġalso'",
           "'Ġstill'",
           "'Ġa'"
          ],
          [
           "'Ġnot'",
           "'Ġnow'",
           "'Ġalso'",
           "'Ġstill'",
           "'Ġa'"
          ],
          [
           "'Ġalso'",
           "'Ġnot'",
           "'Ġnow'",
           "'Ġstill'",
           "'Ġa'"
          ],
          [
           "'Ġalso'",
           "'Ġnow'",
           "'Ġnot'",
           "'Ġstill'",
           "'Ġindeed'"
          ],
          [
           "'Ġalso'",
           "'Ġnot'",
           "'Ġnow'",
           "'Ġprobably'",
           "'Ġoften'"
          ],
          [
           "'Ġalso'",
           "'Ġnot'",
           "'Ġnow'",
           "'Ġoften'",
           "'Ġprobably'"
          ],
          [
           "'Ġprobably'",
           "'Ġalso'",
           "'Ġusually'",
           "'Ġnow'",
           "'Ġnot'"
          ],
          [
           "'Ġprobably'",
           "'Ġalso'",
           "'Ġnow'",
           "'Ġstill'",
           "'Ġquite'"
          ],
          [
           "'Ġincorrect'",
           "':'",
           "'Ġquite'",
           "'Ġnot'",
           "'Ġ''"
          ],
          [
           "':'",
           "'Ġ''",
           "'Ġnot'",
           "'Ġquite'",
           "'Ġalso'"
          ],
          [
           "':'",
           "'Ġ''",
           "'Ġfrom'",
           "'Ġa'",
           "'Ġ\"'"
          ]
         ],
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          0,
          1,
          2,
          3,
          4
         ],
         "xaxis": "x",
         "y": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11
         ],
         "yaxis": "y",
         "z": [
          [
           0.25856339931488037,
           0.12897254526615143,
           0.11089171469211578,
           0.07452113926410675,
           0.043352849781513214
          ],
          [
           0.24852286279201508,
           0.234116330742836,
           0.18417169153690338,
           0.07402073591947556,
           0.01820913888514042
          ],
          [
           0.29373404383659363,
           0.26820534467697144,
           0.10354108363389969,
           0.09390817582607269,
           0.024757681414484978
          ],
          [
           0.28834161162376404,
           0.2511930465698242,
           0.14035063982009888,
           0.05732386186718941,
           0.035104524344205856
          ],
          [
           0.259952574968338,
           0.2284768968820572,
           0.17014314234256744,
           0.06353042274713516,
           0.021078424528241158
          ],
          [
           0.23922376334667206,
           0.2280544638633728,
           0.20444855093955994,
           0.04291168972849846,
           0.034565605223178864
          ],
          [
           0.23423783481121063,
           0.23194600641727448,
           0.09808699041604996,
           0.05494494363665581,
           0.04802682250738144
          ],
          [
           0.15860912203788757,
           0.14885461330413818,
           0.12734079360961914,
           0.11501012742519379,
           0.0489649660885334
          ],
          [
           0.18543823063373566,
           0.09870872646570206,
           0.09851199388504028,
           0.06065351888537407,
           0.0567467175424099
          ],
          [
           0.1293708086013794,
           0.08392251282930374,
           0.07322198152542114,
           0.04554023966193199,
           0.04193507134914398
          ],
          [
           0.10999398678541183,
           0.07483618706464767,
           0.06355626881122589,
           0.047720711678266525,
           0.04182903468608856
          ],
          [
           0.07896627485752106,
           0.0671667605638504,
           0.056609299033880234,
           0.044396888464689255,
           0.030609337612986565
          ]
         ],
         "zmax": 1,
         "zmin": 0
        }
       ],
       "layout": {
        "height": 1000,
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Top 5 tokens at each layer for 'The translation of 'car' in French is"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Tokens"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Layers"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompts DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prompt</th>\n",
       "      <td>The translation of 'car' in French is</td>\n",
       "      <td>The translation of 'cat' in Spanish is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_string</th>\n",
       "      <td>[voiture, bagnole]</td>\n",
       "      <td>[gato, minino]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_string</th>\n",
       "      <td>car</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format_string</th>\n",
       "      <td>'</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_tokens</th>\n",
       "      <td>[vo, Ġvo, b, Ġb]</td>\n",
       "      <td>[g, Ġg, min, Ġmin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_tokens</th>\n",
       "      <td>[car, Ġcar]</td>\n",
       "      <td>[cat, Ġcat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format_tokens</th>\n",
       "      <td>[', Ġ']</td>\n",
       "      <td>[', Ġ']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  \\\n",
       "prompt          The translation of 'car' in French is   \n",
       "target_string                      [voiture, bagnole]   \n",
       "english_string                                    car   \n",
       "format_string                                       '   \n",
       "target_tokens                        [vo, Ġvo, b, Ġb]   \n",
       "english_tokens                            [car, Ġcar]   \n",
       "format_tokens                                 [', Ġ']   \n",
       "\n",
       "                                                     1  \n",
       "prompt          The translation of 'cat' in Spanish is  \n",
       "target_string                           [gato, minino]  \n",
       "english_string                                     cat  \n",
       "format_string                                        '  \n",
       "target_tokens                       [g, Ġg, min, Ġmin]  \n",
       "english_tokens                             [cat, Ġcat]  \n",
       "format_tokens                                  [', Ġ']  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nnterp.display import plot_topk_tokens, prompts_to_df\n",
    "\n",
    "probs = logit_lens(demo_model, prompts_str[0])\n",
    "# Visualize top tokens from logit lens\n",
    "plot_topk_tokens(\n",
    "    probs[0],\n",
    "    demo_model.tokenizer,\n",
    "    k=5,\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    title=\"Top 5 tokens at each layer for 'The translation of 'car' in French is\",\n",
    ")\n",
    "\n",
    "# Convert prompts to DataFrame for analysis\n",
    "df = prompts_to_df(prompts, demo_model.tokenizer)\n",
    "print(\"\\nPrompts DataFrame:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337c0673",
   "metadata": {},
   "source": [
    "# Advanced usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6c6a9",
   "metadata": {},
   "source": [
    "Sometime, your model might not be supported yet by nnterp. In this case, you'll be able to use a `RenameConfig` to properly initialize your model.\n",
    "\n",
    "In this section, I'll show you the steps I took to add support for the `gpt2` to `nnterp`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1deded2",
   "metadata": {},
   "source": [
    "###  Renaming a module not automatically renamed\n",
    "\n",
    "Let's say that you load a `gpt2` model that is a bit special: every module is called \"super_module\" instead of \"module\".\n",
    "\n",
    "First, let's build such a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45d8af2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (super_transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (super_h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (super_mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (super_attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "for layer in model.transformer.h:\n",
    "    layer.super_mlp = layer.mlp\n",
    "    delattr(layer, \"mlp\")\n",
    "    layer.super_attn = layer.attn\n",
    "    delattr(layer, \"attn\")\n",
    "model.transformer.super_h = model.transformer.h\n",
    "delattr(model.transformer, \"h\")\n",
    "# Let's keep the final layer norm as is\n",
    "# model.transformer.super_ln_f = model.transformer.ln_f\n",
    "# delattr(model.transformer, \"ln_f\")\n",
    "model.super_transformer = model.transformer\n",
    "delattr(model, \"transformer\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a9399b",
   "metadata": {},
   "source": [
    "now if we try to use nnterp, the renaming check automatically performed will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e618552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 06:58:37.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_491885/1758275130.py\", line 5, in <module>\n",
      "    StandardizedTransformer(model)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/nnterp/standardized_transformer.py\", line 128, in __init__\n",
      "    self.num_layers = len(self.layers)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 1048, in __getattr__\n",
      "    raise AttributeError(f\"{self} has no attribute {name}\")\n",
      "AttributeError: GPT2LMHeadModel(\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (super_transformer): GPT2Model(\n",
      "    (embed_tokens/wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (ln_final/ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (super_h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (super_mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (super_attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ") has no attribute layers\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "from traceback import print_exc\n",
    "\n",
    "try:\n",
    "    StandardizedTransformer(model)\n",
    "except Exception as e:\n",
    "    print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718b85b",
   "metadata": {},
   "source": [
    "`nnterp` can't find the layers because they're located under `super_transformer`, that nnterp doesn't know about. We have 2 choices in this case:\n",
    "1. Rename `super_transformer` to `model` and `super_h` to `layers` such that it matches the `model.model.layers` Llama architecture and let `nnterp` do the rest.\n",
    "2. Rename `super_transformer.super_h` directly to `layers`, matching the StandardizedTransformer architecture.\n",
    "\n",
    "Let's try the second option first. And let's not forget that we still need to rename\n",
    "\n",
    "In order to do that we can instantiate a `StandardizedTransformer` with a `RenameConfig` with the correct aliases provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f57a33bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 06:58:37.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_491885/4292719532.py\", line 9, in <module>\n",
      "    StandardizedTransformer(model, rename_config=rename_cfg)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/nnterp/standardized_transformer.py\", line 140, in __init__\n",
      "    check_model_renaming(self, model_name, ignores, allow_dispatch)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/nnterp/rename_utils.py\", line 680, in check_model_renaming\n",
      "    raise RenamingError(\n",
      "nnterp.rename_utils.RenamingError: Could not find ln_final module in GPT2LMHeadModel architecture. This means that it was not properly renamed.\n",
      "Please pass the name of the ln_final module to the ln_final_rename argument.\n"
     ]
    }
   ],
   "source": [
    "from nnterp.rename_utils import RenameConfig\n",
    "\n",
    "rename_cfg = RenameConfig(\n",
    "    layers_name=\"super_transformer.super_h\",\n",
    "    attn_name=\"super_attn\",\n",
    "    mlp_name=\"super_mlp\",\n",
    ")\n",
    "try:\n",
    "    StandardizedTransformer(model, rename_config=rename_cfg)\n",
    "except Exception as e:\n",
    "    print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71b0a52",
   "metadata": {},
   "source": [
    "We're still getting an error because `nnterp` doesn't find the `ln_f`. This is because `nnterp` will automatically rename the `ln_f` to `ln_final`, but fails to rename `model.ln_final` to `ln_final`. Again, we can either rename `super_transformer` to `model` or directly rename `super_transformer.ln_f` to `ln_final`.\n",
    "\n",
    "⚠️ The code will still fail, because our \"super_gpt2\" model can't run its forward pass as we deleted its modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64b86b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 06:58:38.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
      "\u001b[32m2025-10-18 06:58:38.397\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnnterp.utils\u001b[0m:\u001b[36mtry_with_scan\u001b[0m:\u001b[36m141\u001b[0m - \u001b[33m\u001b[1mError when trying to scan the model - using .trace() instead (which will dispatch the model)...\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/nnterp/utils.py\", line 145, in try_with_scan\n",
      "    with model.trace(dummy_inputs()) as tracer:\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py\", line 601, in __exit__\n",
      "    self.backend(self)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 24, in __call__\n",
      "    raise wrap_exception(e, tracer.info) from None\n",
      "nnsight.NNsightException: \n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 21, in __call__\n",
      "    tracer.execute(fn)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 385, in execute\n",
      "    self.model.interleave(self.fn, *args, **kwargs)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py\", line 76, in interleave\n",
      "    return super().interleave(fn, *args, **kwargs)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 733, in interleave\n",
      "    fn(*args, **kwargs)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 384, in __call__\n",
      "    else self._module(*args, **kwargs)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1879, in _call_impl\n",
      "    return inner()\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1827, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 137, in nnsight_forward\n",
      "    return forward(*args, **kwargs)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1070, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1962, in __getattr__\n",
      "    raise AttributeError(\n",
      "\n",
      "AttributeError: 'GPT2LMHeadModel' object has no attribute 'transformer'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_491885/3048501515.py\", line 11, in <module>\n",
      "    StandardizedTransformer(\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/nnterp/standardized_transformer.py\", line 140, in __init__\n",
      "    check_model_renaming(self, model_name, ignores, allow_dispatch)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/nnterp/rename_utils.py\", line 702, in check_model_renaming\n",
      "    try_with_scan(\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/nnterp/utils.py\", line 151, in try_with_scan\n",
      "    raise error_to_throw from e2\n",
      "nnterp.rename_utils.RenamingError: Could not check the IO of GPT2LMHeadModel\n"
     ]
    }
   ],
   "source": [
    "rename_cfg = RenameConfig(\n",
    "    model_name=\"super_transformer\",\n",
    "    layers_name=\"super_h\",\n",
    "    attn_name=\"super_attn\",\n",
    "    mlp_name=\"super_mlp\",\n",
    "    ln_final_name=\"super_transformer.ln_f\",\n",
    ")\n",
    "from transformers import AutoConfig\n",
    "\n",
    "try:\n",
    "    StandardizedTransformer(\n",
    "        model, rename_config=rename_cfg, config=AutoConfig.from_pretrained(\"gpt2\")\n",
    "    )\n",
    "except Exception as e:\n",
    "    print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f99d41",
   "metadata": {},
   "source": [
    "## Adding attention probabilities support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d4d81",
   "metadata": {},
   "source": [
    "To access the attention probabilities, `nnterp` uses the `NNsight` ability to hook on most intermediate variables of the forward pass. This is very architecture dependent, as even 2 equivalent models, if they use different names for the intermediate variables, will need different hooks.\n",
    "\n",
    "As I'm writing this tutorial, I'm adding support for attention probabilities for `GPTJ` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5bf6f4",
   "metadata": {},
   "outputs": [],
   "source": "from nnterp import StandardizedTransformer\n\ngptj = StandardizedTransformer(\n    \"yujiepan/gptj-tiny-random\", enable_attention_probs=True\n)  # In the current version of nnterp, this will work out of the box"
  },
  {
   "cell_type": "markdown",
   "id": "88e1b5d5",
   "metadata": {},
   "source": [
    "As you can see, when you load a model,`nnterp` will automatically test if the attention probabilities hook is working and returns a tensor of shape `(batch_size, num_heads, seq_len, seq_len)` where the last dimension sums to 1. In this case, the test failed and `nnterp` logs the error.\n",
    "\n",
    "Now let's look at the `yujiepan/gptj-tiny-random` forward pass and try to understand where are the attention probabilities computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc7148a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```py\n",
       "                                 * def forward(\n",
       "                                 0     self,\n",
       "                                 1     hidden_states: torch.FloatTensor,\n",
       "                                 2     layer_past: Optional[Cache] = None,\n",
       "                                 3     attention_mask: Optional[torch.FloatTensor] = None,\n",
       "                                 4     position_ids: Optional[torch.LongTensor] = None,\n",
       "                                 5     head_mask: Optional[torch.FloatTensor] = None,\n",
       "                                 6     use_cache: Optional[bool] = False,\n",
       "                                 7     output_attentions: Optional[bool] = False,\n",
       "                                 8     cache_position: Optional[torch.LongTensor] = None,\n",
       "                                 9 ) -> Union[\n",
       "                                10     tuple[torch.Tensor, tuple[torch.Tensor]],\n",
       "                                11     Optional[tuple[torch.Tensor, tuple[torch.Tensor], tuple[torch.Tensor, ...]]],\n",
       "                                12 ]:\n",
       " self_q_proj_0               -> 13     query = self.q_proj(hidden_states)\n",
       " self_k_proj_0               -> 14     key = self.k_proj(hidden_states)\n",
       " self_v_proj_0               -> 15     value = self.v_proj(hidden_states)\n",
       "                                16 \n",
       " self__split_heads_0         -> 17     query = self._split_heads(query, self.num_attention_heads, self.head_dim, True)\n",
       " self__split_heads_1         -> 18     key = self._split_heads(key, self.num_attention_heads, self.head_dim, True)\n",
       " self__split_heads_2         -> 19     value = self._split_heads(value, self.num_attention_heads, self.head_dim, False)\n",
       "                                20 \n",
       " is_torch_fx_proxy_0         -> 21     if is_torch_fx_proxy(position_ids) or torch.jit.is_tracing():\n",
       " torch_jit_is_tracing_0      ->  +     ...\n",
       "                                22         # The logic to conditionally copy to GPU could not be traced, so we do this\n",
       "                                23         # every time in the torch.fx case\n",
       " get_embed_positions_0       -> 24         embed_positions = get_embed_positions(self.embed_positions, position_ids)\n",
       "                                25     else:\n",
       " self__get_embed_positions_0 -> 26         embed_positions = self._get_embed_positions(position_ids)\n",
       "                                27 \n",
       " position_ids_unsqueeze_0    -> 28     repeated_position_ids = position_ids.unsqueeze(-1).repeat(1, 1, embed_positions.shape[-1])\n",
       " repeat_0                    ->  +     ...\n",
       " torch_gather_0              -> 29     sincos = torch.gather(embed_positions, 1, repeated_position_ids).to(key.dtype)\n",
       " to_0                        ->  +     ...\n",
       " torch_split_0               -> 30     sin, cos = torch.split(sincos, sincos.shape[-1] // 2, dim=-1)\n",
       "                                31 \n",
       "                                32     if self.rotary_dim is not None:\n",
       "                                33         k_rot = key[:, :, :, : self.rotary_dim]\n",
       "                                34         k_pass = key[:, :, :, self.rotary_dim :]\n",
       "                                35 \n",
       "                                36         q_rot = query[:, :, :, : self.rotary_dim]\n",
       "                                37         q_pass = query[:, :, :, self.rotary_dim :]\n",
       "                                38 \n",
       " apply_rotary_pos_emb_0      -> 39         k_rot = apply_rotary_pos_emb(k_rot, sin, cos)\n",
       " apply_rotary_pos_emb_1      -> 40         q_rot = apply_rotary_pos_emb(q_rot, sin, cos)\n",
       "                                41 \n",
       " torch_cat_0                 -> 42         key = torch.cat([k_rot, k_pass], dim=-1)\n",
       " torch_cat_1                 -> 43         query = torch.cat([q_rot, q_pass], dim=-1)\n",
       "                                44     else:\n",
       " apply_rotary_pos_emb_2      -> 45         key = apply_rotary_pos_emb(key, sin, cos)\n",
       " apply_rotary_pos_emb_3      -> 46         query = apply_rotary_pos_emb(query, sin, cos)\n",
       "                                47 \n",
       " key_permute_0               -> 48     key = key.permute(0, 2, 1, 3)\n",
       " query_permute_0             -> 49     query = query.permute(0, 2, 1, 3)\n",
       "                                50 \n",
       "                                51     if layer_past is not None:\n",
       "                                52         cache_kwargs = {\n",
       "                                53             \"sin\": sin,\n",
       "                                54             \"cos\": cos,\n",
       "                                55             \"partial_rotation_size\": self.rotary_dim,\n",
       "                                56             \"cache_position\": cache_position,\n",
       "                                57         }\n",
       " layer_past_update_0         -> 58         key, value = layer_past.update(key, value, self.layer_idx, cache_kwargs)\n",
       "                                59 \n",
       "                                60     # compute self-attention: V x Softmax(QK^T)\n",
       " self__attn_0                -> 61     attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
       "                                62 \n",
       " self__merge_heads_0         -> 63     attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_dim)\n",
       " self_out_proj_0             -> 64     attn_output = self.out_proj(attn_output)\n",
       " self_resid_dropout_0        -> 65     attn_output = self.resid_dropout(attn_output)\n",
       "                                66 \n",
       "                                67     return attn_output, attn_weights\n",
       "                                68 \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nnterp.utils import display_source\n",
    "\n",
    "display_source(gptj.attentions[0].source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3755d0",
   "metadata": {},
   "source": [
    "Lines 60-61:\n",
    "```py\n",
    "                                60     # compute self-attention: V x Softmax(QK^T)\n",
    " self__attn_0                -> 61     attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
    " ```\n",
    "⚠️ Be careful! if you set the hook here, you'll be able to successfully access the attention probabilities, but not to edit them! ⚠️\n",
    "\n",
    "We need to check the source of `self__attn_0` to see where `attn_weights` is used. In order to access a deeper variable like this, we have to actually run the model with `trace` or `scan`. I'd advise to start with `scan` first, but switch to `trace` if you encounter an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01b91759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```py\n",
       "                             * def _attn(\n",
       "                             0     self,\n",
       "                             1     query,\n",
       "                             2     key,\n",
       "                             3     value,\n",
       "                             4     attention_mask=None,\n",
       "                             5     head_mask=None,\n",
       "                             6 ):\n",
       "                             7     # Keep the attention weights computation in fp32 to avoid overflow issues\n",
       " query_to_0              ->  8     query = query.to(torch.float32)\n",
       " key_to_0                ->  9     key = key.to(torch.float32)\n",
       "                            10 \n",
       " key_transpose_0         -> 11     attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
       " torch_matmul_0          ->  +     ...\n",
       "                            12     attn_weights = attn_weights / self.scale_attn\n",
       "                            13 \n",
       "                            14     if attention_mask is not None:  # no matter the length, we just slice it\n",
       "                            15         causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n",
       "                            16         attn_weights = attn_weights + causal_mask\n",
       "                            17 \n",
       " nn_functional_softmax_0 -> 18     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       " attn_weights_to_0       -> 19     attn_weights = attn_weights.to(value.dtype)\n",
       " self_attn_dropout_0     -> 20     attn_weights = self.attn_dropout(attn_weights)\n",
       "                            21 \n",
       "                            22     # Mask heads if we want to\n",
       "                            23     if head_mask is not None:\n",
       "                            24         attn_weights = attn_weights * head_mask\n",
       "                            25 \n",
       " torch_matmul_1          -> 26     attn_output = torch.matmul(attn_weights, value)\n",
       "                            27 \n",
       "                            28     return attn_output, attn_weights\n",
       "                            29 \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with gptj.scan(\"a\"):\n",
    "    display_source(gptj.attentions[0].source.self__attn_0.source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662d5c4",
   "metadata": {},
   "source": [
    "Here, line 20-24:\n",
    "```py\n",
    " self_attn_dropout_0     -> 20     attn_weights = self.attn_dropout(attn_weights)\n",
    "                            21\n",
    "                            22     # Mask heads if we want to\n",
    "                            23     if head_mask is not None:\n",
    "                            24         attn_weights = attn_weights * head_mask\n",
    "```\n",
    "\n",
    "In the current `NNsight` version, the results of operators like `*` are not hooked. But even if they were, I'd be careful to use line 24 here, as it's inside a `if` statement. Therefore, we'll use `self_attn_dropout_0` instead.\n",
    "\n",
    "Note that we could also look at `torch_matmul_1` input and edit the value here. However, this looks less robust to me as it assumes this is the only place where `attn_weights` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "402b7798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "with gptj.scan(th.tensor([[1, 2, 3]])):\n",
    "    print(\n",
    "        gptj.attentions[0].source.self__attn_0.source.self_attn_dropout_0.output.shape\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f580ca",
   "metadata": {},
   "source": [
    "Nice! The shape looks good. Now we can initialize our model with the right RenameConfig, and let `nnterp` run the tests for us.\n",
    "\n",
    "To do this, we'll need to create a `AttnProbFunction` and implement the `get_attention_prob_source` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ffe15",
   "metadata": {},
   "outputs": [],
   "source": "from nnterp.rename_utils import AttnProbFunction, RenameConfig\n\n\nclass GPTJAttnProbFunction(AttnProbFunction):\n\n    def get_attention_prob_source(\n        self, attention_module, return_module_source: bool = False\n    ):\n        if return_module_source:\n            # in this case, return source of the module from where the attention probabilities are computed\n            return attention_module.source.self__attn_0.source\n        else:\n            # in this case, return the attention probabilities hook\n            return attention_module.source.self__attn_0.source.self_attn_dropout_0\n\n\ngptj = StandardizedTransformer(\n    \"yujiepan/gptj-tiny-random\",\n    enable_attention_probs=True,\n    rename_config=RenameConfig(attn_prob_source=GPTJAttnProbFunction()),\n)\n\nwith gptj.trace(\"Hello world!\"):\n    batch_size, seq_len = gptj.input_size\n    attn_probs = gptj.attention_probabilities[0].save()\n    print(f\"attn_probs.shape: {attn_probs.shape}\")\n    assert attn_probs.shape == (batch_size, gptj.num_heads, seq_len, seq_len)\n    gptj.attention_probabilities[0] = attn_probs / 2\n    corrupt_logits = gptj.logits.save()\n\nwith gptj.trace(\"Hello world!\"):\n    clean_logits = gptj.logits.save()\n\nassert gptj.attention_probabilities.enabled\nassert not th.allclose(clean_logits, corrupt_logits)\nsummed_attn_probs = attn_probs.sum(dim=-1)\nassert th.allclose(summed_attn_probs, th.ones_like(summed_attn_probs))"
  },
  {
   "cell_type": "markdown",
   "id": "874995da",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "`nnterp` provides a unified, standardized interface for working with transformer models, built on top of `nnsight`. Key features include:\n",
    "\n",
    "1. **Standardized naming** across all transformer architectures\n",
    "2. **Easy access** to layer/attention/MLP inputs and outputs\n",
    "3. **Built-in methods** for common operations (steering, skipping layers, projecting to vocab)\n",
    "4. **Efficient activation collection** with batching support\n",
    "5. **Prompt utilities** for tracking target tokens\n",
    "6. **Intervention methods** from mechanistic interpretability research\n",
    "7. **Visualization tools** for analyzing model behavior\n",
    "\n",
    "All of this while maintaining the full power and flexibility of `nnsight` under the hood!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc4bcc7",
   "metadata": {},
   "source": [
    "# Appendix: `NNsight` cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6300a0e7",
   "metadata": {},
   "source": [
    "## 1) You must execute your interventions in order\n",
    "In the new `NNsight` versions, it is enforced that you must access to model internals *in the same order* as the model execute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cec3d994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 06:58:45.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
      "/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:829: UserWarning:\n",
      "\n",
      "Can't initialize NVML\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_491885/2722117291.py\", line 6, in <module>\n",
      "    with nnterp_gpt2.trace(\"My tailor is rich\"):\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py\", line 601, in __exit__\n",
      "    self.backend(self)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 24, in __call__\n",
      "    raise wrap_exception(e, tracer.info) from None\n",
      "nnsight.NNsightException: \n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_491885/2722117291.py\", line 12, in <module>\n",
      "    l1 = nnterp_gpt2.layers_output[1]  # will fail! You need to collect l1 before l2\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/nnterp/rename_utils.py\", line 332, in __getitem__\n",
      "    target = module.output\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 152, in output\n",
      "    return self._interleaver.current.request(\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 797, in request\n",
      "    value = self.send(Events.VALUE, requester)\n",
      "  File \"/mnt/nw/home/c.dumas/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 782, in send\n",
      "    raise response\n",
      "\n",
      "OutOfOrderError: Value was missed for model.transformer.h.1.output.i0. Did you call an Envoy out of order?\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "from traceback import print_exc\n",
    "\n",
    "nnterp_gpt2 = StandardizedTransformer(\"gpt2\")\n",
    "try:\n",
    "    with nnterp_gpt2.trace(\"My tailor is rich\"):\n",
    "        l2 = nnterp_gpt2.layers_output[2]\n",
    "        l1 = nnterp_gpt2.layers_output[1]  # will fail! You need to collect l1 before l2\n",
    "except Exception as e:\n",
    "    print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255426b",
   "metadata": {},
   "source": [
    "## 2) Gradient computation\n",
    "To compute gradients, you need to open a `.backward()` context, and save the gradients *inside it*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bac27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nnterp_gpt2.trace(\"My tailor is rich\"):\n",
    "    l1_out = nnterp_gpt2.layers_output[1]  # get l1 before accessing logits\n",
    "    logits = nnterp_gpt2.output.logits\n",
    "    with logits.sum().backward(\n",
    "        retain_graph=True\n",
    "    ):  # use retain_graph if you want to do multiple backprops\n",
    "        if False:\n",
    "            l1_grad = nnterp_gpt2.layers_output[1].grad.save()\n",
    "            # this would fail as we'd access nnterp_gpt2.layers_output[1] after nnterp_gpt2.output\n",
    "        l1_grad = l1_out.grad.save()\n",
    "    with (logits.sum() ** 2).backward():\n",
    "        l1_grad_2 = l1_out.grad.save()\n",
    "\n",
    "assert not th.allclose(l1_grad, l1_grad_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7809aa53",
   "metadata": {},
   "source": [
    "## 3) Use tracer.stop() to save useless computations\n",
    "If you're just computing activations, don't forget to call `tracer.stop()` at the end of your trace. This will stop the model from executing the rest of its computations, and save you some time, as demonstrated below (with the contribution of Claude 4 Sonnet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2ac14c2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎭 Welcome to the Theatrical Performance Comparison! 🎭\n",
      "============================================================\n",
      "\n",
      "🐌 ACT I: 'The Tragedy of the Unstoppable Tracer' 🐌\n",
      "In which our hero forgets to call tracer.stop()...\n",
      "⏰ Duration of suffering: 1.8352 seconds\n",
      "\n",
      "⚡ ACT II: 'The Redemption of the Stopped Tracer' ⚡\n",
      "Our hero learns the ancient art of tracer.stop()...\n",
      "⏰ Duration of enlightenment: 0.9426 seconds\n",
      "\n",
      "============================================================\n",
      "🎉 THE GRAND RESULTS SPECTACULAR! 🎉\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>🎭 Performance Type</th>\n",
       "      <th>⏱️ Time (seconds)</th>\n",
       "      <th>🎯 Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Without tracer.stop() 🐌</td>\n",
       "      <td>1.8352</td>\n",
       "      <td>Tragic 😭</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With tracer.stop() ⚡</td>\n",
       "      <td>0.9426</td>\n",
       "      <td>Magnificent! 🌟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Time Saved 💰</td>\n",
       "      <td>0.8926</td>\n",
       "      <td>PROFIT! 📈</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        🎭 Performance Type ⏱️ Time (seconds)        🎯 Rating\n",
       "0  Without tracer.stop() 🐌            1.8352        Tragic 😭\n",
       "1     With tracer.stop() ⚡            0.9426  Magnificent! 🌟\n",
       "2             Time Saved 💰            0.8926       PROFIT! 📈"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏎️ SPEEDUP METER 🏎️\n",
      "┌──────────────────────────────────────────────────┐\n",
      "│███████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░│\n",
      "└──────────────────────────────────────────────────┘\n",
      "   💫 COSMIC SPEEDUP: 1.95x FASTER! 💫\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(\n",
    "    \"🎭 Welcome to the Theatrical Performance Comparison! 🎭\\n\"\n",
    "    + \"=\" * 60\n",
    "    + \"\\n\\n🐌 ACT I: 'The Tragedy of the Unstoppable Tracer' 🐌\\nIn which our hero forgets to call tracer.stop()...\"\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(30):\n",
    "    with nnterp_gpt2.trace([\"Neel Samba\", \"Chris Aloha\"]):\n",
    "        out5 = nnterp_gpt2.layers_output[5].save()\n",
    "end_time = time.time()\n",
    "nostop_time = end_time - start_time\n",
    "\n",
    "print(\n",
    "    f\"⏰ Duration of suffering: {nostop_time:.4f} seconds\\n\\n⚡ ACT II: 'The Redemption of the Stopped Tracer' ⚡\\nOur hero learns the ancient art of tracer.stop()...\"\n",
    ")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(30):\n",
    "    with nnterp_gpt2.trace([\"Neel Samba\", \"Chris Aloha\"]) as tracer:\n",
    "        out5 = nnterp_gpt2.layers_output[5].save()\n",
    "        tracer.stop()\n",
    "end_time = time.time()\n",
    "stop_time = end_time - start_time\n",
    "\n",
    "print(f\"⏰ Duration of enlightenment: {stop_time:.4f} seconds\")\n",
    "\n",
    "speedup = nostop_time / stop_time\n",
    "time_saved = nostop_time - stop_time\n",
    "\n",
    "# fun display\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n🎉 THE GRAND RESULTS SPECTACULAR! 🎉\\n\" + \"=\" * 60)\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"🎭 Performance Type\": [\n",
    "            \"Without tracer.stop() 🐌\",\n",
    "            \"With tracer.stop() ⚡\",\n",
    "            \"Time Saved 💰\",\n",
    "        ],\n",
    "        \"⏱️ Time (seconds)\": [\n",
    "            f\"{nostop_time:.4f}\",\n",
    "            f\"{stop_time:.4f}\",\n",
    "            f\"{time_saved:.4f}\",\n",
    "        ],\n",
    "        \"🎯 Rating\": [\"Tragic 😭\", \"Magnificent! 🌟\", \"PROFIT! 📈\"],\n",
    "    }\n",
    ")\n",
    "display(results_df)\n",
    "speedup_bars = int(speedup * 10)\n",
    "meter = \"█\" * min(speedup_bars, 48) + \"░\" * (50 - min(speedup_bars, 48))\n",
    "print(\n",
    "    f\"\\n🏎️ SPEEDUP METER 🏎️\\n┌{'─' * 50}┐\\n│{meter}│\\n└{'─' * 50}┘\\n   💫 COSMIC SPEEDUP: {speedup:.2f}x FASTER! 💫\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6008bf2",
   "metadata": {},
   "source": [
    "## 4) Using NNsight builtin cache to collect activations\n",
    "\n",
    "`NNsight 0.5` introduces a builtin way to cache activations during the forward pass. Be careful not to call `tracer.stop()` before all the module of the cache have been accessed.\n",
    "\n",
    "The cache supports both renamed and original module names. You can access cached activations using attribute notation or dictionary keys.\n",
    "\n",
    "**Note:** There is a bug in the current version of `NNsight` ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfc6ddb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'model.language_model.layers' module path was never cached. 'CacheDict' has no matching attribute.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     cache \u001b[38;5;241m=\u001b[39m tracer\u001b[38;5;241m.\u001b[39mcache(modules\u001b[38;5;241m=\u001b[39m[layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m nnterp_gpt2\u001b[38;5;241m.\u001b[39mlayers[::\u001b[38;5;241m2\u001b[39m]])\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Access with renamed names using attribute notation\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m[\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39moutput)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Or using dictionary syntax with renamed path\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.layers.10\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39moutput)\n",
      "File \u001b[0;32m~/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py:156\u001b[0m, in \u001b[0;36mCache.CacheDict.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    154\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alias[attr]\n\u001b[1;32m    155\u001b[0m     name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mremoveprefix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m module path was never cached. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no matching attribute.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/nnterp/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py:158\u001b[0m, in \u001b[0;36mCache.CacheDict.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m module path was never cached. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no matching attribute.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'model.language_model.layers' module path was never cached. 'CacheDict' has no matching attribute."
     ]
    }
   ],
   "source": [
    "with nnterp_gpt2.trace(\"Hello\") as tracer:\n",
    "    cache = tracer.cache(modules=[layer for layer in nnterp_gpt2.layers[::2]]).save()\n",
    "\n",
    "# Access with renamed names using attribute notation\n",
    "print(cache.model.layers[10].output)\n",
    "# Or using dictionary syntax with renamed path\n",
    "print(cache[\"model.layers.10\"].output)\n",
    "# Original names still work\n",
    "print(cache[\"model.transformer.h.10\"].output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}