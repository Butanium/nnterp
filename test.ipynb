{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.0.dev2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import nnsight\n",
    "nnsight.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-25 12:37:28 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'InterventionGraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnnsight\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VLLM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# NNsight's VLLM wrapper currently supports \"device = cuda\" and device = \"auto\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m vllm \u001b[38;5;241m=\u001b[39m VLLM(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaykeye/TinyLLama-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, dispatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# See supported models: https://docs.vllm.ai/en/v0.6.4.post1/models/supported_models.html\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VLLM\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/vllm.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WrapperModule\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixins\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RemoteableMixin\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGPUWorker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NNsightGPUWorker\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NNsightSamplingParams\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fields\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/workers/GPUWorker.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Worker\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_runners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGPUModelRunner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NNsightGPUModelRunner\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNNsightGPUWorker\u001b[39;00m(Worker):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/model_runners/GPUModelRunner.py:32\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Patch, Patcher\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintervention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterleaver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interleaver\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NNsightSamplingMetadata\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabstract\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttentionBackend\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/sampling.py:45\u001b[0m\n\u001b[1;32m     40\u001b[0m             memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintervention_graph)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintervention_graph\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m, memo\u001b[38;5;241m=\u001b[39mmemo)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNNsightSamplingMetadata\u001b[39;00m(SamplingMetadata):\n\u001b[1;32m     47\u001b[0m     intervention_graph: Optional[InterventionGraph] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     nns_batch_groups: Optional[List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/sampling.py:47\u001b[0m, in \u001b[0;36mNNsightSamplingMetadata\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNNsightSamplingMetadata\u001b[39;00m(SamplingMetadata):\n\u001b[0;32m---> 47\u001b[0m     intervention_graph: Optional[\u001b[43mInterventionGraph\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     nns_batch_groups: Optional[List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     batch_groups: Optional[List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'InterventionGraph' is not defined"
     ]
    }
   ],
   "source": [
    "from nnsight.modeling.vllm import VLLM\n",
    "\n",
    "# NNsight's VLLM wrapper currently supports \"device = cuda\" and device = \"auto\"\n",
    "vllm = VLLM(\"Maykeye/TinyLLama-v0\", device = \"auto\", dispatch = True) # See supported models: https://docs.vllm.ai/en/v0.6.4.post1/models/supported_models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     * def forward(\n",
      "                                     0     self,\n",
      "                                     1     hidden_states: torch.Tensor,\n",
      "                                     2     attention_mask: Optional[torch.Tensor] = None,\n",
      "                                     3     position_ids: Optional[torch.LongTensor] = None,\n",
      "                                     4     past_key_value: Optional[Cache] = None,\n",
      "                                     5     output_attentions: Optional[bool] = False,\n",
      "                                     6     use_cache: Optional[bool] = False,\n",
      "                                     7     cache_position: Optional[torch.LongTensor] = None,\n",
      "                                     8     position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
      "                                     9     **kwargs: Unpack[FlashAttentionKwargs],\n",
      "                                    10 ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
      "                                    11     residual = hidden_states\n",
      " self_input_layernorm_0          -> 12     hidden_states = self.input_layernorm(hidden_states)\n",
      "                                    13 \n",
      "                                    14     # Self Attention\n",
      " self_self_attn_0                -> 15     hidden_states, self_attn_weights = self.self_attn(\n",
      "                                    16         hidden_states=hidden_states,\n",
      "                                    17         attention_mask=attention_mask,\n",
      "                                    18         position_ids=position_ids,\n",
      "                                    19         past_key_value=past_key_value,\n",
      "                                    20         output_attentions=output_attentions,\n",
      "                                    21         use_cache=use_cache,\n",
      "                                    22         cache_position=cache_position,\n",
      "                                    23         position_embeddings=position_embeddings,\n",
      "                                    24         **kwargs,\n",
      "                                    25     )\n",
      "                                    26     hidden_states = residual + hidden_states\n",
      "                                    27 \n",
      "                                    28     # Fully Connected\n",
      "                                    29     residual = hidden_states\n",
      " self_post_attention_layernorm_0 -> 30     hidden_states = self.post_attention_layernorm(hidden_states)\n",
      " self_mlp_0                      -> 31     hidden_states = self.mlp(hidden_states)\n",
      "                                    32     hidden_states = residual + hidden_states\n",
      "                                    33 \n",
      "                                    34     outputs = (hidden_states,)\n",
      "                                    35     if output_attentions:\n",
      "                                    36         outputs += (self_attn_weights,)\n",
      "                                    37 \n",
      "                                    38     return outputs\n",
      "                                    39 \n"
     ]
    }
   ],
   "source": [
    "print(llama_model.layers[0].source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['past_key_value', 'cache_position', 'attention_mask', 'head_mask', 'use_cache', 'output_attentions'])\n",
      "torch.Size([1, 1, 768])\n",
      "False\n",
      "dict_keys(['hidden_states', 'attention_mask', 'position_ids', 'past_key_value', 'output_attentions', 'use_cache', 'cache_position', 'position_embeddings'])\n",
      "torch.Size([1, 2, 64])\n",
      "hs vs input: True\n",
      "False\n",
      "tensor([[[ 3.4593e-01, -5.6658e-01,  4.0431e-01,  4.2073e-01, -1.2793e+00,\n",
      "          -1.6680e-01,  1.5621e-02, -4.5038e-01,  1.4098e-01,  6.4284e-01,\n",
      "          -6.5602e-01, -2.1935e-01,  1.2947e+00,  2.8273e-01, -2.5297e-01,\n",
      "          -1.9284e-02, -1.5790e-01,  1.3371e-01, -6.2846e-01, -2.4550e-01,\n",
      "           7.0501e-01, -2.3435e-01,  1.3520e-01,  8.9260e-01,  1.8702e-01,\n",
      "          -4.8103e-01, -2.1904e-01,  1.0528e+00,  9.3141e-01, -4.1581e-01,\n",
      "           3.0944e-01,  3.2379e-01,  5.5943e-01,  1.5155e-01, -1.0615e+00,\n",
      "          -3.4208e-01, -4.4287e-01, -5.4429e-01,  3.1906e-01, -4.4383e-01,\n",
      "           8.3231e-01,  1.6720e-01,  1.7813e-01,  7.2903e-02, -5.1841e-01,\n",
      "           2.7227e-01,  2.6494e-01, -8.1989e-01, -9.6978e-02, -3.1151e-01,\n",
      "           3.9501e-01, -3.2915e-01, -7.0201e-01,  2.7593e-01,  1.1744e+00,\n",
      "           3.9960e-01, -8.7335e-01,  3.0952e-01,  1.1688e-01,  4.0712e-01,\n",
      "           2.2961e-01, -3.0206e-01, -5.6703e-01, -7.2074e-01],\n",
      "         [-3.4727e-01, -9.4535e-02,  6.2619e-01, -3.7625e-01, -4.8867e-01,\n",
      "           3.0492e-01, -3.4166e-01, -5.2766e-01, -6.0010e-01, -5.4429e-04,\n",
      "           9.8598e-02, -3.6297e-01,  3.6730e-01, -2.8470e-02,  9.8457e-02,\n",
      "           6.0052e-01,  4.7675e-01,  5.7742e-01, -1.4259e-01,  1.3548e-01,\n",
      "           1.3848e-01, -4.4263e-01,  6.2765e-01, -2.4480e-01,  6.1896e-04,\n",
      "          -3.9577e-01,  3.4121e-01,  7.4848e-02,  8.0165e-01,  1.0266e-01,\n",
      "           7.4946e-02, -2.5318e-01,  3.5969e-02, -7.2365e-02, -4.5921e-01,\n",
      "          -1.3125e-01,  1.4520e-01,  4.8371e-01,  3.2229e-01, -3.2390e-01,\n",
      "          -4.4408e-01,  3.3657e-01,  3.1138e-01,  2.5029e-01, -3.2089e-01,\n",
      "          -7.4725e-01,  3.4783e-01, -3.9503e-01,  1.2739e-01, -4.1104e-01,\n",
      "          -2.8195e-01,  5.7564e-02, -6.8767e-01,  6.6420e-02,  8.9407e-01,\n",
      "          -6.8424e-02,  1.1301e-01, -1.5066e-01,  3.0354e-01,  5.6559e-01,\n",
      "           4.2487e-02, -1.3749e-01, -1.1738e-01,  1.6622e-01]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[ 6.4125e-01, -1.0503e+00,  7.4947e-01,  7.7990e-01, -2.3714e+00,\n",
      "          -3.0920e-01,  2.8957e-02, -8.3487e-01,  2.6134e-01,  1.1916e+00,\n",
      "          -1.2161e+00, -4.0661e-01,  2.4000e+00,  5.2410e-01, -4.6892e-01,\n",
      "          -3.5747e-02, -2.9269e-01,  2.4786e-01, -1.1650e+00, -4.5508e-01,\n",
      "           1.3069e+00, -4.3441e-01,  2.5061e-01,  1.6546e+00,  3.4669e-01,\n",
      "          -8.9167e-01, -4.0603e-01,  1.9516e+00,  1.7265e+00, -7.7078e-01,\n",
      "           5.7360e-01,  6.0021e-01,  1.0370e+00,  2.8093e-01, -1.9677e+00,\n",
      "          -6.3411e-01, -8.2095e-01, -1.0089e+00,  5.9143e-01, -8.2273e-01,\n",
      "           1.5428e+00,  3.0995e-01,  3.3020e-01,  1.3514e-01, -9.6097e-01,\n",
      "           5.0471e-01,  4.9113e-01, -1.5198e+00, -1.7977e-01, -5.7744e-01,\n",
      "           7.3222e-01, -6.1013e-01, -1.3013e+00,  5.1149e-01,  2.1771e+00,\n",
      "           7.4074e-01, -1.6189e+00,  5.7375e-01,  2.1667e-01,  7.5468e-01,\n",
      "           4.2562e-01, -5.5993e-01, -1.0511e+00, -1.3360e+00],\n",
      "         [-9.3473e-01, -2.5446e-01,  1.6855e+00, -1.0127e+00, -1.3153e+00,\n",
      "           8.2075e-01, -9.1963e-01, -1.4203e+00, -1.6153e+00, -1.4650e-03,\n",
      "           2.6539e-01, -9.7700e-01,  9.8865e-01, -7.6631e-02,  2.6501e-01,\n",
      "           1.6164e+00,  1.2833e+00,  1.5542e+00, -3.8380e-01,  3.6466e-01,\n",
      "           3.7273e-01, -1.1914e+00,  1.6894e+00, -6.5893e-01,  1.6660e-03,\n",
      "          -1.0653e+00,  9.1842e-01,  2.0147e-01,  2.1578e+00,  2.7631e-01,\n",
      "           2.0173e-01, -6.8146e-01,  9.6816e-02, -1.9478e-01, -1.2360e+00,\n",
      "          -3.5328e-01,  3.9083e-01,  1.3020e+00,  8.6749e-01, -8.7183e-01,\n",
      "          -1.1953e+00,  9.0594e-01,  8.3812e-01,  6.7369e-01, -8.6373e-01,\n",
      "          -2.0113e+00,  9.3624e-01, -1.0633e+00,  3.4288e-01, -1.1064e+00,\n",
      "          -7.5892e-01,  1.5494e-01, -1.8510e+00,  1.7878e-01,  2.4065e+00,\n",
      "          -1.8417e-01,  3.0418e-01, -4.0552e-01,  8.1702e-01,  1.5224e+00,\n",
      "           1.1436e-01, -3.7008e-01, -3.1595e-01,  4.4741e-01]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "from nnterp.nnsight_utils import get_attention\n",
    "import torch as th\n",
    "model = StandardizedTransformer(\"gpt2\", device_map=\"auto\")\n",
    "model.tokenizer.chat_template\n",
    "with model.trace(\"a\"):\n",
    "    print(\n",
    "    get_attention(model, 0).inputs[1].keys())\n",
    "    print(get_attention(model, 0).input.shape)\n",
    "    print(th.allclose(model.layers_output[0], get_attention(model, 1).input))\n",
    "\n",
    "llama_model = StandardizedTransformer(\"Maykeye/TinyLLama-v0\", device_map=\"auto\")\n",
    "with llama_model.trace(\"a\"):\n",
    "    print(\n",
    "    get_attention(llama_model, 0).inputs[1].keys())\n",
    "    print(get_attention(llama_model, 0).input.shape)\n",
    "    layer_output = llama_model.layers_output[0]\n",
    "    attention_input = get_attention(llama_model, 1).input\n",
    "    attn_hidden_states = get_attention(llama_model, 1).inputs[1][\"hidden_states\"]\n",
    "    print(f\"hs vs input: {th.allclose(attn_hidden_states, attention_input)}\")\n",
    "    print(th.allclose(layer_output, attention_input))\n",
    "    print(layer_output)\n",
    "    print(attention_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        * def sdpa_attention_forward(\n",
      "                                                        0     module: torch.nn.Module,\n",
      "                                                        1     query: torch.Tensor,\n",
      "                                                        2     key: torch.Tensor,\n",
      "                                                        3     value: torch.Tensor,\n",
      "                                                        4     attention_mask: Optional[torch.Tensor],\n",
      "                                                        5     dropout: float = 0.0,\n",
      "                                                        6     scaling: Optional[float] = None,\n",
      "                                                        7     is_causal: Optional[bool] = None,\n",
      "                                                        8     **kwargs,\n",
      "                                                        9 ) -> tuple[torch.Tensor, None]:\n",
      " kwargs_get_0                                       -> 10     if kwargs.get(\"output_attentions\", False) or kwargs.get(\"head_mask\", None) is not None:\n",
      " kwargs_get_1                                       ->  +     ...\n",
      " logger_warning_once_0                              -> 11         logger.warning_once(\n",
      "                                                       12             \"`sdpa` attention does not support `output_attentions=True` or `head_mask`.\"\n",
      "                                                       13             \" Please set your attention to `eager` if you want any of these features.\"\n",
      "                                                       14         )\n",
      "                                                       15 \n",
      " hasattr_0                                          -> 16     if hasattr(module, \"num_key_value_groups\"):\n",
      " repeat_kv_0                                        -> 17         key = repeat_kv(key, module.num_key_value_groups)\n",
      " repeat_kv_1                                        -> 18         value = repeat_kv(value, module.num_key_value_groups)\n",
      "                                                       19 \n",
      "                                                       20     if attention_mask is not None and attention_mask.ndim == 4:\n",
      "                                                       21         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n",
      "                                                       22 \n",
      "                                                       23     # SDPA with memory-efficient backend is bugged with non-contiguous inputs and custom attn_mask for some torch versions\n",
      "                                                       24     # Reference: https://github.com/pytorch/pytorch/issues/112577.\n",
      " query_contiguous_0                                 -> 25     query = query.contiguous()\n",
      " key_contiguous_0                                   -> 26     key = key.contiguous()\n",
      " value_contiguous_0                                 -> 27     value = value.contiguous()\n",
      "                                                       28 \n",
      "                                                       29     # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n",
      "                                                       30     # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n",
      "                                                       31     # Note that it is important to check first for the shape, otherwise compile will fail with `argument 'is_causal' must be bool, not SymBool`\n",
      "                                                       32     if is_causal is None:\n",
      "                                                       33         # The last condition is for encoder (decoder) models which specify this by passing their own `is_causal` flag\n",
      "                                                       34         # This is mainly due to those models having mixed implementations for encoder, decoder, and encoder-decoder attns\n",
      " getattr_0                                          -> 35         is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n",
      "                                                       36 \n",
      "                                                       37     # Shapes (e.g. query.shape[2]) are tensors during jit tracing, resulting in `is_causal` being a tensor.\n",
      "                                                       38     # We convert it to a bool for the SDPA kernel that only accepts bools.\n",
      " torch_jit_is_tracing_0                             -> 39     if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):\n",
      " isinstance_0                                       ->  +     ...\n",
      " is_causal_item_0                                   -> 40         is_causal = is_causal.item()\n",
      "                                                       41 \n",
      " torch_nn_functional_scaled_dot_product_attention_0 -> 42     attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "                                                       43         query,\n",
      "                                                       44         key,\n",
      "                                                       45         value,\n",
      "                                                       46         attn_mask=attention_mask,\n",
      "                                                       47         dropout_p=dropout,\n",
      "                                                       48         scale=scaling,\n",
      "                                                       49         is_causal=is_causal,\n",
      "                                                       50     )\n",
      " attn_output_transpose_0                            -> 51     attn_output = attn_output.transpose(1, 2).contiguous()\n",
      " contiguous_0                                       ->  +     ...\n",
      "                                                       52 \n",
      "                                                       53     return attn_output, None\n",
      "                                                       54 \n"
     ]
    },
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_12034/2327001449.py\", line 6, in <module>\n    print(model.model.layers[0].self_attn.source.attention_interface_0.source.torch_nn_functional_scaled_dot_product_attention_0.source)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 1183, in source\n    source, line_numbers, fn = inject(fn, wrap, self.name)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/inject.py\", line 60, in convert\n    source = textwrap.dedent(inspect.getsource(fn))\n  File \"/usr/lib/python3.10/inspect.py\", line 1139, in getsource\n    lines, lnum = getsourcelines(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 1121, in getsourcelines\n    lines, lnum = findsource(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 940, in findsource\n    file = getsourcefile(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 817, in getsourcefile\n    filename = getfile(object)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/package/package_importer.py\", line 725, in _patched_getfile\n    return _orig_getfile(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 797, in getfile\n    raise TypeError('module, class, method, function, traceback, frame, or '\n\nTypeError: module, class, method, function, traceback, frame, or code object was expected, got builtin_function_or_method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnnsight\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LanguageModel\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m LanguageModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaykeye/TinyLLama-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello the fox is jumping\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mattention_interface_0\u001b[38;5;241m.\u001b[39msource)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mattention_interface_0\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mtorch_nn_functional_scaled_dot_product_attention_0\u001b[38;5;241m.\u001b[39msource)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Suppress the ExitTracingException but let other exceptions propagate\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     21\u001b[0m     tracer\u001b[38;5;241m.\u001b[39mexecute(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     Globals\u001b[38;5;241m.\u001b[39mexit()\n",
      "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_12034/2327001449.py\", line 6, in <module>\n    print(model.model.layers[0].self_attn.source.attention_interface_0.source.torch_nn_functional_scaled_dot_product_attention_0.source)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 1183, in source\n    source, line_numbers, fn = inject(fn, wrap, self.name)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/inject.py\", line 60, in convert\n    source = textwrap.dedent(inspect.getsource(fn))\n  File \"/usr/lib/python3.10/inspect.py\", line 1139, in getsource\n    lines, lnum = getsourcelines(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 1121, in getsourcelines\n    lines, lnum = findsource(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 940, in findsource\n    file = getsourcefile(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 817, in getsourcefile\n    filename = getfile(object)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/package/package_importer.py\", line 725, in _patched_getfile\n    return _orig_getfile(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 797, in getfile\n    raise TypeError('module, class, method, function, traceback, frame, or '\n\nTypeError: module, class, method, function, traceback, frame, or code object was expected, got builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel(\"Maykeye/TinyLLama-v0\")\n",
    "with model.trace([\"hello\", \"hello the fox is jumping\"]):\n",
    "    print(model.model.layers[0].self_attn.source.attention_interface_0.source)\n",
    "    print(model.model.layers[0].self_attn.source.attention_interface_0.source.torch_nn_functional_scaled_dot_product_attention_0.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_12034/982287227.py\", line 7, in <module>\n    print(model.attentions[0].source.attention_interface_0.source.torch_nn_functional_scaled_dot_product_attention_0.source)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 1183, in source\n    source, line_numbers, fn = inject(fn, wrap, self.name)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/inject.py\", line 60, in convert\n    source = textwrap.dedent(inspect.getsource(fn))\n  File \"/usr/lib/python3.10/inspect.py\", line 1139, in getsource\n    lines, lnum = getsourcelines(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 1121, in getsourcelines\n    lines, lnum = findsource(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 940, in findsource\n    file = getsourcefile(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 817, in getsourcefile\n    filename = getfile(object)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/package/package_importer.py\", line 725, in _patched_getfile\n    return _orig_getfile(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 797, in getfile\n    raise TypeError('module, class, method, function, traceback, frame, or '\n\nTypeError: module, class, method, function, traceback, frame, or code object was expected, got builtin_function_or_method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m StandardizedTransformer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaykeye/TinyLLama-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;66;03m#, attn_implementation=\"eager\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print(model.attentions[0].source)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(\"================\"*3)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello the fox is jumping\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mattentions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mattention_interface_0\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mtorch_nn_functional_scaled_dot_product_attention_0\u001b[38;5;241m.\u001b[39msource)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mattentions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mattention_interface_0\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Suppress the ExitTracingException but let other exceptions propagate\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     21\u001b[0m     tracer\u001b[38;5;241m.\u001b[39mexecute(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     Globals\u001b[38;5;241m.\u001b[39mexit()\n",
      "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_12034/982287227.py\", line 7, in <module>\n    print(model.attentions[0].source.attention_interface_0.source.torch_nn_functional_scaled_dot_product_attention_0.source)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 1183, in source\n    source, line_numbers, fn = inject(fn, wrap, self.name)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/inject.py\", line 60, in convert\n    source = textwrap.dedent(inspect.getsource(fn))\n  File \"/usr/lib/python3.10/inspect.py\", line 1139, in getsource\n    lines, lnum = getsourcelines(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 1121, in getsourcelines\n    lines, lnum = findsource(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 940, in findsource\n    file = getsourcefile(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 817, in getsourcefile\n    filename = getfile(object)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/package/package_importer.py\", line 725, in _patched_getfile\n    return _orig_getfile(object)\n  File \"/usr/lib/python3.10/inspect.py\", line 797, in getfile\n    raise TypeError('module, class, method, function, traceback, frame, or '\n\nTypeError: module, class, method, function, traceback, frame, or code object was expected, got builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "\n",
    "model = StandardizedTransformer(\"Maykeye/TinyLLama-v0\")#, attn_implementation=\"eager\")\n",
    "# print(model.attentions[0].source)\n",
    "# print(\"================\"*3)\n",
    "with model.trace([\"hello\", \"hello the fox is jumping\"]):\n",
    "    print(model.attentions[0].source.attention_interface_0.source.torch_nn_functional_scaled_dot_product_attention_0.source)\n",
    "    print(model.attentions[0].source.attention_interface_0.output[1].shape)\n",
    "    print(model.layers_output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 4,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 64,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 256,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"num_key_value_heads\": 16,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.53.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "import torch as th\n",
    "model = LanguageModel(\"Maykeye/TinyLLama-v0\", device_map=\"cuda\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__enter__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, world!\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      8\u001b[0m     layer_0_out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m start_from_2(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, world!\u001b[39m\u001b[38;5;124m\"\u001b[39m, layer_0_out) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39moutput)\n",
      "\u001b[0;31mAttributeError\u001b[0m: __enter__"
     ]
    }
   ],
   "source": [
    "from nnterp.nnsight_utils import skip_layers\n",
    "\n",
    "def start_from_2(model: LanguageModel, prompt: str, tensor):\n",
    "    with model.trace(prompt) as trace:\n",
    "        skip_layers(model, 0, 2, tensor)\n",
    "        return trace\n",
    "with model.trace(\"Hello, world!\"):\n",
    "    layer_0_out = model.model.layers[0].output[0].save()\n",
    "with start_from_2(model, \"Hello, world!\", layer_0_out) as trace:\n",
    "    print(model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnterp.nnsight_utils import skip_layers, set_layer_output, get_layer_output\n",
    "import torch as th\n",
    "with model.trace(\"Hello, world!\"):\n",
    "    out = model.lm_head.output.save()\n",
    "with model.trace(\"Hello, world!\"):\n",
    "    skip_layers(model, 1, 5)\n",
    "    skip_out = model.lm_head.output.save()\n",
    "with model.trace(\"Hello, world!\"):\n",
    "    set_layer_output(model, 5, get_layer_output(model, 0))\n",
    "    old_skip_out = model.lm_head.output.save()\n",
    "\n",
    "assert not th.allclose(out, skip_out)\n",
    "assert th.allclose(old_skip_out, skip_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First method succeeded\n"
     ]
    },
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 21, in __call__\n    tracer.execute(fn)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 331, in execute\n    self.model.interleave(interleaver, self.fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py\", line 76, in interleave\n    return super().interleave(interleaver, fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 705, in interleave\n    interleaver(fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 310, in __call__\n    fn(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 372, in __call__\n    else self._module(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/modeling_layers.py\", line 48, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 145, in apply_rotary_pos_emb\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n\nRuntimeError: The size of tensor a (16) must match the size of tensor b (4) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mskip((\u001b[38;5;28minput\u001b[39m,))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst method succeeded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minput\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mskip(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Suppress the ExitTracingException but let other exceptions propagate\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     21\u001b[0m     tracer\u001b[38;5;241m.\u001b[39mexecute(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     Globals\u001b[38;5;241m.\u001b[39mexit()\n",
      "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 21, in __call__\n    tracer.execute(fn)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 331, in execute\n    self.model.interleave(interleaver, self.fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py\", line 76, in interleave\n    return super().interleave(interleaver, fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 705, in interleave\n    interleaver(fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 310, in __call__\n    fn(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 372, in __call__\n    else self._module(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/modeling_layers.py\", line 48, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 145, in apply_rotary_pos_emb\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n\nRuntimeError: The size of tensor a (16) must match the size of tensor b (4) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"Maykeye/TinyLLama-v0\")\n",
    "with model.trace(\"hello\"):\n",
    "    input = model.model.layers[0].input\n",
    "    model.model.layers[0].skip((input,))\n",
    "print(\"First method succeeded\")\n",
    "with model.trace(\"hello\"):\n",
    "    input = model.model.layers[0].input\n",
    "    model.model.layers[0].skip(input)\n",
    "print(\"Second method succeeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn): GPT2SdpaAttention(\n",
      "          (self_attn): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Conv1D' object has no attribute 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# broken_model = LanguageModel(\"gpt2\", rename=rename)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(broken_model)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbroken_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbroken_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_broken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbroken_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Travail\\Documents\\repos\\nnterp\\.venv\\Lib\\site-packages\\nnsight\\intervention\\contexts\\interleaving.py:96\u001b[0m, in \u001b[0;36mInterleavingTracer.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Travail\\Documents\\repos\\nnterp\\.venv\\Lib\\site-packages\\nnsight\\tracing\\contexts\\tracer.py:26\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[0;32m     24\u001b[0m GlobalTracingContext\u001b[38;5;241m.\u001b[39mtry_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Travail\\Documents\\repos\\nnterp\\.venv\\Lib\\site-packages\\nnsight\\tracing\\contexts\\base.py:72\u001b[0m, in \u001b[0;36mContext.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m     69\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(graph\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], graph, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m broken_model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      5\u001b[0m     out \u001b[38;5;241m=\u001b[39m broken_model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m----> 6\u001b[0m     out_broken \u001b[38;5;241m=\u001b[39m \u001b[43mbroken_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\Travail\\Documents\\repos\\nnterp\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Conv1D' object has no attribute 'output'"
     ]
    }
   ],
   "source": [
    "rename = {\".*attn\": \"self_attn\"}\n",
    "# broken_model = LanguageModel(\"gpt2\", rename=rename)\n",
    "print(broken_model)\n",
    "with broken_model.trace(\"hello\"):\n",
    "    out = broken_model.lm_head.output.save()\n",
    "    out_broken = broken_model.transformer.h[9].self_attn.c_attn.output.save()\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(\"gpt2\")\n",
    "with model.trace(\"hello\"):\n",
    "    out_normal = model.lm_head.output.save()\n",
    "from torch.testing import assert_close\n",
    "assert_close(out_normal, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.*\\\\.attn$': 'self_attn', '.*\\\\.self_attention$': 'self_attn', '.*\\\\.attention$': 'self_attn', '.*\\\\.transformer$': 'model', '.*\\\\.gpt_neox$': 'model', '.*\\\\.h$': 'layers', '.*\\\\.final_layer_norm$': 'ln_final', '.*\\\\.ln_f$': 'ln_final', '.*\\\\.embed_out$': 'lm_head'}\n",
      "GPT2LMHeadModel(\n",
      "  (model): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "attention_names = [r\".*\\.attn$\", r\".*\\.self_attention$\", r\".*\\.attention$\"]\n",
    "model_names = [r\".*\\.transformer$\", r\".*\\.gpt_neox$\"]\n",
    "layer_names = [r\".*\\.h$\"]\n",
    "ln_names = [r\".*\\.final_layer_norm$\", r\".*\\.ln_f$\"]\n",
    "lm_head_names = [r\".*\\.embed_out$\"]\n",
    "\n",
    "\n",
    "llm_rename_dict = {}\n",
    "for name in attention_names:\n",
    "    llm_rename_dict[name] = \"self_attn\"\n",
    "for name in model_names:\n",
    "    llm_rename_dict[name] = \"model\"\n",
    "for name in layer_names:\n",
    "    llm_rename_dict[name] = \"layers\"\n",
    "for name in ln_names:\n",
    "    llm_rename_dict[name] = \"ln_final\"\n",
    "for name in lm_head_names:\n",
    "    llm_rename_dict[name] = \"lm_head\"\n",
    "\n",
    "print(llm_rename_dict)\n",
    "patched_model = LanguageModel(\"gpt2\", rename=llm_rename_dict)\n",
    "print(patched_model)\n",
    "# print(LanguageModel(\"gpt2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (generator): Generator(\n",
       "    (streamer): Streamer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LanguageModel(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Travail\\Documents\\repos\\nnterp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Travail\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  (generator): Generator(\n",
       "    (streamer): Streamer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LanguageModel(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (self_attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (self_attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nnterp import load_model\n",
    "patched_model = load_model('gpt2', use_model_renaming=True)\n",
    "patched_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (self_attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (self_attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (self_attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "GPT2SdpaAttention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(patched_model.model)\n",
    "print()\n",
    "print(patched_model.model.layers[0])\n",
    "print()\n",
    "print(patched_model.model.layers[0].self_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "with patched_model.trace(\"hello\"):\n",
    "    out = patched_model.model.layers[0].output[0].save()\n",
    "with patched_model.trace(\"hello\"):\n",
    "    out_old = patched_model.transformer.h[0].output[0].save()\n",
    "torch.allclose(out, out_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'GPT2Block' object has no attribute 'output'\n"
     ]
    }
   ],
   "source": [
    "del patched_model._envoy.transformer\n",
    "with patched_model.trace(\"hello\"):\n",
    "    out = patched_model.model.layers[0].output[0].save()\n",
    "try:\n",
    "    with patched_model.trace(\"hello\"):\n",
    "        out_old = patched_model.transformer.h[0].output[0].save()\n",
    "    torch.allclose(out, out_old)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'GPT2LMHeadModel' object has no attribute 'transformer'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "'GPT2LMHeadModel' object has no attribute 'transformer'\n"
     ]
    }
   ],
   "source": [
    "del patched_model._model.transformer\n",
    "try:\n",
    "    with patched_model.trace(\"hello\"):\n",
    "        out = patched_model.model.layers[0].output[0].save()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(\"-\"*100)\n",
    "try:\n",
    "    with patched_model.trace(\"hello\"):\n",
    "        out_old = patched_model.transformer.h[0].output[0].save()\n",
    "    torch.allclose(out, out_old)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dlabscratch1/cdumas/nnterp/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('gpt2', use_model_renaming=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nnsight.envoy.Envoy"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model._envoy.transformer.h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 4096)\n",
       "    (word_embeddings_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-29): 30 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=250880, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model(\"bigscience/bloom-7b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 768)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXSdpaAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model(\"EleutherAI/pythia-160m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
