{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.0.dev2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import nnsight\n",
    "nnsight.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-25 12:37:28 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'InterventionGraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnnsight\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VLLM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# NNsight's VLLM wrapper currently supports \"device = cuda\" and device = \"auto\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m vllm \u001b[38;5;241m=\u001b[39m VLLM(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaykeye/TinyLLama-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, dispatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# See supported models: https://docs.vllm.ai/en/v0.6.4.post1/models/supported_models.html\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VLLM\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/vllm.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WrapperModule\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixins\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RemoteableMixin\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGPUWorker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NNsightGPUWorker\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NNsightSamplingParams\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fields\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/workers/GPUWorker.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Worker\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_runners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGPUModelRunner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NNsightGPUModelRunner\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNNsightGPUWorker\u001b[39;00m(Worker):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/model_runners/GPUModelRunner.py:32\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Patch, Patcher\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintervention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterleaver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interleaver\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NNsightSamplingMetadata\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabstract\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttentionBackend\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/sampling.py:45\u001b[0m\n\u001b[1;32m     40\u001b[0m             memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintervention_graph)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintervention_graph\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m, memo\u001b[38;5;241m=\u001b[39mmemo)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNNsightSamplingMetadata\u001b[39;00m(SamplingMetadata):\n\u001b[1;32m     47\u001b[0m     intervention_graph: Optional[InterventionGraph] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     nns_batch_groups: Optional[List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/sampling.py:47\u001b[0m, in \u001b[0;36mNNsightSamplingMetadata\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNNsightSamplingMetadata\u001b[39;00m(SamplingMetadata):\n\u001b[0;32m---> 47\u001b[0m     intervention_graph: Optional[\u001b[43mInterventionGraph\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     nns_batch_groups: Optional[List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     batch_groups: Optional[List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'InterventionGraph' is not defined"
     ]
    }
   ],
   "source": [
    "from nnsight.modeling.vllm import VLLM\n",
    "\n",
    "# NNsight's VLLM wrapper currently supports \"device = cuda\" and device = \"auto\"\n",
    "vllm = VLLM(\"Maykeye/TinyLLama-v0\", device = \"auto\", dispatch = True) # See supported models: https://docs.vllm.ai/en/v0.6.4.post1/models/supported_models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     * def forward(\n",
      "                                     0     self,\n",
      "                                     1     hidden_states: torch.Tensor,\n",
      "                                     2     attention_mask: Optional[torch.Tensor] = None,\n",
      "                                     3     position_ids: Optional[torch.LongTensor] = None,\n",
      "                                     4     past_key_value: Optional[Cache] = None,\n",
      "                                     5     output_attentions: Optional[bool] = False,\n",
      "                                     6     use_cache: Optional[bool] = False,\n",
      "                                     7     cache_position: Optional[torch.LongTensor] = None,\n",
      "                                     8     position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
      "                                     9     **kwargs: Unpack[FlashAttentionKwargs],\n",
      "                                    10 ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
      "                                    11     residual = hidden_states\n",
      " self_input_layernorm_0          -> 12     hidden_states = self.input_layernorm(hidden_states)\n",
      "                                    13 \n",
      "                                    14     # Self Attention\n",
      " self_self_attn_0                -> 15     hidden_states, self_attn_weights = self.self_attn(\n",
      "                                    16         hidden_states=hidden_states,\n",
      "                                    17         attention_mask=attention_mask,\n",
      "                                    18         position_ids=position_ids,\n",
      "                                    19         past_key_value=past_key_value,\n",
      "                                    20         output_attentions=output_attentions,\n",
      "                                    21         use_cache=use_cache,\n",
      "                                    22         cache_position=cache_position,\n",
      "                                    23         position_embeddings=position_embeddings,\n",
      "                                    24         **kwargs,\n",
      "                                    25     )\n",
      "                                    26     hidden_states = residual + hidden_states\n",
      "                                    27 \n",
      "                                    28     # Fully Connected\n",
      "                                    29     residual = hidden_states\n",
      " self_post_attention_layernorm_0 -> 30     hidden_states = self.post_attention_layernorm(hidden_states)\n",
      " self_mlp_0                      -> 31     hidden_states = self.mlp(hidden_states)\n",
      "                                    32     hidden_states = residual + hidden_states\n",
      "                                    33 \n",
      "                                    34     outputs = (hidden_states,)\n",
      "                                    35     if output_attentions:\n",
      "                                    36         outputs += (self_attn_weights,)\n",
      "                                    37 \n",
      "                                    38     return outputs\n",
      "                                    39 \n"
     ]
    }
   ],
   "source": [
    "print(llama_model.layers[0].source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/config.json\n",
      "Model config Gemma2Config {\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/config.json\n",
      "Model config Gemma2Config {\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/tokenizer.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Instantiating Gemma2ForCausalLM model under default dtype torch.float32.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Gemma2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "cfg = AutoConfig.from_pretrained(\"google/gemma-2-2b\")\n",
    "print(cfg._attn_implementation)\n",
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"google/gemma-2-2b\", device_map=\"auto\", attn_implementation=\"flash_attention_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/config.json\n",
      "Model config Gemma2Config {\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/tokenizer.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Instantiating Gemma2ForCausalLM model under default dtype torch.float32.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch in attribute '__dict__[_attn_implementation_internal]': model.config=None, model._model.config='sdpa'\n",
      "Mismatch in attribute '__dict__[_attn_implementation_autoset]': model.config=False, model._model.config=True\n",
      "Mismatch in attribute '_attn_implementation': model.config='eager', model._model.config='sdpa'\n",
      "Mismatch in attribute '_attn_implementation_autoset': model.config=False, model._model.config=True\n",
      "Mismatch in attribute '_attn_implementation_internal': model.config=None, model._model.config='sdpa'\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"google/gemma-2-2b\", device_map=\"auto\")\n",
    "model_config = model.config\n",
    "model_model_config = model._model.config\n",
    "\n",
    "def get_attrs(obj):\n",
    "    return [attr for attr in dir(obj) if not callable(getattr(obj, attr))]\n",
    "\n",
    "attrs = set(get_attrs(model_config)) | set(get_attrs(model_model_config))\n",
    "\n",
    "for attr in sorted(attrs):\n",
    "    val1 = getattr(model_config, attr, None)\n",
    "    val2 = getattr(model_model_config, attr, None)\n",
    "    if val1 != val2:\n",
    "        if isinstance(val1, dict) and isinstance(val2, dict):\n",
    "            if val1.keys() != val2.keys():\n",
    "                print(f\"Mismatch in attribute keys '{attr}': model.config={val1.keys()}, model._model.config={val2.keys()}\")\n",
    "            else:\n",
    "                for k, v in val1.items():\n",
    "                    if v != val2[k]:\n",
    "                        print(f\"Mismatch in attribute '{attr}[{k}]': model.config={v!r}, model._model.config={val2[k]!r}\")\n",
    "        else:\n",
    "            print(f\"Mismatch in attribute '{attr}': model.config={val1!r}, model._model.config={val2!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/config.json\n",
      "Model config Gemma2Config {\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "Instantiating Gemma2ForCausalLM model under default dtype torch.float32.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager\n",
      "eager\n",
      "eager\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a02a03a0f646ef8f122d46c7120646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Gemma2ForCausalLM.\n",
      "\n",
      "All the weights of Gemma2ForCausalLM were initialized from the model checkpoint at google/gemma-2-2b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Gemma2ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager\n",
      "eager\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "cfg = AutoConfig.from_pretrained(\"google/gemma-2-2b\", attn_implementation=\"eager\")\n",
    "model = AutoModelForCausalLM.from_config(cfg)\n",
    "print(cfg._attn_implementation)\n",
    "print(model.config._attn_implementation)\n",
    "print(model.model.layers[0].self_attn.config._attn_implementation)\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\", config=cfg)\n",
    "model2.config\n",
    "print(model2.config._attn_implementation)\n",
    "print(model2.model.layers[0].self_attn.config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AutoConfig.__init__() got an unexpected keyword argument 'architectures'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mAutoConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: AutoConfig.__init__() got an unexpected keyword argument 'architectures'"
     ]
    }
   ],
   "source": [
    "from transformers.utils import cached_file\n",
    "import json\n",
    "file = cached_file(\"google/gemma-2-2b\", \"config.json\")\n",
    "with open(file, \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "AutoConfig(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['past_key_value', 'cache_position', 'attention_mask', 'head_mask', 'use_cache', 'output_attentions'])\n",
      "torch.Size([1, 1, 768])\n",
      "False\n",
      "dict_keys(['hidden_states', 'attention_mask', 'position_ids', 'past_key_value', 'output_attentions', 'use_cache', 'cache_position', 'position_embeddings'])\n",
      "torch.Size([1, 2, 64])\n",
      "hs vs input: True\n",
      "False\n",
      "tensor([[[ 3.4593e-01, -5.6658e-01,  4.0431e-01,  4.2073e-01, -1.2793e+00,\n",
      "          -1.6680e-01,  1.5621e-02, -4.5038e-01,  1.4098e-01,  6.4284e-01,\n",
      "          -6.5602e-01, -2.1935e-01,  1.2947e+00,  2.8273e-01, -2.5297e-01,\n",
      "          -1.9284e-02, -1.5790e-01,  1.3371e-01, -6.2846e-01, -2.4550e-01,\n",
      "           7.0501e-01, -2.3435e-01,  1.3520e-01,  8.9260e-01,  1.8702e-01,\n",
      "          -4.8103e-01, -2.1904e-01,  1.0528e+00,  9.3141e-01, -4.1581e-01,\n",
      "           3.0944e-01,  3.2379e-01,  5.5943e-01,  1.5155e-01, -1.0615e+00,\n",
      "          -3.4208e-01, -4.4287e-01, -5.4429e-01,  3.1906e-01, -4.4383e-01,\n",
      "           8.3231e-01,  1.6720e-01,  1.7813e-01,  7.2903e-02, -5.1841e-01,\n",
      "           2.7227e-01,  2.6494e-01, -8.1989e-01, -9.6978e-02, -3.1151e-01,\n",
      "           3.9501e-01, -3.2915e-01, -7.0201e-01,  2.7593e-01,  1.1744e+00,\n",
      "           3.9960e-01, -8.7335e-01,  3.0952e-01,  1.1688e-01,  4.0712e-01,\n",
      "           2.2961e-01, -3.0206e-01, -5.6703e-01, -7.2074e-01],\n",
      "         [-3.4727e-01, -9.4535e-02,  6.2619e-01, -3.7625e-01, -4.8867e-01,\n",
      "           3.0492e-01, -3.4166e-01, -5.2766e-01, -6.0010e-01, -5.4429e-04,\n",
      "           9.8598e-02, -3.6297e-01,  3.6730e-01, -2.8470e-02,  9.8457e-02,\n",
      "           6.0052e-01,  4.7675e-01,  5.7742e-01, -1.4259e-01,  1.3548e-01,\n",
      "           1.3848e-01, -4.4263e-01,  6.2765e-01, -2.4480e-01,  6.1896e-04,\n",
      "          -3.9577e-01,  3.4121e-01,  7.4848e-02,  8.0165e-01,  1.0266e-01,\n",
      "           7.4946e-02, -2.5318e-01,  3.5969e-02, -7.2365e-02, -4.5921e-01,\n",
      "          -1.3125e-01,  1.4520e-01,  4.8371e-01,  3.2229e-01, -3.2390e-01,\n",
      "          -4.4408e-01,  3.3657e-01,  3.1138e-01,  2.5029e-01, -3.2089e-01,\n",
      "          -7.4725e-01,  3.4783e-01, -3.9503e-01,  1.2739e-01, -4.1104e-01,\n",
      "          -2.8195e-01,  5.7564e-02, -6.8767e-01,  6.6420e-02,  8.9407e-01,\n",
      "          -6.8424e-02,  1.1301e-01, -1.5066e-01,  3.0354e-01,  5.6559e-01,\n",
      "           4.2487e-02, -1.3749e-01, -1.1738e-01,  1.6622e-01]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[ 6.4125e-01, -1.0503e+00,  7.4947e-01,  7.7990e-01, -2.3714e+00,\n",
      "          -3.0920e-01,  2.8957e-02, -8.3487e-01,  2.6134e-01,  1.1916e+00,\n",
      "          -1.2161e+00, -4.0661e-01,  2.4000e+00,  5.2410e-01, -4.6892e-01,\n",
      "          -3.5747e-02, -2.9269e-01,  2.4786e-01, -1.1650e+00, -4.5508e-01,\n",
      "           1.3069e+00, -4.3441e-01,  2.5061e-01,  1.6546e+00,  3.4669e-01,\n",
      "          -8.9167e-01, -4.0603e-01,  1.9516e+00,  1.7265e+00, -7.7078e-01,\n",
      "           5.7360e-01,  6.0021e-01,  1.0370e+00,  2.8093e-01, -1.9677e+00,\n",
      "          -6.3411e-01, -8.2095e-01, -1.0089e+00,  5.9143e-01, -8.2273e-01,\n",
      "           1.5428e+00,  3.0995e-01,  3.3020e-01,  1.3514e-01, -9.6097e-01,\n",
      "           5.0471e-01,  4.9113e-01, -1.5198e+00, -1.7977e-01, -5.7744e-01,\n",
      "           7.3222e-01, -6.1013e-01, -1.3013e+00,  5.1149e-01,  2.1771e+00,\n",
      "           7.4074e-01, -1.6189e+00,  5.7375e-01,  2.1667e-01,  7.5468e-01,\n",
      "           4.2562e-01, -5.5993e-01, -1.0511e+00, -1.3360e+00],\n",
      "         [-9.3473e-01, -2.5446e-01,  1.6855e+00, -1.0127e+00, -1.3153e+00,\n",
      "           8.2075e-01, -9.1963e-01, -1.4203e+00, -1.6153e+00, -1.4650e-03,\n",
      "           2.6539e-01, -9.7700e-01,  9.8865e-01, -7.6631e-02,  2.6501e-01,\n",
      "           1.6164e+00,  1.2833e+00,  1.5542e+00, -3.8380e-01,  3.6466e-01,\n",
      "           3.7273e-01, -1.1914e+00,  1.6894e+00, -6.5893e-01,  1.6660e-03,\n",
      "          -1.0653e+00,  9.1842e-01,  2.0147e-01,  2.1578e+00,  2.7631e-01,\n",
      "           2.0173e-01, -6.8146e-01,  9.6816e-02, -1.9478e-01, -1.2360e+00,\n",
      "          -3.5328e-01,  3.9083e-01,  1.3020e+00,  8.6749e-01, -8.7183e-01,\n",
      "          -1.1953e+00,  9.0594e-01,  8.3812e-01,  6.7369e-01, -8.6373e-01,\n",
      "          -2.0113e+00,  9.3624e-01, -1.0633e+00,  3.4288e-01, -1.1064e+00,\n",
      "          -7.5892e-01,  1.5494e-01, -1.8510e+00,  1.7878e-01,  2.4065e+00,\n",
      "          -1.8417e-01,  3.0418e-01, -4.0552e-01,  8.1702e-01,  1.5224e+00,\n",
      "           1.1436e-01, -3.7008e-01, -3.1595e-01,  4.4741e-01]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "from nnterp.nnsight_utils import get_attention\n",
    "import torch as th\n",
    "model = StandardizedTransformer(\"gpt2\", device_map=\"auto\")\n",
    "model.tokenizer.chat_template\n",
    "with model.trace(\"a\"):\n",
    "    print(\n",
    "    get_attention(model, 0).inputs[1].keys())\n",
    "    print(get_attention(model, 0).input.shape)\n",
    "    print(th.allclose(model.layers_output[0], get_attention(model, 1).input))\n",
    "\n",
    "llama_model = StandardizedTransformer(\"Maykeye/TinyLLama-v0\", device_map=\"auto\")\n",
    "with llama_model.trace(\"a\"):\n",
    "    print(\n",
    "    get_attention(llama_model, 0).inputs[1].keys())\n",
    "    print(get_attention(llama_model, 0).input.shape)\n",
    "    layer_output = llama_model.layers_output[0]\n",
    "    attention_input = get_attention(llama_model, 1).input\n",
    "    attn_hidden_states = get_attention(llama_model, 1).inputs[1][\"hidden_states\"]\n",
    "    print(f\"hs vs input: {th.allclose(attn_hidden_states, attention_input)}\")\n",
    "    print(th.allclose(layer_output, attention_input))\n",
    "    print(layer_output)\n",
    "    print(attention_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_31106/2793749493.py\", line 2, in <module>\n    print(model.transformer.source)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 974, in __getattr__\n    raise AttributeError(f\"{self} has no attribute {name}\")\n\nAttributeError: GPT2Model(\n  (wte): Embedding(50257, 768)\n  (wpe): Embedding(1024, 768)\n  (drop): Dropout(p=0.1, inplace=False)\n  (h): ModuleList(\n    (0-11): 12 x GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n) has no attribute source",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39msource)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Suppress the ExitTracingException but let other exceptions propagate\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     21\u001b[0m     tracer\u001b[38;5;241m.\u001b[39mexecute(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     Globals\u001b[38;5;241m.\u001b[39mexit()\n",
      "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_31106/2793749493.py\", line 2, in <module>\n    print(model.transformer.source)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 974, in __getattr__\n    raise AttributeError(f\"{self} has no attribute {name}\")\n\nAttributeError: GPT2Model(\n  (wte): Embedding(50257, 768)\n  (wpe): Embedding(1024, 768)\n  (drop): Dropout(p=0.1, inplace=False)\n  (h): ModuleList(\n    (0-11): 12 x GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n) has no attribute source"
     ]
    }
   ],
   "source": [
    "with model.trace(\"hello\"):\n",
    "    print(model.transformer.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_31106/1970214495.py\", line 2, in <module>\n    print(model.transformer.h.source)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 358, in source\n    source, line_numbers, forward = inject(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/inject.py\", line 77, in convert\n    code_obj = compile(astor.to_source(tree), filename, 'exec')\n\nSyntaxError: f-string expression part cannot include a backslash (<nnsight>, line 15)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3579\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[20], line 1\u001b[0m\n    with model.trace(\"hello\"):\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m in \u001b[1;35m__exit__\u001b[0m\n    self.backend(self)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0;36m in \u001b[0;35m__call__\u001b[0;36m\n\u001b[0;31m    raise wrap_exception(e, tracer.info) from None\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<nnsight>:15\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mNNsightException\u001b[0m\u001b[0;31m:\u001b[0m f-string expression part cannot include a backslash\n"
     ]
    }
   ],
   "source": [
    "with model.trace(\"hello\"):\n",
    "    print(model.transformer.h.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_31106/2023299643.py\", line 4, in <module>\n    print(model.source)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 974, in __getattr__\n    raise AttributeError(f\"{self} has no attribute {name}\")\n\nAttributeError: GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  (generator): WrapperModule()\n) has no attribute source",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnnsight\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LanguageModel\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LanguageModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msource)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Suppress the ExitTracingException but let other exceptions propagate\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     21\u001b[0m     tracer\u001b[38;5;241m.\u001b[39mexecute(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     Globals\u001b[38;5;241m.\u001b[39mexit()\n",
      "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_31106/2023299643.py\", line 4, in <module>\n    print(model.source)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 974, in __getattr__\n    raise AttributeError(f\"{self} has no attribute {name}\")\n\nAttributeError: GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  (generator): WrapperModule()\n) has no attribute source"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"gpt2\")\n",
    "with model.trace(\"hello\"):\n",
    "    print(model.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-01 17:25:41.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m197\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available\u001b[0m\n"
     ]
    },
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_31106/3786076022.py\", line 7, in <module>\n    print(model.model.layers.source)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 358, in source\n    source, line_numbers, forward = inject(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/inject.py\", line 77, in convert\n    code_obj = compile(astor.to_source(tree), filename, 'exec')\n\nSyntaxError: f-string expression part cannot include a backslash (<nnsight>, line 15)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3579\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[24], line 6\u001b[0m\n    with model.trace([\"hello\", \"hello the fox is jumping\"]):\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m in \u001b[1;35m__exit__\u001b[0m\n    self.backend(self)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0;36m in \u001b[0;35m__call__\u001b[0;36m\n\u001b[0;31m    raise wrap_exception(e, tracer.info) from None\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<nnsight>:15\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mNNsightException\u001b[0m\u001b[0;31m:\u001b[0m f-string expression part cannot include a backslash\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "mname = \"Maykeye/TinyLLama-v0\"\n",
    "# mname = \"google/gemma-2-2b\"\n",
    "# mname = \"bigscience/bigscience-small-testing\"\n",
    "model = StandardizedTransformer(mname)\n",
    "with model.trace([\"hello\", \"hello the fox is jumping\"]):\n",
    "    print(model.model.layers.source)\n",
    "    # print(model.attentions[0].source)#.attention_interface_0.output[1])\n",
    "    # print([t.shape if t is not None else \"None\" for t in model.attentions[0].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Model forward method:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method LlamaForCausalLM.forward of LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 64, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (up_proj): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (down_proj): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((64,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((64,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((64,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=64, out_features=32000, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Forward method source code:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "    @can_return_tuple\n",
       "    @auto_docstring\n",
       "    def forward(\n",
       "        self,\n",
       "        input_ids: Optional[torch.LongTensor] = None,\n",
       "        attention_mask: Optional[torch.Tensor] = None,\n",
       "        position_ids: Optional[torch.LongTensor] = None,\n",
       "        past_key_values: Optional[Cache] = None,\n",
       "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
       "        labels: Optional[torch.LongTensor] = None,\n",
       "        use_cache: Optional[bool] = None,\n",
       "        output_attentions: Optional[bool] = None,\n",
       "        output_hidden_states: Optional[bool] = None,\n",
       "        cache_position: Optional[torch.LongTensor] = None,\n",
       "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
       "        **kwargs: Unpack[KwargsForCausalLM],\n",
       "    ) -> CausalLMOutputWithPast:\n",
       "        r\"\"\"\n",
       "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
       "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
       "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
       "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
       "\n",
       "        Example:\n",
       "\n",
       "        ```python\n",
       "        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n",
       "\n",
       "        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
       "        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
       "\n",
       "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
       "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
       "\n",
       "        >>> # Generate\n",
       "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
       "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
       "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
       "        ```\"\"\"\n",
       "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
       "        output_hidden_states = (\n",
       "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
       "        )\n",
       "\n",
       "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
       "        outputs: BaseModelOutputWithPast = self.model(\n",
       "            input_ids=input_ids,\n",
       "            attention_mask=attention_mask,\n",
       "            position_ids=position_ids,\n",
       "            past_key_values=past_key_values,\n",
       "            inputs_embeds=inputs_embeds,\n",
       "            use_cache=use_cache,\n",
       "            output_attentions=output_attentions,\n",
       "            output_hidden_states=output_hidden_states,\n",
       "            cache_position=cache_position,\n",
       "            **kwargs,\n",
       "        )\n",
       "\n",
       "        hidden_states = outputs.last_hidden_state\n",
       "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
       "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
       "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
       "\n",
       "        loss = None\n",
       "        if labels is not None:\n",
       "            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n",
       "\n",
       "        return CausalLMOutputWithPast(\n",
       "            loss=loss,\n",
       "            logits=logits,\n",
       "            past_key_values=outputs.past_key_values,\n",
       "            hidden_states=outputs.hidden_states,\n",
       "            attentions=outputs.attentions,\n",
       "        )\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the model's forward method\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(\"**Model forward method:**\"))\n",
    "display(model._model.forward)\n",
    "\n",
    "display(Markdown(\"---\"))\n",
    "\n",
    "# Get the source code if available\n",
    "try:\n",
    "    import inspect\n",
    "    source_code = inspect.getsource(model._model.forward)\n",
    "    display(Markdown(\"**Forward method source code:**\"))\n",
    "    display(Markdown(f\"```python\\n{source_code}\\n```\"))\n",
    "except Exception as e:\n",
    "    display(Markdown(f\"**Could not get source code:** {e}\"))\n",
    "    display(Markdown(\"**Forward method signature:**\"))\n",
    "    display(Markdown(f\"```python\\n{inspect.signature(model._model.forward)}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 * @classmethod\n",
      "                                                 0 def apply(cls, *args, **kwargs):\n",
      "                                                 1     def bind_default_args(func, *args, **kwargs):\n",
      " inspect_signature_0                         ->  2         signature = inspect.signature(func)\n",
      " signature_bind_0                            ->  3         bound_args = signature.bind(*args, **kwargs)\n",
      " bound_args_apply_defaults_0                 ->  4         bound_args.apply_defaults()\n",
      "                                                 5 \n",
      "                                                 6         return bound_args.args\n",
      "                                                 7 \n",
      " _is_setup_context_defined_0                 ->  8     is_setup_ctx_defined = _is_setup_context_defined(cls.setup_context)\n",
      "                                                 9     if is_setup_ctx_defined:\n",
      " bind_default_args_0                         -> 10         args = bind_default_args(cls.forward, *args, **kwargs)\n",
      "                                                11 \n",
      " torch__C__are_functorch_transforms_active_0 -> 12     if not torch._C._are_functorch_transforms_active():\n",
      "                                                13         # See NOTE: [functorch vjp and autograd interaction]\n",
      " _functorch_utils_unwrap_dead_wrappers_0     -> 14         args = _functorch.utils.unwrap_dead_wrappers(args)\n",
      " super_0                                     -> 15         return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      " apply_0                                     ->  +         ...\n",
      "                                                16 \n",
      "                                                17     if not is_setup_ctx_defined:\n",
      " RuntimeError_0                              -> 18         raise RuntimeError(\n",
      "                                                19             \"In order to use an autograd.Function with functorch transforms \"\n",
      "                                                20             \"(vmap, grad, jvp, jacrev, ...), it must override the setup_context \"\n",
      "                                                21             \"staticmethod. For more details, please see \"\n",
      "                                                22             \"https://pytorch.org/docs/main/notes/extending.func.html\"\n",
      "                                                23         )\n",
      "                                                24 \n",
      " custom_function_call_0                      -> 25     return custom_function_call(cls, *args, **kwargs)\n",
      "                                                26 \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<nnsight>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m LanguageModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaykeye/TinyLLama-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello the fox is jumping\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# print(model.model.layers[0].self_attn.source.attention_interface_0.source)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mattention_interface_0\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39m_flash_attention_forward_0\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39m_flash_attn_varlen_func_0\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mFlashAttnVarlenFunc_apply_0\u001b[38;5;241m.\u001b[39msource)\u001b[38;5;66;03m#.custom_function_call_0.source)\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'KeyError'>, KeyError('<nnsight>'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2181\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2178\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2181\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2183\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/util.py:269\u001b[0m, in \u001b[0;36mwrap_exception.<locals>.NNsightException.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExceptionWrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/util.py:189\u001b[0m, in \u001b[0;36mExceptionWrapper.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Case 1: <nnsight> - our traced code\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<nnsight\u001b[39m\u001b[38;5;124m\"\u001b[39m):                \n\u001b[0;32m--> 189\u001b[0m     fname \u001b[38;5;241m=\u001b[39m \u001b[43mfilename_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    190\u001b[0m     start_line \u001b[38;5;241m=\u001b[39m start_lines[filename]\n\u001b[1;32m    191\u001b[0m     co_name \u001b[38;5;241m=\u001b[39m co_names[filename] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__nnsight_tracing_info__\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mf_locals \u001b[38;5;28;01melse\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n",
      "\u001b[0;31mKeyError\u001b[0m: '<nnsight>'"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "import torch as th\n",
    "model = LanguageModel(\"Maykeye/TinyLLama-v0\", attn_implementation=\"flash_attention_2\", device_map=\"auto\", torch_dtype=th.float16)\n",
    "with model.trace([\"hello\", \"hello the fox is jumping\"]):\n",
    "    # print(model.model.layers[0].self_attn.source.attention_interface_0.source)\n",
    "    print(model.model.layers[0].self_attn.source.attention_interface_0.source._flash_attention_forward_0.source._flash_attn_varlen_func_0.source.FlashAttnVarlenFunc_apply_0.source)#.custom_function_call_0.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (standardized_transformer.py, line 222)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3579\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[3], line 1\u001b[0m\n    from nnterp import StandardizedTransformer\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/workspace/nnterp/nnterp/__init__.py:7\u001b[0;36m\n\u001b[0;31m    from .standardized_transformer import StandardizedTransformer, load_model\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m/workspace/nnterp/nnterp/standardized_transformer.py:222\u001b[0;36m\u001b[0m\n\u001b[0;31m    if self._mod\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "\n",
    "model = StandardizedTransformer(\"Maykeye/TinyLLama-v0\", attn_implementation=\"eager\")\n",
    "# print(model.attentions[0].source)\n",
    "# print(\"================\"*3)\n",
    "with model.trace([\"hello\", \"hello the fox is jumping\"]):\n",
    "    print(model.attentions[0].source.attention_interface_0.source.torch_nn_functional_scaled_dot_product_attention_0.source)\n",
    "    # print(model.attentions[0].source.attention_interface_0.output[1].shape)\n",
    "    # print(model.layers_output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 4,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 64,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 256,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"num_key_value_heads\": 16,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.53.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "import torch as th\n",
    "model = LanguageModel(\"Maykeye/TinyLLama-v0\", device_map=\"cuda\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__enter__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, world!\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      8\u001b[0m     layer_0_out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m start_from_2(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, world!\u001b[39m\u001b[38;5;124m\"\u001b[39m, layer_0_out) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39moutput)\n",
      "\u001b[0;31mAttributeError\u001b[0m: __enter__"
     ]
    }
   ],
   "source": [
    "from nnterp.nnsight_utils import skip_layers\n",
    "\n",
    "def start_from_2(model: LanguageModel, prompt: str, tensor):\n",
    "    with model.trace(prompt) as trace:\n",
    "        skip_layers(model, 0, 2, tensor)\n",
    "        return trace\n",
    "with model.trace(\"Hello, world!\"):\n",
    "    layer_0_out = model.model.layers[0].output[0].save()\n",
    "with start_from_2(model, \"Hello, world!\", layer_0_out) as trace:\n",
    "    print(model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnterp.nnsight_utils import skip_layers, set_layer_output, get_layer_output\n",
    "import torch as th\n",
    "with model.trace(\"Hello, world!\"):\n",
    "    out = model.lm_head.output.save()\n",
    "with model.trace(\"Hello, world!\"):\n",
    "    skip_layers(model, 1, 5)\n",
    "    skip_out = model.lm_head.output.save()\n",
    "with model.trace(\"Hello, world!\"):\n",
    "    set_layer_output(model, 5, get_layer_output(model, 0))\n",
    "    old_skip_out = model.lm_head.output.save()\n",
    "\n",
    "assert not th.allclose(out, skip_out)\n",
    "assert th.allclose(old_skip_out, skip_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First method succeeded\n"
     ]
    },
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 21, in __call__\n    tracer.execute(fn)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 331, in execute\n    self.model.interleave(interleaver, self.fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py\", line 76, in interleave\n    return super().interleave(interleaver, fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 705, in interleave\n    interleaver(fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 310, in __call__\n    fn(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 372, in __call__\n    else self._module(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/modeling_layers.py\", line 48, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 145, in apply_rotary_pos_emb\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n\nRuntimeError: The size of tensor a (16) must match the size of tensor b (4) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mskip((\u001b[38;5;28minput\u001b[39m,))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst method succeeded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minput\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mskip(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Suppress the ExitTracingException but let other exceptions propagate\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     21\u001b[0m     tracer\u001b[38;5;241m.\u001b[39mexecute(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     Globals\u001b[38;5;241m.\u001b[39mexit()\n",
      "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 21, in __call__\n    tracer.execute(fn)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 331, in execute\n    self.model.interleave(interleaver, self.fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py\", line 76, in interleave\n    return super().interleave(interleaver, fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 705, in interleave\n    interleaver(fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 310, in __call__\n    fn(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 372, in __call__\n    else self._module(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 688, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 453, in forward\n    layer_outputs = decoder_layer(\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/modeling_layers.py\", line 48, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 308, in forward\n    hidden_states, self_attn_weights = self.self_attn(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 247, in forward\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 145, in apply_rotary_pos_emb\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n\nRuntimeError: The size of tensor a (16) must match the size of tensor b (4) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"Maykeye/TinyLLama-v0\")\n",
    "with model.trace(\"hello\"):\n",
    "    input = model.model.layers[0].input\n",
    "    model.model.layers[0].skip((input,))\n",
    "print(\"First method succeeded\")\n",
    "with model.trace(\"hello\"):\n",
    "    input = model.model.layers[0].input\n",
    "    model.model.layers[0].skip(input)\n",
    "print(\"Second method succeeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn): GPT2SdpaAttention(\n",
      "          (self_attn): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Conv1D' object has no attribute 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# broken_model = LanguageModel(\"gpt2\", rename=rename)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(broken_model)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbroken_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbroken_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_broken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbroken_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Travail\\Documents\\repos\\nnterp\\.venv\\Lib\\site-packages\\nnsight\\intervention\\contexts\\interleaving.py:96\u001b[0m, in \u001b[0;36mInterleavingTracer.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Travail\\Documents\\repos\\nnterp\\.venv\\Lib\\site-packages\\nnsight\\tracing\\contexts\\tracer.py:26\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[0;32m     24\u001b[0m GlobalTracingContext\u001b[38;5;241m.\u001b[39mtry_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Travail\\Documents\\repos\\nnterp\\.venv\\Lib\\site-packages\\nnsight\\tracing\\contexts\\base.py:72\u001b[0m, in \u001b[0;36mContext.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m     69\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(graph\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], graph, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m broken_model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      5\u001b[0m     out \u001b[38;5;241m=\u001b[39m broken_model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m----> 6\u001b[0m     out_broken \u001b[38;5;241m=\u001b[39m \u001b[43mbroken_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\Travail\\Documents\\repos\\nnterp\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Conv1D' object has no attribute 'output'"
     ]
    }
   ],
   "source": [
    "rename = {\".*attn\": \"self_attn\"}\n",
    "# broken_model = LanguageModel(\"gpt2\", rename=rename)\n",
    "print(broken_model)\n",
    "with broken_model.trace(\"hello\"):\n",
    "    out = broken_model.lm_head.output.save()\n",
    "    out_broken = broken_model.transformer.h[9].self_attn.c_attn.output.save()\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(\"gpt2\")\n",
    "with model.trace(\"hello\"):\n",
    "    out_normal = model.lm_head.output.save()\n",
    "from torch.testing import assert_close\n",
    "assert_close(out_normal, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.*\\\\.attn$': 'self_attn', '.*\\\\.self_attention$': 'self_attn', '.*\\\\.attention$': 'self_attn', '.*\\\\.transformer$': 'model', '.*\\\\.gpt_neox$': 'model', '.*\\\\.h$': 'layers', '.*\\\\.final_layer_norm$': 'ln_final', '.*\\\\.ln_f$': 'ln_final', '.*\\\\.embed_out$': 'lm_head'}\n",
      "GPT2LMHeadModel(\n",
      "  (model): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "attention_names = [r\".*\\.attn$\", r\".*\\.self_attention$\", r\".*\\.attention$\"]\n",
    "model_names = [r\".*\\.transformer$\", r\".*\\.gpt_neox$\"]\n",
    "layer_names = [r\".*\\.h$\"]\n",
    "ln_names = [r\".*\\.final_layer_norm$\", r\".*\\.ln_f$\"]\n",
    "lm_head_names = [r\".*\\.embed_out$\"]\n",
    "\n",
    "\n",
    "llm_rename_dict = {}\n",
    "for name in attention_names:\n",
    "    llm_rename_dict[name] = \"self_attn\"\n",
    "for name in model_names:\n",
    "    llm_rename_dict[name] = \"model\"\n",
    "for name in layer_names:\n",
    "    llm_rename_dict[name] = \"layers\"\n",
    "for name in ln_names:\n",
    "    llm_rename_dict[name] = \"ln_final\"\n",
    "for name in lm_head_names:\n",
    "    llm_rename_dict[name] = \"lm_head\"\n",
    "\n",
    "print(llm_rename_dict)\n",
    "patched_model = LanguageModel(\"gpt2\", rename=llm_rename_dict)\n",
    "print(patched_model)\n",
    "# print(LanguageModel(\"gpt2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (generator): Generator(\n",
       "    (streamer): Streamer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LanguageModel(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Travail\\Documents\\repos\\nnterp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Travail\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  (generator): Generator(\n",
       "    (streamer): Streamer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LanguageModel(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (self_attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (self_attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nnterp import load_model\n",
    "patched_model = load_model('gpt2', use_model_renaming=True)\n",
    "patched_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (self_attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (self_attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (act): NewGELUActivation()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (self_attn): GPT2SdpaAttention(\n",
      "    (c_attn): Conv1D()\n",
      "    (c_proj): Conv1D()\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "GPT2SdpaAttention(\n",
      "  (c_attn): Conv1D()\n",
      "  (c_proj): Conv1D()\n",
      "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(patched_model.model)\n",
    "print()\n",
    "print(patched_model.model.layers[0])\n",
    "print()\n",
    "print(patched_model.model.layers[0].self_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "with patched_model.trace(\"hello\"):\n",
    "    out = patched_model.model.layers[0].output[0].save()\n",
    "with patched_model.trace(\"hello\"):\n",
    "    out_old = patched_model.transformer.h[0].output[0].save()\n",
    "torch.allclose(out, out_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'GPT2Block' object has no attribute 'output'\n"
     ]
    }
   ],
   "source": [
    "del patched_model._envoy.transformer\n",
    "with patched_model.trace(\"hello\"):\n",
    "    out = patched_model.model.layers[0].output[0].save()\n",
    "try:\n",
    "    with patched_model.trace(\"hello\"):\n",
    "        out_old = patched_model.transformer.h[0].output[0].save()\n",
    "    torch.allclose(out, out_old)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'GPT2LMHeadModel' object has no attribute 'transformer'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "'GPT2LMHeadModel' object has no attribute 'transformer'\n"
     ]
    }
   ],
   "source": [
    "del patched_model._model.transformer\n",
    "try:\n",
    "    with patched_model.trace(\"hello\"):\n",
    "        out = patched_model.model.layers[0].output[0].save()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(\"-\"*100)\n",
    "try:\n",
    "    with patched_model.trace(\"hello\"):\n",
    "        out_old = patched_model.transformer.h[0].output[0].save()\n",
    "    torch.allclose(out, out_old)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dlabscratch1/cdumas/nnterp/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('gpt2', use_model_renaming=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nnsight.envoy.Envoy"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model._envoy.transformer.h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 4096)\n",
       "    (word_embeddings_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-29): 30 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=250880, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model(\"bigscience/bloom-7b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 768)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXSdpaAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model(\"EleutherAI/pythia-160m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
