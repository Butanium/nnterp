{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.5.0.dev3'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import nnsight\n",
    "nnsight.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q nnsight flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install  git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='longrope': {'short_mscale', 'long_mscale'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='longrope': {'short_mscale', 'long_mscale'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='longrope': {'short_mscale', 'long_mscale'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'transformers_modules.microsoft.Phi-3'\n",
      "yujiepan/phi-3.5-moe-tiny-random: hf_success: True, nnsight_success: False\n",
      "yujiepan/phi-3.5-moe-tiny-random: hf_success: True, nnsight_success: False\n",
      "'Llama4Config' object has no attribute 'vocab_size'\n",
      "'Llama4Config' object has no attribute 'vocab_size'\n",
      "yujiepan/llama-4-tiny-random: hf_success: False, nnsight_success: False\n",
      "yujiepan/llama-4-tiny-random: hf_success: False, nnsight_success: False\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from nnsight import LanguageModel\n",
    "LLAMA_LIKE_MODELS = [\n",
    "    # \"sbintuitions/tiny-lm-chat\",\n",
    "    # \"Maykeye/TinyLLama-v0\", \n",
    "    # \"axolotl-ai-co/gemma-3-34M\",\n",
    "    # \"yujiepan/gemma-tiny-random\",\n",
    "    # \"yujiepan/llama-3.3-tiny-random\",\n",
    "    # \"yujiepan/llama-2-tiny-random\",\n",
    "    # \"yujiepan/mistral-tiny-random\",\n",
    "    # \"yujiepan/mixtral-8xtiny-random\",\n",
    "    # \"yujiepan/deepseek-llm-tiny-random\",\n",
    "    # \"yujiepan/phi-3-tiny-random\",\n",
    "    \"yujiepan/phi-3.5-moe-tiny-random\",\n",
    "    \"yujiepan/llama-4-tiny-random\",\n",
    "]\n",
    "for model_name in LLAMA_LIKE_MODELS:\n",
    "    try:\n",
    "        hf_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "        hf_success = True\n",
    "        LanguageModel(hf_model)\n",
    "    except Exception as e:\n",
    "        hf_success = False\n",
    "        print(e)\n",
    "    try:\n",
    "        LanguageModel(model_name)\n",
    "        nnsight_success = True\n",
    "    except Exception as e:\n",
    "        nnsight_success = False\n",
    "        print(e)\n",
    "        # raise e\n",
    "    print(f\"{model_name}: hf_success: {hf_success}, nnsight_success: {nnsight_success}\")\n",
    "    if not hf_success or not nnsight_success:\n",
    "        print(f\"{model_name}: hf_success: {hf_success}, nnsight_success: {nnsight_success}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea29d6720cc4701bf1144f4817ed411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d24b705924447dd9f3dd29e6c797383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3e6a9bef2e46efa2e4e0be2eec9b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0dc16e3b48c4998a6cad7a14437b738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80553e73de9d462caddc167da46a03e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7c2c7d48a54e93b3e46f06d2d6a5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a421083a1914a36ab37badc6544b105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-4b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping layer 0\n",
      "skipping layer 1\n",
      "skipping layer -1\n"
     ]
    }
   ],
   "source": [
    "from nnterp.nnsight_utils import get_num_layers, skip_layer, skip_layers, get_layer_output, get_layer_input\n",
    "import torch as th\n",
    "from nnterp import StandardizedTransformer\n",
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"yujiepan/phi-3-tiny-random\", device_map=\"auto\", dispatch=True)\n",
    "\n",
    "prompt = \"Hello, world!\"\n",
    "for layer in [0, 1, -1]:\n",
    "    if layer >= get_num_layers(model):\n",
    "        break\n",
    "    # Test skip_layer function (single layer skipping)\n",
    "    print(f\"skipping layer {layer}\")\n",
    "    with model.trace(prompt):\n",
    "        skip_layer(model, layer)\n",
    "        single_skip_output = model.lm_head.output.save()\n",
    "\n",
    "    # Test that skip_layer(2) is equivalent to skip_layers(2, 2)\n",
    "    with model.trace(prompt):\n",
    "        skip_layers(model, layer, layer)\n",
    "        equivalent_skip_output = model.lm_head.output.save()\n",
    "\n",
    "    assert th.allclose(\n",
    "        single_skip_output, equivalent_skip_output, atol=1e-5\n",
    "    ), \"skip_layer should be equivalent to skip_layers with same start and end layer\"\n",
    "\n",
    "# Test multiple skip_layer calls vs single skip_layers call\n",
    "for r in [1, 2, 3]:\n",
    "    if get_num_layers(model) - r < 1:\n",
    "        break\n",
    "    with model.trace(prompt):\n",
    "        for i in range(get_num_layers(model) - r):\n",
    "            skip_layer(model, i)\n",
    "        multiple_single_skips = model.lm_head.output.save()\n",
    "\n",
    "    with model.trace(prompt):\n",
    "        skip_layers(model, 0, get_num_layers(model) - 1 - r)\n",
    "        single_multiple_skip = model.lm_head.output.save()\n",
    "\n",
    "    assert th.allclose(\n",
    "        multiple_single_skips, single_multiple_skip, atol=1e-5\n",
    "    ), \"Multiple skip_layer calls should be equivalent to single skip_layers call\"\n",
    "\n",
    "# Test skip_with parameter - use layer 0 output as input to skip layer 3\n",
    "\n",
    "# Test skip_with parameter in skip_layers\n",
    "if get_num_layers(model) > 3:\n",
    "    with model.trace(prompt):\n",
    "        layer_0_output = get_layer_output(model, 0)\n",
    "        skip_layers(model, 1, 3, skip_with=layer_0_output)\n",
    "        skip_layers_with_custom_output = model.lm_head.output.save()\n",
    "\n",
    "# Test that skip_with=None (default) vs explicit layer input are equivalent\n",
    "with model.trace(prompt):\n",
    "    skip_layer(model, 0)  # skip_with=None (default)\n",
    "    default_skip_output = model.lm_head.output.save()\n",
    "\n",
    "with model.trace(prompt):\n",
    "    layer_0_input = get_layer_input(model, 0)\n",
    "    skip_layer(model, 0, skip_with=layer_0_input)\n",
    "    explicit_input_skip_output = model.lm_head.output.save()\n",
    "\n",
    "assert th.allclose(\n",
    "    default_skip_output, explicit_input_skip_output, atol=1e-5\n",
    "), \"skip_with=None should be equivalent to skip_with=get_layer_input(layer)\"\n",
    "if get_num_layers(model) > 3:\n",
    "    with model.trace(prompt):\n",
    "        layer_0_output = get_layer_output(model, 0)\n",
    "        skip_layer(model, 3, skip_with=layer_0_output)\n",
    "        skip_with_custom_output = model.lm_head.output.save()\n",
    "    # Verify that custom skip_with produces different results than default\n",
    "    assert not th.allclose(\n",
    "        skip_with_custom_output, default_skip_output\n",
    "    ), \"Using custom skip_with should produce different results than default\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-25 12:37:28 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'InterventionGraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnnsight\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VLLM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# NNsight's VLLM wrapper currently supports \"device = cuda\" and device = \"auto\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m vllm \u001b[38;5;241m=\u001b[39m VLLM(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaykeye/TinyLLama-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, dispatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# See supported models: https://docs.vllm.ai/en/v0.6.4.post1/models/supported_models.html\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VLLM\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/vllm.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WrapperModule\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixins\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RemoteableMixin\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGPUWorker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NNsightGPUWorker\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NNsightSamplingParams\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fields\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/workers/GPUWorker.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Worker\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_runners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mGPUModelRunner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NNsightGPUModelRunner\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNNsightGPUWorker\u001b[39;00m(Worker):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/model_runners/GPUModelRunner.py:32\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Patch, Patcher\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintervention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterleaver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interleaver\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NNsightSamplingMetadata\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabstract\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttentionBackend\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/sampling.py:45\u001b[0m\n\u001b[1;32m     40\u001b[0m             memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintervention_graph)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintervention_graph\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m, memo\u001b[38;5;241m=\u001b[39mmemo)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNNsightSamplingMetadata\u001b[39;00m(SamplingMetadata):\n\u001b[1;32m     47\u001b[0m     intervention_graph: Optional[InterventionGraph] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     nns_batch_groups: Optional[List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/modeling/vllm/sampling.py:47\u001b[0m, in \u001b[0;36mNNsightSamplingMetadata\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNNsightSamplingMetadata\u001b[39;00m(SamplingMetadata):\n\u001b[0;32m---> 47\u001b[0m     intervention_graph: Optional[\u001b[43mInterventionGraph\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     nns_batch_groups: Optional[List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     batch_groups: Optional[List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'InterventionGraph' is not defined"
     ]
    }
   ],
   "source": [
    "from nnsight.modeling.vllm import VLLM\n",
    "\n",
    "# NNsight's VLLM wrapper currently supports \"device = cuda\" and device = \"auto\"\n",
    "vllm = VLLM(\"Maykeye/TinyLLama-v0\", device = \"auto\", dispatch = True) # See supported models: https://docs.vllm.ai/en/v0.6.4.post1/models/supported_models.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     * def forward(\n",
      "                                     0     self,\n",
      "                                     1     hidden_states: torch.Tensor,\n",
      "                                     2     attention_mask: Optional[torch.Tensor] = None,\n",
      "                                     3     position_ids: Optional[torch.LongTensor] = None,\n",
      "                                     4     past_key_value: Optional[Cache] = None,\n",
      "                                     5     output_attentions: Optional[bool] = False,\n",
      "                                     6     use_cache: Optional[bool] = False,\n",
      "                                     7     cache_position: Optional[torch.LongTensor] = None,\n",
      "                                     8     position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
      "                                     9     **kwargs: Unpack[FlashAttentionKwargs],\n",
      "                                    10 ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
      "                                    11     residual = hidden_states\n",
      " self_input_layernorm_0          -> 12     hidden_states = self.input_layernorm(hidden_states)\n",
      "                                    13 \n",
      "                                    14     # Self Attention\n",
      " self_self_attn_0                -> 15     hidden_states, self_attn_weights = self.self_attn(\n",
      "                                    16         hidden_states=hidden_states,\n",
      "                                    17         attention_mask=attention_mask,\n",
      "                                    18         position_ids=position_ids,\n",
      "                                    19         past_key_value=past_key_value,\n",
      "                                    20         output_attentions=output_attentions,\n",
      "                                    21         use_cache=use_cache,\n",
      "                                    22         cache_position=cache_position,\n",
      "                                    23         position_embeddings=position_embeddings,\n",
      "                                    24         **kwargs,\n",
      "                                    25     )\n",
      "                                    26     hidden_states = residual + hidden_states\n",
      "                                    27 \n",
      "                                    28     # Fully Connected\n",
      "                                    29     residual = hidden_states\n",
      " self_post_attention_layernorm_0 -> 30     hidden_states = self.post_attention_layernorm(hidden_states)\n",
      " self_mlp_0                      -> 31     hidden_states = self.mlp(hidden_states)\n",
      "                                    32     hidden_states = residual + hidden_states\n",
      "                                    33 \n",
      "                                    34     outputs = (hidden_states,)\n",
      "                                    35     if output_attentions:\n",
      "                                    36         outputs += (self_attn_weights,)\n",
      "                                    37 \n",
      "                                    38     return outputs\n",
      "                                    39 \n"
     ]
    }
   ],
   "source": [
    "print(llama_model.layers[0].source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/config.json\n",
      "Model config Gemma2Config {\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/config.json\n",
      "Model config Gemma2Config {\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/tokenizer.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Instantiating Gemma2ForCausalLM model under default dtype torch.float32.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Gemma2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello, world!\"\n",
    "for layer in [0, 1, -1]:\n",
    "    if layer >= get_num_layers(model):\n",
    "        break\n",
    "    # Test skip_layer function (single layer skipping)\n",
    "    with model.trace(prompt):\n",
    "        skip_layer(model, layer)\n",
    "        single_skip_output = model.lm_head.output.save()\n",
    "\n",
    "    # Test that skip_layer(2) is equivalent to skip_layers(2, 2)\n",
    "    with model.trace(prompt):\n",
    "        skip_layers(model, layer, layer)\n",
    "        equivalent_skip_output = model.lm_head.output.save()\n",
    "\n",
    "    assert th.allclose(\n",
    "        single_skip_output, equivalent_skip_output, atol=1e-5\n",
    "    ), \"skip_layer should be equivalent to skip_layers with same start and end layer\"\n",
    "\n",
    "# Test multiple skip_layer calls vs single skip_layers call\n",
    "for r in [1, 2, 3]:\n",
    "    if get_num_layers(model) - r < 1:\n",
    "        break\n",
    "    with model.trace(prompt):\n",
    "        for i in range(get_num_layers(model) - r):\n",
    "            skip_layer(model, i)\n",
    "        multiple_single_skips = model.lm_head.output.save()\n",
    "\n",
    "    with model.trace(prompt):\n",
    "        skip_layers(model, 0, get_num_layers(model) - 1 - r)\n",
    "        single_multiple_skip = model.lm_head.output.save()\n",
    "\n",
    "    assert th.allclose(\n",
    "        multiple_single_skips, single_multiple_skip, atol=1e-5\n",
    "    ), \"Multiple skip_layer calls should be equivalent to single skip_layers call\"\n",
    "\n",
    "# Test skip_with parameter - use layer 0 output as input to skip layer 3\n",
    "\n",
    "# Test skip_with parameter in skip_layers\n",
    "if get_num_layers(model) > 3:\n",
    "    with model.trace(prompt):\n",
    "        layer_0_output = get_layer_output(model, 0)\n",
    "        skip_layers(model, 2, 4, skip_with=layer_0_output)\n",
    "        skip_layers_with_custom_output = model.lm_head.output.save()\n",
    "\n",
    "# Test that skip_with=None (default) vs explicit layer input are equivalent\n",
    "with model.trace(prompt):\n",
    "    skip_layer(model, 0)  # skip_with=None (default)\n",
    "    default_skip_output = model.lm_head.output.save()\n",
    "\n",
    "with model.trace(prompt):\n",
    "    layer_0_input = get_layer_input(model, 0)\n",
    "    skip_layer(model, 0, skip_with=layer_0_input)\n",
    "    explicit_input_skip_output = model.lm_head.output.save()\n",
    "\n",
    "assert th.allclose(\n",
    "    default_skip_output, explicit_input_skip_output, atol=1e-5\n",
    "), \"skip_with=None should be equivalent to skip_with=get_layer_input(layer)\"\n",
    "if get_num_layers(model) > 3:\n",
    "    with model.trace(prompt):\n",
    "        layer_0_output = get_layer_output(model, 0)\n",
    "        skip_layer(model, 3, skip_with=layer_0_output)\n",
    "        skip_with_custom_output = model.lm_head.output.save()\n",
    "    # Verify that custom skip_with produces different results than default\n",
    "    assert not th.allclose(\n",
    "        skip_with_custom_output, default_skip_output\n",
    "    ), \"Using custom skip_with should produce different results than default\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/config.json\n",
      "Model config Gemma2Config {\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/tokenizer.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Instantiating Gemma2ForCausalLM model under default dtype torch.float32.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch in attribute '__dict__[_attn_implementation_internal]': model.config=None, model._model.config='sdpa'\n",
      "Mismatch in attribute '__dict__[_attn_implementation_autoset]': model.config=False, model._model.config=True\n",
      "Mismatch in attribute '_attn_implementation': model.config='eager', model._model.config='sdpa'\n",
      "Mismatch in attribute '_attn_implementation_autoset': model.config=False, model._model.config=True\n",
      "Mismatch in attribute '_attn_implementation_internal': model.config=None, model._model.config='sdpa'\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"google/gemma-2-2b\", device_map=\"auto\")\n",
    "model_config = model.config\n",
    "model_model_config = model._model.config\n",
    "\n",
    "def get_attrs(obj):\n",
    "    return [attr for attr in dir(obj) if not callable(getattr(obj, attr))]\n",
    "\n",
    "attrs = set(get_attrs(model_config)) | set(get_attrs(model_model_config))\n",
    "\n",
    "for attr in sorted(attrs):\n",
    "    val1 = getattr(model_config, attr, None)\n",
    "    val2 = getattr(model_model_config, attr, None)\n",
    "    if val1 != val2:\n",
    "        if isinstance(val1, dict) and isinstance(val2, dict):\n",
    "            if val1.keys() != val2.keys():\n",
    "                print(f\"Mismatch in attribute keys '{attr}': model.config={val1.keys()}, model._model.config={val2.keys()}\")\n",
    "            else:\n",
    "                for k, v in val1.items():\n",
    "                    if v != val2[k]:\n",
    "                        print(f\"Mismatch in attribute '{attr}[{k}]': model.config={v!r}, model._model.config={val2[k]!r}\")\n",
    "        else:\n",
    "            print(f\"Mismatch in attribute '{attr}': model.config={val1!r}, model._model.config={val2!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/config.json\n",
      "Model config Gemma2Config {\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 2304,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9216,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 26,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"query_pre_attn_scalar\": 256,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "Instantiating Gemma2ForCausalLM model under default dtype torch.float32.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager\n",
      "eager\n",
      "eager\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a02a03a0f646ef8f122d46c7120646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Gemma2ForCausalLM.\n",
      "\n",
      "All the weights of Gemma2ForCausalLM were initialized from the model checkpoint at google/gemma-2-2b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Gemma2ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--gemma-2-2b/snapshots/c5ebcd40d208330abc697524c919956e692655cf/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager\n",
      "eager\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "cfg = AutoConfig.from_pretrained(\"google/gemma-2-2b\", attn_implementation=\"eager\")\n",
    "model = AutoModelForCausalLM.from_config(cfg)\n",
    "print(cfg._attn_implementation)\n",
    "print(model.config._attn_implementation)\n",
    "print(model.model.layers[0].self_attn.config._attn_implementation)\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\", config=cfg)\n",
    "model2.config\n",
    "print(model2.config._attn_implementation)\n",
    "print(model2.model.layers[0].self_attn.config._attn_implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"yujiepan/opt-tiny-2layers-random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-1): 2 x OPTDecoderLayer(\n",
       "    (self_attn): OPTAttention(\n",
       "      (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "      (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "      (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "      (out_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "    )\n",
       "    (activation_fn): ReLU()\n",
       "    (self_attn_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
       "    (final_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.decoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-1): 2 x OPTDecoderLayer(\n",
       "    (self_attn): OPTAttention(\n",
       "      (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "      (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "      (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "      (out_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "    )\n",
       "    (activation_fn): ReLU()\n",
       "    (self_attn_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
       "    (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
       "    (final_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"yujiepan/opt-tiny-2layers-random\", rename={\".decoder.layers\": \"layers\", \".foo.decoder\":\"dec\"})\n",
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.opt.modeling_opt.OPTForCausalLM"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(std._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-02 20:10:51.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 8, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 8)\n",
       "      (norm/final_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (out_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
       "          (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
       "          (norm/final_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x OPTDecoderLayer(\n",
       "        (self_attn): OPTAttention(\n",
       "          (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (out_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
       "        (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
       "        (norm/final_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=8, out_features=50272, bias=False)\n",
       "  (generator): WrapperModule()\n",
       "  (model): OPTDecoder(\n",
       "    (embed_tokens): Embedding(50272, 8, padding_idx=1)\n",
       "    (embed_positions): OPTLearnedPositionalEmbedding(2050, 8)\n",
       "    (norm/final_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x OPTDecoderLayer(\n",
       "        (self_attn): OPTAttention(\n",
       "          (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "          (out_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
       "        (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
       "        (norm/final_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-1): 2 x OPTDecoderLayer(\n",
       "      (self_attn): OPTAttention(\n",
       "        (k_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (v_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (q_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "        (out_proj): Linear(in_features=8, out_features=8, bias=True)\n",
       "      )\n",
       "      (activation_fn): ReLU()\n",
       "      (self_attn_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
       "      (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (norm/final_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "std = StandardizedTransformer(\"yujiepan/opt-tiny-2layers-random\", check_renaming=False)\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((8,), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nnterp.nnsight_utils import get_unembed_norm\n",
    "get_unembed_norm(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-02 20:36:04.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find mlp module in yujiepan/opt-tiny-2layers-random architecture. This means that it was not properly renamed.\nPlease pass the name of the mlp module to the mlp_rename argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/workspace/nnterp/nnterp/standardized_transformer.py:368\u001b[0m, in \u001b[0;36mStandardizedTransformer._check_renaming\u001b[0;34m(self, repo_id)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mget_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/workspace/nnterp/nnterp/nnsight_utils.py:134\u001b[0m, in \u001b[0;36mget_mlp\u001b[0;34m(nn_model, layer)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03mGet the MLP of a layer\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py:974\u001b[0m, in \u001b[0;36mEnvoy.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 974\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: OPTDecoderLayer(\n  (self_attn): OPTAttention(\n    (k_proj): Linear(in_features=8, out_features=8, bias=True)\n    (v_proj): Linear(in_features=8, out_features=8, bias=True)\n    (q_proj): Linear(in_features=8, out_features=8, bias=True)\n    (out_proj): Linear(in_features=8, out_features=8, bias=True)\n  )\n  (activation_fn): ReLU()\n  (self_attn_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n  (fc1): Linear(in_features=8, out_features=32, bias=True)\n  (fc2): Linear(in_features=32, out_features=8, bias=True)\n  (norm/final_layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n) has no attribute mlp",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnnterp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardizedTransformer\n\u001b[0;32m----> 2\u001b[0m \u001b[43mStandardizedTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myujiepan/opt-tiny-2layers-random\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_renaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/nnterp/nnterp/standardized_transformer.py:247\u001b[0m, in \u001b[0;36mStandardizedTransformer.__init__\u001b[0;34m(self, repo_id, trust_remote_code, attn_rename, mlp_rename, ln_final_rename, lm_head_rename, model_rename, layers_rename, check_renaming, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    231\u001b[0m     repo_id,\n\u001b[1;32m    232\u001b[0m     attn_implementation\u001b[38;5;241m=\u001b[39mimpl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    244\u001b[0m )\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_renaming:\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_renaming\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m=\u001b[39m get_num_layers(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# Create accessor instances\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/nnterp/nnterp/standardized_transformer.py:370\u001b[0m, in \u001b[0;36mStandardizedTransformer._check_renaming\u001b[0;34m(self, repo_id)\u001b[0m\n\u001b[1;32m    368\u001b[0m     _ \u001b[38;5;241m=\u001b[39m get_mlp(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find mlp module in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m architecture. This means that it was not properly renamed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass the name of the mlp module to the mlp_rename argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find mlp module in yujiepan/opt-tiny-2layers-random architecture. This means that it was not properly renamed.\nPlease pass the name of the mlp module to the mlp_rename argument."
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "StandardizedTransformer(\"yujiepan/opt-tiny-2layers-random\", check_renaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-02 20:34:12.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "model = StandardizedTransformer(\"gpt2\", check_renaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nnsight.intervention.envoy.Envoy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-02 20:35:26.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 64])\n",
      "torch.Size([1, 2, 64])\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "from nnterp.nnsight_utils import get_attention\n",
    "import torch as th\n",
    "model = StandardizedTransformer(\"Maykeye/TinyLLama-v0\", device_map=\"auto\")\n",
    "with model.trace(\"a\"):\n",
    "    print(model.attentions[0].input.shape)\n",
    "    print(model.mlps[0].input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['past_key_value', 'cache_position', 'attention_mask', 'head_mask', 'use_cache', 'output_attentions'])\n",
      "torch.Size([1, 1, 768])\n",
      "False\n",
      "dict_keys(['hidden_states', 'attention_mask', 'position_ids', 'past_key_value', 'output_attentions', 'use_cache', 'cache_position', 'position_embeddings'])\n",
      "torch.Size([1, 2, 64])\n",
      "hs vs input: True\n",
      "False\n",
      "tensor([[[ 3.4593e-01, -5.6658e-01,  4.0431e-01,  4.2073e-01, -1.2793e+00,\n",
      "          -1.6680e-01,  1.5621e-02, -4.5038e-01,  1.4098e-01,  6.4284e-01,\n",
      "          -6.5602e-01, -2.1935e-01,  1.2947e+00,  2.8273e-01, -2.5297e-01,\n",
      "          -1.9284e-02, -1.5790e-01,  1.3371e-01, -6.2846e-01, -2.4550e-01,\n",
      "           7.0501e-01, -2.3435e-01,  1.3520e-01,  8.9260e-01,  1.8702e-01,\n",
      "          -4.8103e-01, -2.1904e-01,  1.0528e+00,  9.3141e-01, -4.1581e-01,\n",
      "           3.0944e-01,  3.2379e-01,  5.5943e-01,  1.5155e-01, -1.0615e+00,\n",
      "          -3.4208e-01, -4.4287e-01, -5.4429e-01,  3.1906e-01, -4.4383e-01,\n",
      "           8.3231e-01,  1.6720e-01,  1.7813e-01,  7.2903e-02, -5.1841e-01,\n",
      "           2.7227e-01,  2.6494e-01, -8.1989e-01, -9.6978e-02, -3.1151e-01,\n",
      "           3.9501e-01, -3.2915e-01, -7.0201e-01,  2.7593e-01,  1.1744e+00,\n",
      "           3.9960e-01, -8.7335e-01,  3.0952e-01,  1.1688e-01,  4.0712e-01,\n",
      "           2.2961e-01, -3.0206e-01, -5.6703e-01, -7.2074e-01],\n",
      "         [-3.4727e-01, -9.4535e-02,  6.2619e-01, -3.7625e-01, -4.8867e-01,\n",
      "           3.0492e-01, -3.4166e-01, -5.2766e-01, -6.0010e-01, -5.4429e-04,\n",
      "           9.8598e-02, -3.6297e-01,  3.6730e-01, -2.8470e-02,  9.8457e-02,\n",
      "           6.0052e-01,  4.7675e-01,  5.7742e-01, -1.4259e-01,  1.3548e-01,\n",
      "           1.3848e-01, -4.4263e-01,  6.2765e-01, -2.4480e-01,  6.1896e-04,\n",
      "          -3.9577e-01,  3.4121e-01,  7.4848e-02,  8.0165e-01,  1.0266e-01,\n",
      "           7.4946e-02, -2.5318e-01,  3.5969e-02, -7.2365e-02, -4.5921e-01,\n",
      "          -1.3125e-01,  1.4520e-01,  4.8371e-01,  3.2229e-01, -3.2390e-01,\n",
      "          -4.4408e-01,  3.3657e-01,  3.1138e-01,  2.5029e-01, -3.2089e-01,\n",
      "          -7.4725e-01,  3.4783e-01, -3.9503e-01,  1.2739e-01, -4.1104e-01,\n",
      "          -2.8195e-01,  5.7564e-02, -6.8767e-01,  6.6420e-02,  8.9407e-01,\n",
      "          -6.8424e-02,  1.1301e-01, -1.5066e-01,  3.0354e-01,  5.6559e-01,\n",
      "           4.2487e-02, -1.3749e-01, -1.1738e-01,  1.6622e-01]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[ 6.4125e-01, -1.0503e+00,  7.4947e-01,  7.7990e-01, -2.3714e+00,\n",
      "          -3.0920e-01,  2.8957e-02, -8.3487e-01,  2.6134e-01,  1.1916e+00,\n",
      "          -1.2161e+00, -4.0661e-01,  2.4000e+00,  5.2410e-01, -4.6892e-01,\n",
      "          -3.5747e-02, -2.9269e-01,  2.4786e-01, -1.1650e+00, -4.5508e-01,\n",
      "           1.3069e+00, -4.3441e-01,  2.5061e-01,  1.6546e+00,  3.4669e-01,\n",
      "          -8.9167e-01, -4.0603e-01,  1.9516e+00,  1.7265e+00, -7.7078e-01,\n",
      "           5.7360e-01,  6.0021e-01,  1.0370e+00,  2.8093e-01, -1.9677e+00,\n",
      "          -6.3411e-01, -8.2095e-01, -1.0089e+00,  5.9143e-01, -8.2273e-01,\n",
      "           1.5428e+00,  3.0995e-01,  3.3020e-01,  1.3514e-01, -9.6097e-01,\n",
      "           5.0471e-01,  4.9113e-01, -1.5198e+00, -1.7977e-01, -5.7744e-01,\n",
      "           7.3222e-01, -6.1013e-01, -1.3013e+00,  5.1149e-01,  2.1771e+00,\n",
      "           7.4074e-01, -1.6189e+00,  5.7375e-01,  2.1667e-01,  7.5468e-01,\n",
      "           4.2562e-01, -5.5993e-01, -1.0511e+00, -1.3360e+00],\n",
      "         [-9.3473e-01, -2.5446e-01,  1.6855e+00, -1.0127e+00, -1.3153e+00,\n",
      "           8.2075e-01, -9.1963e-01, -1.4203e+00, -1.6153e+00, -1.4650e-03,\n",
      "           2.6539e-01, -9.7700e-01,  9.8865e-01, -7.6631e-02,  2.6501e-01,\n",
      "           1.6164e+00,  1.2833e+00,  1.5542e+00, -3.8380e-01,  3.6466e-01,\n",
      "           3.7273e-01, -1.1914e+00,  1.6894e+00, -6.5893e-01,  1.6660e-03,\n",
      "          -1.0653e+00,  9.1842e-01,  2.0147e-01,  2.1578e+00,  2.7631e-01,\n",
      "           2.0173e-01, -6.8146e-01,  9.6816e-02, -1.9478e-01, -1.2360e+00,\n",
      "          -3.5328e-01,  3.9083e-01,  1.3020e+00,  8.6749e-01, -8.7183e-01,\n",
      "          -1.1953e+00,  9.0594e-01,  8.3812e-01,  6.7369e-01, -8.6373e-01,\n",
      "          -2.0113e+00,  9.3624e-01, -1.0633e+00,  3.4288e-01, -1.1064e+00,\n",
      "          -7.5892e-01,  1.5494e-01, -1.8510e+00,  1.7878e-01,  2.4065e+00,\n",
      "          -1.8417e-01,  3.0418e-01, -4.0552e-01,  8.1702e-01,  1.5224e+00,\n",
      "           1.1436e-01, -3.7008e-01, -3.1595e-01,  4.4741e-01]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "from nnterp.nnsight_utils import get_attention\n",
    "import torch as th\n",
    "model = StandardizedTransformer(\"gpt2\", device_map=\"auto\")\n",
    "model.tokenizer.chat_template\n",
    "with model.trace(\"a\"):\n",
    "    print(\n",
    "    get_attention(model, 0).inputs[1].keys())\n",
    "    print(get_attention(model, 0).input.shape)\n",
    "    print(th.allclose(model.layers_output[0], get_attention(model, 1).input))\n",
    "\n",
    "llama_model = StandardizedTransformer(\"Maykeye/TinyLLama-v0\", device_map=\"auto\")\n",
    "with llama_model.trace(\"a\"):\n",
    "    print(\n",
    "    get_attention(llama_model, 0).inputs[1].keys())\n",
    "    print(get_attention(llama_model, 0).input.shape)\n",
    "    layer_output = llama_model.layers_output[0]\n",
    "    attention_input = get_attention(llama_model, 1).input\n",
    "    attn_hidden_states = get_attention(llama_model, 1).inputs[1][\"hidden_states\"]\n",
    "    print(f\"hs vs input: {th.allclose(attn_hidden_states, attention_input)}\")\n",
    "    print(th.allclose(layer_output, attention_input))\n",
    "    print(layer_output)\n",
    "    print(attention_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_31106/2023299643.py\", line 4, in <module>\n    print(model.source)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 974, in __getattr__\n    raise AttributeError(f\"{self} has no attribute {name}\")\n\nAttributeError: GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  (generator): WrapperModule()\n) has no attribute source",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnnsight\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LanguageModel\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LanguageModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msource)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Suppress the ExitTracingException but let other exceptions propagate\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     21\u001b[0m     tracer\u001b[38;5;241m.\u001b[39mexecute(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     Globals\u001b[38;5;241m.\u001b[39mexit()\n",
      "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_31106/2023299643.py\", line 4, in <module>\n    print(model.source)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 974, in __getattr__\n    raise AttributeError(f\"{self} has no attribute {name}\")\n\nAttributeError: GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  (generator): WrapperModule()\n) has no attribute source"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"gpt2\")\n",
    "with model.trace(\"hello\"):\n",
    "    print(model.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  * def forward(self, x):\n",
      "                  0     foo = x + 2  # can't access from source\n",
      " add_0        ->  1     foo_func = add(x, 2)  # can access as go through the artificial `add` function\n",
      " self_layer_0 ->  2     foo_trap = self.layer(x) + 1  # accesses `self.layer(x)` but not `self.layer(x) + 1`\n",
      " self_layer_1 ->  3     return self.layer(x)\n",
      "                  4 \n"
     ]
    }
   ],
   "source": [
    "from nnsight import NNsight\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        foo = x + 2  # can't access from source\n",
    "        foo_func = add(x, 2)  # can access as go through the artificial `add` function\n",
    "        foo_trap = self.layer(x) + 1  # accesses `self.layer(x)` but not `self.layer(x) + 1`\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "model = NNsight(MyModel())\n",
    "with model.trace(th.randn(10, 10)):\n",
    "    print(model.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bloom.modeling_bloom.BloomForCausalLM"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-02 14:46:41.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m197\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         * def forward(\n",
      "                                         0     self,\n",
      "                                         1     input_ids: Optional[torch.LongTensor] = None,\n",
      "                                         2     attention_mask: Optional[torch.Tensor] = None,\n",
      "                                         3     position_ids: Optional[torch.LongTensor] = None,\n",
      "                                         4     past_key_values: Optional[Cache] = None,\n",
      "                                         5     inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "                                         6     use_cache: Optional[bool] = None,\n",
      "                                         7     output_attentions: Optional[bool] = None,\n",
      "                                         8     output_hidden_states: Optional[bool] = None,\n",
      "                                         9     cache_position: Optional[torch.LongTensor] = None,\n",
      "                                        10     **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n",
      "                                        11 ) -> BaseModelOutputWithPast:\n",
      "                                        12     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "                                        13     output_hidden_states = (\n",
      "                                        14         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "                                        15     )\n",
      "                                        16     use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "                                        17 \n",
      "                                        18     if (input_ids is None) ^ (inputs_embeds is not None):\n",
      " ValueError_0                        -> 19         raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
      "                                        20 \n",
      "                                        21     if self.gradient_checkpointing and self.training and use_cache:\n",
      " logger_warning_once_0               -> 22         logger.warning_once(\n",
      "                                        23             \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n",
      "                                        24         )\n",
      "                                        25         use_cache = False\n",
      "                                        26 \n",
      "                                        27     if inputs_embeds is None:\n",
      " self_embed_tokens_0                 -> 28         inputs_embeds = self.embed_tokens(input_ids)\n",
      "                                        29 \n",
      "                                        30     if use_cache and past_key_values is None and not self.training:\n",
      " DynamicCache_0                      -> 31         past_key_values = DynamicCache()\n",
      "                                        32 \n",
      "                                        33     if cache_position is None:\n",
      " past_key_values_get_seq_length_0    -> 34         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
      " torch_arange_0                      -> 35         cache_position = torch.arange(\n",
      "                                        36             past_seen_tokens,\n",
      "                                        37             past_seen_tokens + inputs_embeds.shape[1],\n",
      "                                        38             device=inputs_embeds.device,\n",
      "                                        39         )\n",
      "                                        40 \n",
      "                                        41     if position_ids is None:\n",
      " cache_position_unsqueeze_0          -> 42         position_ids = cache_position.unsqueeze(0)\n",
      "                                        43 \n",
      "                                        44     # It may already have been prepared by e.g. `generate`\n",
      " isinstance_0                        -> 45     if not isinstance(causal_mask_mapping := attention_mask, dict):\n",
      "                                        46         # Prepare mask arguments\n",
      "                                        47         mask_kwargs = {\n",
      "                                        48             \"config\": self.config,\n",
      "                                        49             \"input_embeds\": inputs_embeds,\n",
      "                                        50             \"attention_mask\": attention_mask,\n",
      "                                        51             \"cache_position\": cache_position,\n",
      "                                        52             \"past_key_values\": past_key_values,\n",
      "                                        53         }\n",
      "                                        54         # Create the masks\n",
      "                                        55         causal_mask_mapping = {\n",
      " create_causal_mask_0                -> 56             \"full_attention\": create_causal_mask(**mask_kwargs),\n",
      " create_sliding_window_causal_mask_0 -> 57             \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n",
      "                                        58         }\n",
      "                                        59 \n",
      "                                        60     # embed positions\n",
      "                                        61     hidden_states = inputs_embeds\n",
      "                                        62 \n",
      "                                        63     # create position embeddings to be shared across the decoder layers\n",
      " self_rotary_emb_0                   -> 64     position_embeddings_global = self.rotary_emb(hidden_states, position_ids)\n",
      " self_rotary_emb_local_0             -> 65     position_embeddings_local = self.rotary_emb_local(hidden_states, position_ids)\n",
      "                                        66 \n",
      "                                        67     # decoder layers\n",
      "                                        68     all_hidden_states = () if output_hidden_states else None\n",
      "                                        69     all_self_attns = () if output_attentions else None\n",
      "                                        70 \n",
      "                                        71     for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
      "                                        72         if output_hidden_states:\n",
      "                                        73             all_hidden_states += (hidden_states,)\n",
      "                                        74 \n",
      " decoder_layer_0                     -> 75         layer_outputs = decoder_layer(\n",
      "                                        76             hidden_states,\n",
      "                                        77             position_embeddings_global=position_embeddings_global,\n",
      "                                        78             position_embeddings_local=position_embeddings_local,\n",
      "                                        79             attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n",
      "                                        80             position_ids=position_ids,\n",
      "                                        81             past_key_value=past_key_values,\n",
      "                                        82             output_attentions=output_attentions,\n",
      "                                        83             use_cache=use_cache,\n",
      "                                        84             cache_position=cache_position,\n",
      "                                        85             **flash_attn_kwargs,\n",
      "                                        86         )\n",
      "                                        87 \n",
      "                                        88         hidden_states = layer_outputs[0]\n",
      "                                        89 \n",
      "                                        90         if output_attentions:\n",
      "                                        91             all_self_attns += (layer_outputs[1],)\n",
      "                                        92 \n",
      " self_norm_0                         -> 93     hidden_states = self.norm(hidden_states)\n",
      "                                        94 \n",
      "                                        95     if output_hidden_states:\n",
      "                                        96         all_hidden_states += (hidden_states,)\n",
      "                                        97 \n",
      " BaseModelOutputWithPast_0           -> 98     return BaseModelOutputWithPast(\n",
      "                                        99         last_hidden_state=hidden_states,\n",
      "                                       100         past_key_values=past_key_values,\n",
      "                                       101         hidden_states=all_hidden_states,\n",
      "                                       102         attentions=all_self_attns,\n",
      "                                       103     )\n",
      "                                       104 \n"
     ]
    }
   ],
   "source": [
    "print(StandardizedTransformer(\"axolotl-ai-co/gemma-3-34M\").model.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_7654/3324543563.py\", line 5, in <module>\n    layer_out = model.transformer.h[0].output.save()\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 140, in output\n    return self._interleaver.current.request(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 801, in request\n    value = self.send(Events.VALUE, requester)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 785, in send\n    raise response\n\nOutOfOrderError: Value was missed for model.transformer.h.0.output.i0. Did you call an Envoy out of order?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnnsight\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LanguageModel\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LanguageModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m      5\u001b[0m     layer_out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Suppress the ExitTracingException but let other exceptions propagate\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     21\u001b[0m     tracer\u001b[38;5;241m.\u001b[39mexecute(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     Globals\u001b[38;5;241m.\u001b[39mexit()\n",
      "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_7654/3324543563.py\", line 5, in <module>\n    layer_out = model.transformer.h[0].output.save()\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 140, in output\n    return self._interleaver.current.request(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 801, in request\n    value = self.send(Events.VALUE, requester)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 785, in send\n    raise response\n\nOutOfOrderError: Value was missed for model.transformer.h.0.output.i0. Did you call an Envoy out of order?"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"gpt2\")\n",
    "with model.trace(\"a\"):\n",
    "    logits = model.output.logits.save()\n",
    "    layer_out = model.transformer.h[0].output.save()\n",
    "print(logits.shape)\n",
    "print(layer_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 4,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 64,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 256,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"num_key_value_heads\": 16,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.53.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attn_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mattention_interface_0\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(attn_out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mattn_out\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attn_out' is not defined"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "if \"attn_out\" in globals():\n",
    "    del attn_out\n",
    "model = LanguageModel(\"Maykeye/TinyLLama-v0\")\n",
    "with model.trace([\"hello\", \"hello the fox is jumping\"]):\n",
    "    logits = model.output.logits.save()\n",
    "    attn_out = model.model.layers[0].self_attn.source.attention_interface_0.output[0].save()\n",
    "    print(attn_out.shape)\n",
    "\n",
    "print(attn_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-03 11:42:23.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "mname = \"Maykeye/TinyLLama-v0\"\n",
    "model = StandardizedTransformer(mname, )\n",
    "with model.trace([\"hello\", \"hello the fox is jumping\"]):\n",
    "    # print(model.model.layers[0].source)\n",
    "    logits = model.output.logits.save()\n",
    "    attn_probs = model.attentions[0].source.attention_interface_0.source.nn_functional_dropout_0.output.save()\n",
    "    #.attention_interface_0.output[1])\n",
    "    # print([t.shape if t is not None else \"None\" for t in model.attentions[0].output])\n",
    "    print(attn_probs.shape)\n",
    "\n",
    "print(attn_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-02 13:52:49.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m197\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              * def forward(\n",
      "                              0     self,\n",
      "                              1     hidden_states: torch.Tensor,\n",
      "                              2     residual: torch.Tensor,\n",
      "                              3     alibi: torch.Tensor,\n",
      "                              4     attention_mask: torch.Tensor,\n",
      "                              5     layer_past: Optional[Cache] = None,\n",
      "                              6     head_mask: Optional[torch.Tensor] = None,\n",
      "                              7     use_cache: bool = False,\n",
      "                              8     output_attentions: bool = False,\n",
      "                              9     cache_position: Optional[torch.LongTensor] = None,\n",
      "                             10 ):\n",
      "                             11     batch_size, q_length, _ = hidden_states.shape\n",
      " self_query_key_value_0   -> 12     fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n",
      "                             13     # 3 x [batch_size, num_heads, seq_length, head_dim]\n",
      " self__reshape_0          -> 14     query_layer, key_layer, value_layer = self._reshape(fused_qkv)\n",
      "                             15 \n",
      "                             16     if layer_past is not None:\n",
      "                             17         cache_kwargs = {\"cache_position\": cache_position}\n",
      " layer_past_update_0      -> 18         key_layer, value_layer = layer_past.update(key_layer, value_layer, self.layer_idx, cache_kwargs)\n",
      "                             19 \n",
      "                             20     # reshape qkv for further computations\n",
      " query_layer_reshape_0    -> 21     query_layer = query_layer.reshape(batch_size * self.num_heads, -1, self.head_dim)\n",
      " key_layer_reshape_0      -> 22     key_layer = key_layer.reshape(batch_size * self.num_heads, -1, self.head_dim).transpose(-1, -2)\n",
      " transpose_0              ->  +     ...\n",
      " value_layer_reshape_0    -> 23     value_layer = value_layer.reshape(batch_size * self.num_heads, -1, self.head_dim)\n",
      "                             24 \n",
      "                             25     # [batch_size * num_heads, q_length, kv_length]\n",
      " alibi_baddbmm_0          -> 26     attention_scores = alibi.baddbmm(\n",
      "                             27         batch1=query_layer,\n",
      "                             28         batch2=key_layer,\n",
      "                             29         beta=self.beta,\n",
      "                             30         alpha=self.inv_norm_factor,\n",
      "                             31     )\n",
      "                             32 \n",
      "                             33     # change view to [batch_size, num_heads, q_length, kv_length]\n",
      " attention_scores_view_0  -> 34     attn_weights = attention_scores.view(batch_size, self.num_heads, q_length, -1)\n",
      "                             35     if attention_mask is not None:  # no matter the length, we just slice it\n",
      "                             36         causal_mask = attention_mask[:, :, :, : key_layer.shape[-1]]\n",
      "                             37         attn_weights = attn_weights + causal_mask\n",
      "                             38 \n",
      "                             39     # cast attention scores to fp32, compute scaled softmax and cast back to initial dtype\n",
      " F_softmax_0              -> 40     attention_probs = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_layer.dtype)\n",
      " to_0                     ->  +     ...\n",
      "                             41 \n",
      "                             42     # [batch_size, num_heads, q_length, kv_length]\n",
      " self_attention_dropout_0 -> 43     attention_probs = self.attention_dropout(attention_probs)\n",
      "                             44 \n",
      "                             45     if head_mask is not None:\n",
      "                             46         attention_probs = attention_probs * head_mask\n",
      "                             47 \n",
      "                             48     # change view [batch_size x num_heads, q_length, kv_length]\n",
      " attention_probs_view_0   -> 49     attention_probs_reshaped = attention_probs.view(batch_size * self.num_heads, q_length, -1)\n",
      "                             50 \n",
      "                             51     # matmul: [batch_size * num_heads, q_length, head_dim]\n",
      " torch_bmm_0              -> 52     context_layer = torch.bmm(attention_probs_reshaped, value_layer)\n",
      "                             53 \n",
      "                             54     # change view [batch_size, q_length, num_heads * head_dim]\n",
      " self__merge_heads_0      -> 55     context_layer = self._merge_heads(context_layer)\n",
      "                             56 \n",
      "                             57     # aggregate results across tp ranks. See here: https://github.com/pytorch/pytorch/issues/76232\n",
      "                             58     if self.pretraining_tp > 1 and self.slow_but_exact:\n",
      "                             59         slices = self.hidden_size / self.pretraining_tp\n",
      " torch_zeros_like_0       -> 60         output_tensor = torch.zeros_like(context_layer)\n",
      " range_0                  -> 61         for i in range(self.pretraining_tp):\n",
      " F_linear_0               -> 62             output_tensor = output_tensor + F.linear(\n",
      " int_0                    -> 63                 context_layer[:, :, int(i * slices) : int((i + 1) * slices)],\n",
      " int_1                    ->  +                 ...\n",
      " int_2                    -> 64                 self.dense.weight[:, int(i * slices) : int((i + 1) * slices)],\n",
      " int_3                    ->  +                 ...\n",
      "                             65             )\n",
      "                             66     else:\n",
      " self_dense_0             -> 67         output_tensor = self.dense(context_layer)\n",
      "                             68 \n",
      " dropout_add_0            -> 69     output_tensor = dropout_add(output_tensor, residual, self.hidden_dropout, self.training)\n",
      "                             70 \n",
      "                             71     outputs = (output_tensor, layer_past)\n",
      "                             72     if output_attentions:\n",
      "                             73         outputs += (attention_probs,)\n",
      "                             74 \n",
      "                             75     return outputs\n",
      "                             76 \n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "mname = \"Maykeye/TinyLLama-v0\"\n",
    "# mname = \"google/gemma-2-2b\"\n",
    "mname = \"bigscience/bigscience-small-testing\"\n",
    "model = StandardizedTransformer(mname, )\n",
    "with model.trace([\"hello\", \"hello the fox is jumping\"], output_attentions=True):\n",
    "    # print(model.model.layers[0].source)\n",
    "    print(model.attentions[0].source)#.attention_interface_0.output[1])\n",
    "    # print([t.shape if t is not None else \"None\" for t in model.attentions[0].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeTensor(..., device='meta', size=(2, 16, 6, 6), dtype=torch.bfloat16,\n",
      "           grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "\n",
    "model = StandardizedTransformer(\"Maykeye/TinyLLama-v0\", attn_implementation=\"eager\")\n",
    "# print(model.attentions[0].source)\n",
    "# print(\"================\"*3)\n",
    "with model.scan([\"hello\", \"hello the fox is jumping\"]):\n",
    "    print(model.attentions[0].source.attention_interface_0.source.nn_functional_dropout_0.output)\n",
    "    # print(model.attentions[0].source.attention_interface_0.output[1].shape)\n",
    "    # print(model.layers_output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<nnsight>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LanguageModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m, attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mattention_interface_0\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mmodule_attn_dropout_0\u001b[38;5;241m.\u001b[39moutput)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'KeyError'>, KeyError('<nnsight>'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2181\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2178\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2181\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2183\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/util.py:269\u001b[0m, in \u001b[0;36mwrap_exception.<locals>.NNsightException.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExceptionWrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/util.py:189\u001b[0m, in \u001b[0;36mExceptionWrapper.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Case 1: <nnsight> - our traced code\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<nnsight\u001b[39m\u001b[38;5;124m\"\u001b[39m):                \n\u001b[0;32m--> 189\u001b[0m     fname \u001b[38;5;241m=\u001b[39m \u001b[43mfilename_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    190\u001b[0m     start_line \u001b[38;5;241m=\u001b[39m start_lines[filename]\n\u001b[1;32m    191\u001b[0m     co_name \u001b[38;5;241m=\u001b[39m co_names[filename] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__nnsight_tracing_info__\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mf_locals \u001b[38;5;28;01melse\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n",
      "\u001b[0;31mKeyError\u001b[0m: '<nnsight>'"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"gpt2\", attn_implementation=\"eager\")\n",
    "with model.trace(\"a\"):\n",
    "    print(model.transformer.h[0].attn.source.attention_interface_0.source.module_attn_dropout_0.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not int or float\n"
     ]
    }
   ],
   "source": [
    "if isinstance(\"a\", (int, float)):\n",
    "    print(\"int or float\")\n",
    "else:\n",
    "    print(\"not int or float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0702 22:53:54.026000 172684 torch/fx/experimental/symbolic_shapes.py:6679] failed during evaluate_expr(u0, hint=None, size_oblivious=False, forcing_spec=False\n",
      "E0702 22:53:54.027000 172684 torch/fx/experimental/recording.py:299] failed while running evaluate_expr(*(u0, None, False, False), **{})\n"
     ]
    },
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 21, in __call__\n    tracer.execute(fn)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 487, in execute\n    super().execute(fn)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 331, in execute\n    self.model.interleave(interleaver, self.fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py\", line 76, in interleave\n    return super().interleave(interleaver, fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 705, in interleave\n    interleaver(fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 312, in __call__\n    fn(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 372, in __call__\n    else self._module(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n    return inner()\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 943, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\", line 747, in forward\n    outputs: MoeModelOutputWithPast = self.model(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n    return inner()\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 943, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\", line 539, in forward\n    layer_outputs = decoder_layer(\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/modeling_layers.py\", line 83, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n    return inner()\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\", line 365, in forward\n    hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n    return inner()\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\", line 127, in forward\n    for expert_idx in expert_hitted:\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_tensor.py\", line 1195, in __iter__\n    return iter(self.unbind(0))\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 2770, in __torch_function__\n    return func(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/utils/_stats.py\", line 27, in wrapper\n    return fn(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1282, in __torch_dispatch__\n    return self.dispatch(func, types, args, kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1823, in dispatch\n    return self._cached_dispatch_impl(func, types, args, kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1393, in _cached_dispatch_impl\n    output = self._dispatch_impl(func, types, args, kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 2333, in _dispatch_impl\n    decomposition_table[func](*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_refs/__init__.py\", line 4002, in unbind\n    torch.squeeze(s, dim) for s in torch.tensor_split(t, t.shape[dim], dim)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/utils/_stats.py\", line 27, in wrapper\n    return fn(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1282, in __torch_dispatch__\n    return self.dispatch(func, types, args, kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1823, in dispatch\n    return self._cached_dispatch_impl(func, types, args, kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1393, in _cached_dispatch_impl\n    output = self._dispatch_impl(func, types, args, kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 2338, in _dispatch_impl\n    r = func.decompose(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_ops.py\", line 799, in decompose\n    return self._op_dk(dk, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py\", line 516, in guard_int\n    r = self.evaluate()\n  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py\", line 510, in evaluate\n    return self.shape_env.evaluate_sym_node(self, size_oblivious)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6655, in evaluate_sym_node\n    return self.evaluate_expr(\n  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/recording.py\", line 263, in wrapper\n    return retlog(fn(*args, **kwargs))\n  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6671, in evaluate_expr\n    return self._evaluate_expr(\n  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6894, in _evaluate_expr\n    raise self._make_data_dependent_error(\n\nGuardOnDataDependentSymNode: Could not extract specialized integer from data-dependent expression u0 (unhinted: u0).  (Size-like symbols: u0)\n\nCaused by: (_ops.py:799 in decompose)\nFor more information, run with TORCH_LOGS=\"dynamic\"\nFor extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\"\nIf you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\nFor more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n\nFor C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnnsight\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LanguageModel\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LanguageModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myujiepan/mixtral-8xtiny-random\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mscan(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Suppress the ExitTracingException but let other exceptions propagate\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     21\u001b[0m     tracer\u001b[38;5;241m.\u001b[39mexecute(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     Globals\u001b[38;5;241m.\u001b[39mexit()\n",
      "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 21, in __call__\n    tracer.execute(fn)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 487, in execute\n    super().execute(fn)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 331, in execute\n    self.model.interleave(interleaver, self.fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py\", line 76, in interleave\n    return super().interleave(interleaver, fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 705, in interleave\n    interleaver(fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 312, in __call__\n    fn(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 372, in __call__\n    else self._module(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n    return inner()\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 943, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\", line 747, in forward\n    outputs: MoeModelOutputWithPast = self.model(\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n    return inner()\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 943, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\", line 539, in forward\n    layer_outputs = decoder_layer(\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/modeling_layers.py\", line 83, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n    return inner()\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\", line 365, in forward\n    hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n    value = fn(module, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n    return inner()\n  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\", line 127, in forward\n    for expert_idx in expert_hitted:\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_tensor.py\", line 1195, in __iter__\n    return iter(self.unbind(0))\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 2770, in __torch_function__\n    return func(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/utils/_stats.py\", line 27, in wrapper\n    return fn(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1282, in __torch_dispatch__\n    return self.dispatch(func, types, args, kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1823, in dispatch\n    return self._cached_dispatch_impl(func, types, args, kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1393, in _cached_dispatch_impl\n    output = self._dispatch_impl(func, types, args, kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 2333, in _dispatch_impl\n    decomposition_table[func](*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_refs/__init__.py\", line 4002, in unbind\n    torch.squeeze(s, dim) for s in torch.tensor_split(t, t.shape[dim], dim)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/utils/_stats.py\", line 27, in wrapper\n    return fn(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1282, in __torch_dispatch__\n    return self.dispatch(func, types, args, kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1823, in dispatch\n    return self._cached_dispatch_impl(func, types, args, kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1393, in _cached_dispatch_impl\n    output = self._dispatch_impl(func, types, args, kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 2338, in _dispatch_impl\n    r = func.decompose(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_ops.py\", line 799, in decompose\n    return self._op_dk(dk, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py\", line 516, in guard_int\n    r = self.evaluate()\n  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py\", line 510, in evaluate\n    return self.shape_env.evaluate_sym_node(self, size_oblivious)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6655, in evaluate_sym_node\n    return self.evaluate_expr(\n  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/recording.py\", line 263, in wrapper\n    return retlog(fn(*args, **kwargs))\n  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6671, in evaluate_expr\n    return self._evaluate_expr(\n  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6894, in _evaluate_expr\n    raise self._make_data_dependent_error(\n\nGuardOnDataDependentSymNode: Could not extract specialized integer from data-dependent expression u0 (unhinted: u0).  (Size-like symbols: u0)\n\nCaused by: (_ops.py:799 in decompose)\nFor more information, run with TORCH_LOGS=\"dynamic\"\nFor extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\"\nIf you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\nFor more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n\nFor C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "model = LanguageModel(\"yujiepan/mixtral-8xtiny-random\", compile=False)\n",
    "with model.scan(\"a\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-02 22:38:10.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m221\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([2, 4, 8]), torch.Size([8, 8])]\n"
     ]
    }
   ],
   "source": [
    "from nnterp import StandardizedTransformer\n",
    "model = StandardizedTransformer(\"yujiepan/mixtral-8xtiny-random\")\n",
    "\n",
    "with model.trace([\"a\", \"azazza\"]):\n",
    "    print([t.shape  for t in model.mlps[0].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```py\n",
       "                                     * def forward(\n",
       "                                     0     self,\n",
       "                                     1     hidden_states: torch.Tensor,\n",
       "                                     2     attention_mask: Optional[torch.Tensor] = None,\n",
       "                                     3     position_ids: Optional[torch.LongTensor] = None,\n",
       "                                     4     past_key_value: Optional[tuple[torch.Tensor]] = None,\n",
       "                                     5     output_attentions: Optional[bool] = False,\n",
       "                                     6     output_router_logits: Optional[bool] = False,\n",
       "                                     7     use_cache: Optional[bool] = False,\n",
       "                                     8     cache_position: Optional[torch.LongTensor] = None,\n",
       "                                     9     position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
       "                                    10     **kwargs: Unpack[FlashAttentionKwargs],\n",
       "                                    11 ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
       "                                    12     \"\"\"\n",
       "                                    13     Args:\n",
       "                                    14         hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
       "                                    15         attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n",
       "                                    16             `(batch, sequence_length)` where padding elements are indicated by 0.\n",
       "                                    17         past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n",
       "                                    18         output_attentions (`bool`, *optional*):\n",
       "                                    19             Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
       "                                    20             returned tensors for more detail.\n",
       "                                    21         output_router_logits (`bool`, *optional*):\n",
       "                                    22             Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n",
       "                                    23             should not be returned during inference.\n",
       "                                    24         use_cache (`bool`, *optional*):\n",
       "                                    25             If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n",
       "                                    26             (see `past_key_values`).\n",
       "                                    27         cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n",
       "                                    28             Indices depicting the position of the input sequence tokens in the sequence.\n",
       "                                    29         kwargs (`dict`, *optional*):\n",
       "                                    30             Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n",
       "                                    31             into the model\n",
       "                                    32     \"\"\"\n",
       "                                    33 \n",
       "                                    34     residual = hidden_states\n",
       "                                    35 \n",
       " self_input_layernorm_0          -> 36     hidden_states = self.input_layernorm(hidden_states)\n",
       "                                    37 \n",
       "                                    38     # Self Attention\n",
       " self_self_attn_0                -> 39     hidden_states, self_attn_weights = self.self_attn(\n",
       "                                    40         hidden_states=hidden_states,\n",
       "                                    41         position_embeddings=position_embeddings,\n",
       "                                    42         attention_mask=attention_mask,\n",
       "                                    43         position_ids=position_ids,\n",
       "                                    44         past_key_value=past_key_value,\n",
       "                                    45         output_attentions=output_attentions,\n",
       "                                    46         use_cache=use_cache,\n",
       "                                    47         cache_position=cache_position,\n",
       "                                    48         **kwargs,\n",
       "                                    49     )\n",
       "                                    50     hidden_states = residual + hidden_states\n",
       "                                    51 \n",
       "                                    52     # Fully Connected\n",
       "                                    53     residual = hidden_states\n",
       " self_post_attention_layernorm_0 -> 54     hidden_states = self.post_attention_layernorm(hidden_states)\n",
       " self_block_sparse_moe_0         -> 55     hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n",
       "                                    56     hidden_states = residual + hidden_states\n",
       "                                    57 \n",
       "                                    58     outputs = (hidden_states,)\n",
       "                                    59 \n",
       "                                    60     if output_attentions:\n",
       "                                    61         outputs += (self_attn_weights,)\n",
       "                                    62 \n",
       "                                    63     if output_router_logits:\n",
       "                                    64         outputs += (router_logits,)\n",
       "                                    65 \n",
       "                                    66     return outputs\n",
       "                                    67 \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(f\"\"\"```py\n",
    "{model.layers[0].source}\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0702 22:51:06.512000 172684 torch/fx/experimental/symbolic_shapes.py:6679] failed during evaluate_expr(u0, hint=None, size_oblivious=False, forcing_spec=False\n",
      "E0702 22:51:06.513000 172684 torch/fx/experimental/recording.py:299] failed while running evaluate_expr(*(u0, None, False, False), **{})\n",
      "\u001b[32m2025-07-02 22:51:06.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnnterp.standardized_transformer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m221\u001b[0m - \u001b[1mEnforcing eager attention implementation for attention pattern tracing. The HF default would be to use sdpa if available. To use sdpa, set attn_implementation='sdpa' or None to use the HF default.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'transformers.models.mixtral.modeling_mixtral.MixtralSparseMoeBlock'>, <class 'torch.nn.modules.module.Module'>, <class 'object'>)\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 21, in __call__\n",
      "    tracer.execute(fn)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 487, in execute\n",
      "    super().execute(fn)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 331, in execute\n",
      "    self.model.interleave(interleaver, self.fn, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py\", line 76, in interleave\n",
      "    return super().interleave(interleaver, fn, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 705, in interleave\n",
      "    interleaver(fn, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 312, in __call__\n",
      "    fn(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 372, in __call__\n",
      "    else self._module(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n",
      "    value = fn(module, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n",
      "    return inner()\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 943, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\", line 747, in forward\n",
      "    outputs: MoeModelOutputWithPast = self.model(\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n",
      "    value = fn(module, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n",
      "    return inner()\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/transformers/utils/generic.py\", line 943, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\", line 539, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/transformers/modeling_layers.py\", line 83, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n",
      "    value = fn(module, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n",
      "    return inner()\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\", line 365, in forward\n",
      "    hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 127, in inner\n",
      "    value = fn(module, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n",
      "    return inner()\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\", line 127, in forward\n",
      "    for expert_idx in expert_hitted:\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/_tensor.py\", line 1195, in __iter__\n",
      "    return iter(self.unbind(0))\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 2770, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/utils/_stats.py\", line 27, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1282, in __torch_dispatch__\n",
      "    return self.dispatch(func, types, args, kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1823, in dispatch\n",
      "    return self._cached_dispatch_impl(func, types, args, kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1393, in _cached_dispatch_impl\n",
      "    output = self._dispatch_impl(func, types, args, kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 2333, in _dispatch_impl\n",
      "    decomposition_table[func](*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/_refs/__init__.py\", line 4002, in unbind\n",
      "    torch.squeeze(s, dim) for s in torch.tensor_split(t, t.shape[dim], dim)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/utils/_stats.py\", line 27, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1282, in __torch_dispatch__\n",
      "    return self.dispatch(func, types, args, kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1823, in dispatch\n",
      "    return self._cached_dispatch_impl(func, types, args, kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 1393, in _cached_dispatch_impl\n",
      "    output = self._dispatch_impl(func, types, args, kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/_subclasses/fake_tensor.py\", line 2338, in _dispatch_impl\n",
      "    r = func.decompose(*args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/_ops.py\", line 799, in decompose\n",
      "    return self._op_dk(dk, *args, **kwargs)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py\", line 516, in guard_int\n",
      "    r = self.evaluate()\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/sym_node.py\", line 510, in evaluate\n",
      "    return self.shape_env.evaluate_sym_node(self, size_oblivious)\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6655, in evaluate_sym_node\n",
      "    return self.evaluate_expr(\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/recording.py\", line 263, in wrapper\n",
      "    return retlog(fn(*args, **kwargs))\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6671, in evaluate_expr\n",
      "    return self._evaluate_expr(\n",
      "  File \"/root/.venv/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 6894, in _evaluate_expr\n",
      "    raise self._make_data_dependent_error(\n",
      "\n",
      "GuardOnDataDependentSymNode: Could not extract specialized integer from data-dependent expression u0 (unhinted: u0).  (Size-like symbols: u0)\n",
      "\n",
      "Caused by: (_ops.py:799 in decompose)\n",
      "For more information, run with TORCH_LOGS=\"dynamic\"\n",
      "For extended logs when we create symbols, also add TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"u0\"\n",
      "If you suspect the guard was triggered from C++, add TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
      "For more debugging help, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit?usp=sharing\n",
      "\n",
      "For C++ stack trace, run with TORCHDYNAMO_EXTENDED_DEBUG_CPP=1\n",
      "<class 'torch._subclasses.fake_tensor.FakeTensor'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(model._model.model.layers[0].block_sparse_moe).__mro__)\n",
    "try:\n",
    "    with model.scan(\"a\"):\n",
    "            pass\n",
    "            print(type(model.mlps_output[0]))\n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "\n",
    "gpt2 = StandardizedTransformer(\"gpt2\")\n",
    "with gpt2.scan(\"a\"):\n",
    "    pass\n",
    "    print(type(gpt2.mlps_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.54.0.dev0\n",
      "0.5.0.dev3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import nnsight\n",
    "print(transformers.__version__)\n",
    "print(nnsight.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel; model = LanguageModel(\"gpt2\")\n",
    "with model.trace(\"a\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000, -0.3108, -0.0311,  ..., -0.0029,  0.1946,  0.3862]]],\n",
      "       device='cuda:1')\n"
     ]
    },
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 21, in __call__\n    tracer.execute(fn)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 331, in execute\n    self.model.interleave(interleaver, self.fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py\", line 76, in interleave\n    return super().interleave(interleaver, fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 705, in interleave\n    interleaver(fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 312, in __call__\n    fn(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/modeling/language.py\", line 145, in __nnsight_generate__\n    output = self._model.generate(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2626, in generate\n    result = self._sample(\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 3610, in _sample\n    outputs = model_forward(**model_inputs, return_dict=True)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 659, in _fn\n    raise e.with_traceback(None) from None\n\nUnsupported: Unsupported context manager\n  Explanation: Dynamo does not know how to enter a `lock` context manager.\n  Hint: Avoid using the unsupported context manager.\n  Hint: File an issue to PyTorch. Simple context managers can potentially be supported, but note that context managers can't be supported in general\n\n  Developer debug context: Attempted SETUP_WITH/BEFORE_WITH on UserDefinedObjectVariable(lock)\n\n\nfrom user code:\n   File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 122, in inner\n    inputs = self.handle(self.iterate(f\"{provider}.input\"), (args, kwargs))\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 356, in handle\n    mediator.handle(provider)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 521, in handle\n    process = not self.event_queue.empty()\n  File \"/usr/lib/python3.10/queue.py\", line 108, in empty\n    with self.mutex:\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnnsight\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LanguageModel\n\u001b[1;32m      2\u001b[0m gemma3 \u001b[38;5;241m=\u001b[39m LanguageModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxolotl-ai-co/gemma-3-34M\u001b[39m\u001b[38;5;124m\"\u001b[39m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gemma3\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, world!\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(gemma3\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39msave())\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/base.py:387\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Suppress the ExitTracingException but let other exceptions propagate\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# Execute the traced code using the configured backend\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py:24\u001b[0m, in \u001b[0;36mExecutionBackend.__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     21\u001b[0m     tracer\u001b[38;5;241m.\u001b[39mexecute(fn)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer\u001b[38;5;241m.\u001b[39minfo) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     Globals\u001b[38;5;241m.\u001b[39mexit()\n",
      "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/backends/execution.py\", line 21, in __call__\n    tracer.execute(fn)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/tracing/tracer.py\", line 331, in execute\n    self.model.interleave(interleaver, self.fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/modeling/mixins/meta.py\", line 76, in interleave\n    return super().interleave(interleaver, fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/envoy.py\", line 705, in interleave\n    interleaver(fn, *args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 312, in __call__\n    fn(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/modeling/language.py\", line 145, in __nnsight_generate__\n    output = self._model.generate(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2626, in generate\n    result = self._sample(\n  File \"/root/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 3610, in _sample\n    outputs = model_forward(**model_inputs, return_dict=True)\n  File \"/root/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 659, in _fn\n    raise e.with_traceback(None) from None\n\nUnsupported: Unsupported context manager\n  Explanation: Dynamo does not know how to enter a `lock` context manager.\n  Hint: Avoid using the unsupported context manager.\n  Hint: File an issue to PyTorch. Simple context managers can potentially be supported, but note that context managers can't be supported in general\n\n  Developer debug context: Attempted SETUP_WITH/BEFORE_WITH on UserDefinedObjectVariable(lock)\n\n\nfrom user code:\n   File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 122, in inner\n    inputs = self.handle(self.iterate(f\"{provider}.input\"), (args, kwargs))\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 356, in handle\n    mediator.handle(provider)\n  File \"/root/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 521, in handle\n    process = not self.event_queue.empty()\n  File \"/usr/lib/python3.10/queue.py\", line 108, in empty\n    with self.mutex:\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "gemma3 = LanguageModel(\"axolotl-ai-co/gemma-3-34M\", device_map=\"auto\", dispatch=True)\n",
    "with gemma3.generate(\"Hello, world!\", max_length=10):\n",
    "    print(gemma3.output.logits.save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
